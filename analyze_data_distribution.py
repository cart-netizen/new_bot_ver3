#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
- –í—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ (gaps, –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç—å)
- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (class imbalance)
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List
from datetime import datetime, timedelta
from collections import Counter, defaultdict
import matplotlib.pyplot as plt


class DataDistributionAnalyzer:
  """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö."""

  def __init__(self, data_dir: str = "data/ml_training"):
    self.data_dir = Path(data_dir)

  def analyze_symbol(self, symbol: str) -> Dict:
    """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–∞."""
    symbol_path = self.data_dir / symbol
    labels_dir = symbol_path / "labels"

    if not labels_dir.exists():
      print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {labels_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
      return {}

    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö labels
    all_labels = []
    label_files = sorted(labels_dir.glob("*.json"))

    for label_file in label_files:
      with open(label_file) as f:
        all_labels.extend(json.load(f))

    if not all_labels:
      print(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")
      return {}

    print(f"\n{'=' * 80}")
    print(f"–ê–ù–ê–õ–ò–ó –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –î–ê–ù–ù–´–•: {symbol}")
    print(f"{'=' * 80}\n")

    # –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã
    results = {
      "symbol": symbol,
      "total_samples": len(all_labels),
      "temporal": self._analyze_temporal(all_labels),
      "class_distribution": self._analyze_class_distribution(all_labels),
      "market_conditions": self._analyze_market_conditions(all_labels),
      "signal_quality": self._analyze_signal_quality(all_labels),
      "data_quality": self._analyze_data_quality(all_labels)
    }

    # –ü–µ—á–∞—Ç—å –æ—Ç—á–µ—Ç–∞
    self._print_report(results)

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    self._print_recommendations(results)

    return results

  def _analyze_temporal(self, labels: List[Dict]) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è."""
    timestamps = [
      label.get("timestamp", 0)
      for label in labels
      if label.get("timestamp")
    ]

    if not timestamps:
      return {"error": "–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö timestamps"}

    timestamps = sorted(timestamps)

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ datetime
    start_time = datetime.fromtimestamp(timestamps[0] / 1000)
    end_time = datetime.fromtimestamp(timestamps[-1] / 1000)
    duration = end_time - start_time

    # –ü–æ–∏—Å–∫ gaps (–ø—Ä–æ–ø—É—Å–∫–æ–≤ > 1 –º–∏–Ω—É—Ç—ã)
    gaps = []
    for i in range(1, len(timestamps)):
      gap_ms = timestamps[i] - timestamps[i - 1]
      if gap_ms > 60_000:  # > 1 –º–∏–Ω—É—Ç–∞
        gap_start = datetime.fromtimestamp(timestamps[i - 1] / 1000)
        gap_end = datetime.fromtimestamp(timestamps[i] / 1000)
        gap_duration = gap_end - gap_start
        gaps.append({
          "start": gap_start.isoformat(),
          "end": gap_end.isoformat(),
          "duration_minutes": gap_duration.total_seconds() / 60
        })

    # –°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Å–µ–º–ø–ª–∞–º–∏
    intervals = np.diff(timestamps) / 1000  # –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    avg_interval = np.mean(intervals)

    return {
      "start_time": start_time.isoformat(),
      "end_time": end_time.isoformat(),
      "duration_days": duration.total_seconds() / 86400,
      "total_samples": len(timestamps),
      "avg_interval_seconds": float(avg_interval),
      "gaps_count": len(gaps),
      "gaps": gaps[:10]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
    }

  def _analyze_class_distribution(self, labels: List[Dict]) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤."""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ future labels
    has_future = any("future_direction_10s" in label for label in labels)

    if not has_future:
      return {
        "error": "–ù–µ—Ç future labels - —Ç—Ä–µ–±—É–µ—Ç—Å—è preprocessing",
        "samples_without_labels": len(labels)
      }

    # –°—á–∏—Ç–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤
    distributions = {}

    for horizon in ["10s", "30s", "60s"]:
      field = f"future_direction_{horizon}"

      directions = [
        label.get(field)
        for label in labels
        if field in label and label.get(field) is not None
      ]

      if directions:
        counter = Counter(directions)
        total = len(directions)

        distributions[horizon] = {
          "total": total,
          "SELL (-1)": counter.get(-1, 0),
          "HOLD (0)": counter.get(0, 0),
          "BUY (1)": counter.get(1, 0),
          "percentages": {
            "SELL": (counter.get(-1, 0) / total) * 100,
            "HOLD": (counter.get(0, 0) / total) * 100,
            "BUY": (counter.get(1, 0) / total) * 100
          },
          "imbalance_ratio": max(counter.values()) / min(counter.values()) if min(counter.values()) > 0 else float(
            'inf')
        }

    return distributions

  def _analyze_market_conditions(self, labels: List[Dict]) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π."""
    mid_prices = [
      label.get("current_mid_price")
      for label in labels
      if label.get("current_mid_price")
    ]

    spreads = [
      label.get("current_spread")
      for label in labels
      if label.get("current_spread")
    ]

    imbalances = [
      label.get("current_imbalance")
      for label in labels
      if label.get("current_imbalance")
    ]

    result = {}

    if mid_prices:
      mid_prices = np.array(mid_prices)
      returns = np.diff(mid_prices) / mid_prices[:-1]

      result["price"] = {
        "min": float(np.min(mid_prices)),
        "max": float(np.max(mid_prices)),
        "mean": float(np.mean(mid_prices)),
        "std": float(np.std(mid_prices)),
        "range_pct": ((np.max(mid_prices) - np.min(mid_prices)) / np.mean(mid_prices)) * 100
      }

      result["returns"] = {
        "mean": float(np.mean(returns)),
        "std": float(np.std(returns)),
        "min": float(np.min(returns)),
        "max": float(np.max(returns))
      }

    if spreads:
      spreads = np.array(spreads)
      result["spread"] = {
        "mean": float(np.mean(spreads)),
        "median": float(np.median(spreads)),
        "min": float(np.min(spreads)),
        "max": float(np.max(spreads))
      }

    if imbalances:
      imbalances = np.array(imbalances)
      result["imbalance"] = {
        "mean": float(np.mean(imbalances)),
        "median": float(np.median(imbalances)),
        "std": float(np.std(imbalances))
      }

    return result

  def _analyze_signal_quality(self, labels: List[Dict]) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–æ–≤."""
    signals_with_type = sum(
      1 for label in labels
      if label.get("signal_type") is not None
    )

    signal_types = Counter(
      label.get("signal_type")
      for label in labels
      if label.get("signal_type")
    )

    confidences = [
      label.get("signal_confidence")
      for label in labels
      if label.get("signal_confidence") is not None
    ]

    return {
      "total_samples": len(labels),
      "samples_with_signals": signals_with_type,
      "samples_without_signals": len(labels) - signals_with_type,
      "signal_types": dict(signal_types),
      "avg_confidence": float(np.mean(confidences)) if confidences else None,
      "coverage_pct": (signals_with_type / len(labels)) * 100
    }

  def _analyze_data_quality(self, labels: List[Dict]) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö."""
    missing_timestamp = sum(
      1 for label in labels
      if not label.get("timestamp") or label.get("timestamp") == 0
    )

    missing_price = sum(
      1 for label in labels
      if not label.get("current_mid_price")
    )

    missing_future = sum(
      1 for label in labels
      if "future_direction_10s" not in label
    )

    return {
      "total_samples": len(labels),
      "missing_timestamp": missing_timestamp,
      "missing_price": missing_price,
      "missing_future_labels": missing_future,
      "quality_score": ((len(labels) - missing_timestamp - missing_price) / len(labels)) * 100
    }

  def _print_report(self, results: Dict):
    """–ü–µ—á–∞—Ç—å –æ—Ç—á–µ—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞."""
    symbol = results["symbol"]
    total = results["total_samples"]

    print(f"üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print(f"  ‚Ä¢ –í—Å–µ–≥–æ —Å–µ–º–ø–ª–æ–≤: {total:,}\n")

    # –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑
    temporal = results.get("temporal", {})
    if "error" not in temporal:
      print(f"‚è∞ –í–†–ï–ú–ï–ù–ù–û–ï –ü–û–ö–†–´–¢–ò–ï")
      print(f"  ‚Ä¢ –ü–µ—Ä–∏–æ–¥: {temporal['start_time']} - {temporal['end_time']}")
      print(f"  ‚Ä¢ –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {temporal['duration_days']:.2f} –¥–Ω–µ–π")
      print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: {temporal['avg_interval_seconds']:.2f} —Å–µ–∫")

      if temporal['gaps_count'] > 0:
        print(f"  ‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤: {temporal['gaps_count']}")
        if temporal['gaps']:
          print(f"  ‚Ä¢ –ü–µ—Ä–≤—ã–µ gaps:")
          for gap in temporal['gaps'][:3]:
            print(f"    - {gap['start']} ({gap['duration_minutes']:.1f} –º–∏–Ω)")
      else:
        print(f"  ‚úÖ –ü—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
      print()

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    class_dist = results.get("class_distribution", {})
    if "error" not in class_dist:
      print(f"üìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í (Future Direction)")

      for horizon, dist in class_dist.items():
        print(f"\n  –ì–æ—Ä–∏–∑–æ–Ω—Ç {horizon}:")
        print(f"    ‚Ä¢ SELL (-1): {dist['SELL (-1)']:,} ({dist['percentages']['SELL']:.1f}%)")
        print(f"    ‚Ä¢ HOLD (0):  {dist['HOLD (0)']:,} ({dist['percentages']['HOLD']:.1f}%)")
        print(f"    ‚Ä¢ BUY (1):   {dist['BUY (1)']:,} ({dist['percentages']['BUY']:.1f}%)")

        if dist['imbalance_ratio'] > 2.0:
          print(f"    ‚ö†Ô∏è  –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {dist['imbalance_ratio']:.2f}x")
        else:
          print(f"    ‚úÖ –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –ø—Ä–∏–µ–º–ª–µ–º—ã–π")
    else:
      print(f"‚ùå {class_dist['error']}")

    print()

    # –†—ã–Ω–æ—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
    market = results.get("market_conditions", {})
    if market:
      print(f"üíπ –†–´–ù–û–ß–ù–´–ï –£–°–õ–û–í–ò–Ø")
      if "price" in market:
        p = market["price"]
        print(f"  ‚Ä¢ –¶–µ–Ω–∞: {p['min']:.2f} - {p['max']:.2f} (–¥–∏–∞–ø–∞–∑–æ–Ω: {p['range_pct']:.2f}%)")
      if "spread" in market:
        s = market["spread"]
        print(f"  ‚Ä¢ –°–ø—Ä–µ–¥: mean={s['mean']:.4f}, median={s['median']:.4f}")
      print()

    # –ö–∞—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤
    signals = results.get("signal_quality", {})
    if signals:
      print(f"üéØ –ö–ê–ß–ï–°–¢–í–û –°–ò–ì–ù–ê–õ–û–í")
      print(f"  ‚Ä¢ –°–µ–º–ø–ª–æ–≤ —Å —Å–∏–≥–Ω–∞–ª–∞–º–∏: {signals['samples_with_signals']:,} ({signals['coverage_pct']:.1f}%)")
      if signals['signal_types']:
        print(f"  ‚Ä¢ –¢–∏–ø—ã —Å–∏–≥–Ω–∞–ª–æ–≤: {signals['signal_types']}")
      if signals['avg_confidence']:
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {signals['avg_confidence']:.3f}")
      print()

    # –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
    quality = results.get("data_quality", {})
    if quality:
      print(f"‚úÖ –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•")
      print(f"  ‚Ä¢ Score: {quality['quality_score']:.1f}%")
      if quality['missing_timestamp'] > 0:
        print(f"  ‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç timestamp: {quality['missing_timestamp']:,}")
      if quality['missing_future_labels'] > 0:
        print(f"  ‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç future labels: {quality['missing_future_labels']:,}")
      print()

  def _print_recommendations(self, results: Dict):
    """–ü–µ—á–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
    print(f"{'=' * 80}")
    print(f"üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø")
    print(f"{'=' * 80}\n")

    recommendations = []

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö
    total = results["total_samples"]
    if total < 100_000:
      recommendations.append(
        f"üìä –ö–†–ò–¢–ò–ß–ù–û: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({total:,} < 100,000)\n"
        f"   ‚Üí –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ —Å–±–æ—Ä –º–∏–Ω–∏–º—É–º –¥–æ 1,000,000 —Å–µ–º–ø–ª–æ–≤ (~2 –Ω–µ–¥–µ–ª–∏)"
      )
    elif total < 1_000_000:
      recommendations.append(
        f"‚ö†Ô∏è  –î–∞–Ω–Ω—ã—Ö –º–∞–ª–æ–≤–∞—Ç–æ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω ({total:,} < 1,000,000)\n"
        f"   ‚Üí –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–±—Ä–∞—Ç—å 5,000,000+ —Å–µ–º–ø–ª–æ–≤ (~1-2 –º–µ—Å—è—Ü–∞)"
      )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ gaps
    temporal = results.get("temporal", {})
    if "error" not in temporal and temporal.get("gaps_count", 0) > 10:
      recommendations.append(
        f"‚ö†Ô∏è  –ú–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö ({temporal['gaps_count']})\n"
        f"   ‚Üí –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö\n"
        f"   ‚Üí –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ gaps (forward fill)"
      )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ class imbalance
    class_dist = results.get("class_distribution", {})
    if "error" not in class_dist:
      for horizon, dist in class_dist.items():
        if dist['imbalance_ratio'] > 3.0:
          recommendations.append(
            f"‚ö†Ô∏è  –°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è {horizon} ({dist['imbalance_ratio']:.1f}x)\n"
            f"   ‚Üí –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ class weights –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏\n"
            f"   ‚Üí –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ oversampling/undersampling\n"
            f"   ‚Üí –ü–æ–ø—Ä–æ–±—É–π—Ç–µ focal loss"
          )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ future labels
    quality = results.get("data_quality", {})
    if quality.get("missing_future_labels", 0) > 0:
      recommendations.append(
        f"‚ùå –ö–†–ò–¢–ò–ß–ù–û: –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç future labels\n"
        f"   ‚Üí –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python preprocessing_add_future_labels.py"
      )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤
    signals = results.get("signal_quality", {})
    if signals and signals.get("coverage_pct", 0) < 5:
      recommendations.append(
        f"üí° –ü–æ—á—Ç–∏ –Ω–µ—Ç —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ ({signals['coverage_pct']:.1f}%)\n"
        f"   ‚Üí –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç direction\n"
        f"   ‚Üí –ù–æ –ø–æ–ª–µ–∑–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
      )

    # –í—ã–≤–æ–¥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    if recommendations:
      for i, rec in enumerate(recommendations, 1):
        print(f"{i}. {rec}\n")
    else:
      print("‚úÖ –î–∞–Ω–Ω—ã–µ –≤—ã–≥–ª—è–¥—è—Ç —Ö–æ—Ä–æ—à–æ! –ì–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é.\n")

    print(f"{'=' * 80}\n")


def main():
  """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
  import argparse

  parser = argparse.ArgumentParser(
    description="–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML"
  )
  parser.add_argument(
    "--symbol",
    required=True,
    help="–¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, BTCUSDT)"
  )
  parser.add_argument(
    "--data-dir",
    default="data/ml_training",
    help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏"
  )

  args = parser.parse_args()

  analyzer = DataDistributionAnalyzer(data_dir=args.data_dir)
  analyzer.analyze_symbol(args.symbol)


if __name__ == "__main__":
  main()