# üéÆ GPU Memory Optimization –¥–ª—è RTX 3060 12GB

## ‚úÖ –ü—Ä–∏–º–µ–Ω—ë–Ω–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è (v1.0)

### –ü—Ä–æ–±–ª–µ–º–∞
```
CUDA out of memory. Tried to allocate 1.35 GiB.
GPU 0 has a total capacity of 12.00 GiB of which 4.62 GiB is free.
```

### –†–µ—à–µ–Ω–∏–µ

#### 1. **–£–º–µ–Ω—å—à–µ–Ω batch_size**
```python
# model_trainer_v2.py:82
batch_size: int = 128  # –ë—ã–ª–æ 256
```
**–≠–∫–æ–Ω–æ–º–∏—è:** ~3.5 GB GPU –ø–∞–º—è—Ç–∏

#### 2. **Gradient Accumulation** (—Å–æ—Ö—Ä–∞–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch)
```python
# model_trainer_v2.py:87
gradient_accumulation_steps: int = 2  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch = 128*2 = 256
```
**–≠—Ñ—Ñ–µ–∫—Ç:** –¢–æ—Ç –∂–µ —Ä–∞–∑–º–µ—Ä batch, –Ω–æ –≤ 2 –ø—Ä–æ—Ö–æ–¥–∞ (–º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ –∑–∞ —Ä–∞–∑)

#### 3. **CUDA Memory Allocator**
```python
# training_orchestrator.py:26
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
```
**–≠—Ñ—Ñ–µ–∫—Ç:** –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—é –ø–∞–º—è—Ç–∏, –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã

#### 4. **–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –∫–µ—à–∞**
```python
# model_trainer_v2.py:270
torch.cuda.empty_cache()  # –ü—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏

# model_trainer_v2.py:653
if (batch_idx + 1) % 50 == 0:
    torch.cuda.empty_cache()  # –ö–∞–∂–¥—ã–µ 50 –±–∞—Ç—á–µ–π
```
**–≠—Ñ—Ñ–µ–∫—Ç:** –û—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–∞–º—è—Ç—å

#### 5. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è num_workers**
```python
# data_loader.py:52
num_workers: int = 4  # –ë—ã–ª–æ 8
```
**–ü—Ä–∏—á–∏–Ω–∞:** –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ workers ‚Üí –±–æ–ª—å—à–µ CPU –ø–∞–º—è—Ç–∏ ‚Üí –¥–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ GPU

---

## üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏

### –î–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
- Batch size: 256
- Memory allocated: 5.04 GiB
- Memory reserved: 1.28 GiB
- **Attempted allocation: 1.35 GiB ‚Üí OOM** ‚ùå

### –ü–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
- Batch size: 128 (√ó 2 accumulation)
- Expected allocated: ~3.0-3.5 GiB
- Memory reserved: ~0.8 GiB
- **Free memory: ~7-8 GiB** ‚úÖ

---

## üîç –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏

### –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:
```bash
# Windows PowerShell
nvidia-smi -l 1

# –ò—â–∏—Ç–µ:
# - Memory-Usage: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å ~50-70% (6-8 GB –∏–∑ 12 GB)
# - GPU-Util: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 95-100%
```

### Python –∫–æ–¥ (–≤ trainer):
```python
import torch

# –¢–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
allocated = torch.cuda.memory_allocated() / 1e9
reserved = torch.cuda.memory_reserved() / 1e9
print(f"Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB")

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
max_allocated = torch.cuda.max_memory_allocated() / 1e9
print(f"Max allocated: {max_allocated:.2f} GB")
```

---

## üõ†Ô∏è –î–∞–ª—å–Ω–µ–π—à–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)

### –í–∞—Ä–∏–∞–Ω—Ç A: –ï—â—ë –º–µ–Ω—å—à–µ batch_size
```python
batch_size: int = 64  # –ë—ã–ª–æ 128
gradient_accumulation_steps: int = 4  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π = 256
```
**–ö–æ–≥–¥–∞:** –ï—Å–ª–∏ –≤—Å—ë –µ—â—ë OOM

### –í–∞—Ä–∏–∞–Ω—Ç B: –û—Ç–∫–ª—é—á–∏—Ç—å Mixed Precision (–≤—Ä–µ–º–µ–Ω–Ω–æ)
```python
use_mixed_precision: bool = False  # –ë—ã–ª–æ True
```
**–ö–æ–≥–¥–∞:** –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ (mixed precision –∏–Ω–æ–≥–¥–∞ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏)

### –í–∞—Ä–∏–∞–Ω—Ç C: Gradient Checkpointing
```python
# –í –º–æ–¥–µ–ª–∏ HybridCNNLSTM –¥–æ–±–∞–≤–∏—Ç—å:
from torch.utils.checkpoint import checkpoint

def forward(self, x):
    # –í–º–µ—Å—Ç–æ –æ–±—ã—á–Ω–æ–≥–æ forward:
    x = checkpoint(self.cnn_blocks, x)
    x = checkpoint(self.lstm, x)
    ...
```
**–≠—Ñ—Ñ–µ–∫—Ç:** –≠–∫–æ–Ω–æ–º–∏—è ~30-50% –ø–∞–º—è—Ç–∏ –∑–∞ —Å—á—ë—Ç –ø–µ—Ä–µ–≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤ backward

### –í–∞—Ä–∏–∞–Ω—Ç D: –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏
```python
# –í ModelConfig:
cnn_channels: Tuple[int, ...] = (32, 64, 128)  # –ë—ã–ª–æ (64, 128, 256)
lstm_hidden: int = 128  # –ë—ã–ª–æ 256
```
**–≠—Ñ—Ñ–µ–∫—Ç:** –ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ = –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è RTX 3060 12GB

### Production (—Ç–µ–∫—É—â–∞—è):
```python
TrainerConfigV2(
    batch_size=128,
    gradient_accumulation_steps=2,
    use_mixed_precision=True,
    num_workers=4,
)
```
**Memory usage:** ~60-70% (7-8 GB)
**Speed:** ~2-3 batch/s

### Conservative (–µ—Å–ª–∏ OOM):
```python
TrainerConfigV2(
    batch_size=64,
    gradient_accumulation_steps=4,
    use_mixed_precision=False,
    num_workers=2,
)
```
**Memory usage:** ~40-50% (5-6 GB)
**Speed:** ~1.5-2 batch/s

### Aggressive (–µ—Å–ª–∏ –µ—Å—Ç—å –∑–∞–ø–∞—Å):
```python
TrainerConfigV2(
    batch_size=192,
    gradient_accumulation_steps=1,
    use_mixed_precision=True,
    num_workers=6,
)
```
**Memory usage:** ~80-90% (10-11 GB)
**Speed:** ~3-4 batch/s
‚ö†Ô∏è **–†–∏—Å–∫ OOM!**

---

## üêõ Troubleshooting

### OOM –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ —ç–ø–æ—Ö–∏:
**–ü—Ä–∏—á–∏–Ω–∞:** –§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
**–†–µ—à–µ–Ω–∏–µ:**
```python
# –£–≤–µ–ª–∏—á–∏—Ç—å —á–∞—Å—Ç–æ—Ç—É –æ—á–∏—Å—Ç–∫–∏ –∫–µ—à–∞
if (batch_idx + 1) % 25 == 0:  # –ë—ã–ª–æ 50
    torch.cuda.empty_cache()
```

### OOM –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:
**–ü—Ä–∏—á–∏–Ω–∞:** Validation batch —Ç–æ–∂–µ –±–æ–ª—å—à–æ–π
**–†–µ—à–µ–Ω–∏–µ:**
```python
# –í _validate_epoch –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à–∏–π batch
val_batch_size = self.config.batch_size // 2
```

### OOM –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–ø–æ—Ö:
**–ü—Ä–∏—á–∏–Ω–∞:** –£—Ç–µ—á–∫–∞ –ø–∞–º—è—Ç–∏ (references)
**–†–µ—à–µ–Ω–∏–µ:**
```python
# –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏:
torch.cuda.empty_cache()
gc.collect()
```

### Mixed Precision –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç:
**–ü—Ä–∏—á–∏–Ω–∞:** Mixed precision —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç 2 –∫–æ–ø–∏–∏ –≤–µ—Å–æ–≤ (FP32 + FP16)
**–†–µ—à–µ–Ω–∏–µ:** –û—Ç–∫–ª—é—á–∏—Ç—å –Ω–∞ –≤—Ä–µ–º—è –æ—Ç–ª–∞–¥–∫–∏

---

## üìù Checklist –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –æ–±—É—á–µ–Ω–∏—è

- [ ] `batch_size <= 128` –¥–ª—è RTX 3060 12GB
- [ ] `gradient_accumulation_steps >= 2` –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ batch
- [ ] `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
- [ ] `num_workers <= 4` –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ CPU/GPU –ø–∞–º—è—Ç–∏
- [ ] –ó–∞–∫—Ä—ã—Ç—ã –¥—Ä—É–≥–∏–µ GPU –ø—Ä–æ—Ü–µ—Å—Å—ã (–±—Ä–∞—É–∑–µ—Ä, –∏–≥—Ä—ã, etc.)
- [ ] –ó–∞–ø—É—â–µ–Ω `nvidia-smi -l 1` –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
- [ ] –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –Ω–∞–ª–∏—á–∏–µ `torch.cuda.empty_cache()` –≤ –∫–æ–¥–µ

---

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [PyTorch Memory Management](https://pytorch.org/docs/stable/notes/cuda.html#memory-management)
- [Mixed Precision Training](https://pytorch.org/docs/stable/notes/amp_examples.html)
- [Gradient Checkpointing](https://pytorch.org/docs/stable/checkpoint.html)

---

–°–æ–∑–¥–∞–Ω–æ: 2025-11-27
–í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞: RTX 3060 12GB
–°—Ç–∞—Ç—É—Å: ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ
