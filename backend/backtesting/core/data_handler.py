"""
Historical Data Handler - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞.

–§—É–Ω–∫—Ü–∏–∏:
- –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–≤–µ—á–µ–π (candles) –∏–∑ Bybit API
- –ó–∞–≥—Ä—É–∑–∫–∞ —Å–Ω–∏–º–∫–æ–≤ orderbook (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
- –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (gaps, outliers, quality)
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤
"""

import asyncio
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any, Tuple
from pathlib import Path
import pickle
import hashlib

from backend.core.logger import get_logger
from backend.strategy.candle_manager import Candle
from backend.models.orderbook import OrderBookSnapshot, OrderBookMetrics
from backend.exchange.rest_client import rest_client
from backend.utils.helpers import get_timestamp_ms

logger = get_logger(__name__)


class DataQualityReport:
    """–û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö."""

    def __init__(self):
        self.is_valid = True
        self.issues: List[str] = []
        self.quality_score = 100.0  # 0-100
        self.total_candles = 0
        self.gaps_count = 0
        self.outliers_count = 0
        self.invalid_ohlc_count = 0

    def add_issue(self, issue: str, severity: float = 1.0):
        """
        –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É –∫–∞—á–µ—Å—Ç–≤–∞.

        Args:
            issue: –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
            severity: –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å (0-10), –≤–ª–∏—è–µ—Ç –Ω–∞ quality_score
        """
        self.issues.append(issue)
        self.quality_score -= severity
        if self.quality_score < 60:
            self.is_valid = False

    def __repr__(self):
        return (
            f"<DataQualityReport valid={self.is_valid} score={self.quality_score:.1f} "
            f"gaps={self.gaps_count} outliers={self.outliers_count}>"
        )


class HistoricalDataHandler:
    """
    –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞.

    –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:
    1. Bybit API - –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ (kline endpoint)
    2. –õ–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à - –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤

    Best Practices:
    - Chunked loading –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
    - –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ rate limits API
    """

    def __init__(
        self,
        cache_dir: str = "data/backtest_cache",
        enable_cache: bool = True,
        chunk_size_days: int = 7,  # –ó–∞–≥—Ä—É–∂–∞—Ç—å –ø–æ 7 –¥–Ω–µ–π
        rate_limit_requests_per_second: int = 10
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è data handler.

        Args:
            cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∞ –¥–∞–Ω–Ω—ã—Ö
            enable_cache: –í–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
            chunk_size_days: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–¥–Ω–∏)
            rate_limit_requests_per_second: –õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API
        """
        self.cache_dir = Path(cache_dir)
        self.enable_cache = enable_cache
        self.chunk_size_days = chunk_size_days
        self.rate_limit = rate_limit_requests_per_second

        # –°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∫—ç—à–∞
        if self.enable_cache:
            self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Rate limiting
        self._last_request_time = datetime.now()
        self._request_interval = 1.0 / rate_limit_requests_per_second

        logger.info(
            f"HistoricalDataHandler –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: "
            f"cache={enable_cache}, chunk_size={chunk_size_days}d"
        )

    async def get_candles(
        self,
        symbol: str,
        start: datetime,
        end: datetime,
        interval: str = "1"
    ) -> List[Candle]:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–≤–µ—á–∏ –∑–∞ –ø–µ—Ä–∏–æ–¥.

        Args:
            symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞ (BTCUSDT, ETHUSDT)
            start: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
            end: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞
            interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª (1, 3, 5, 15, 30, 60, 120, 240, 360, 720, D, W, M)

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–≤–µ—á–µ–π, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        """
        logger.info(
            f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Å–≤–µ—á–µ–π: {symbol} {interval}m "
            f"{start.strftime('%Y-%m-%d')} ‚Üí {end.strftime('%Y-%m-%d')}"
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_key = self._generate_cache_key(symbol, start, end, interval)
        cached_candles = self._load_from_cache(cache_key)

        if cached_candles is not None:
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫—ç—à–∞: {len(cached_candles)} —Å–≤–µ—á–µ–π")
            return cached_candles

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ —á–∞–Ω–∫–∞–º
        all_candles = []
        current_start = start

        while current_start < end:
            current_end = min(
                current_start + timedelta(days=self.chunk_size_days),
                end
            )

            logger.debug(
                f"  –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–∞: "
                f"{current_start.strftime('%Y-%m-%d')} ‚Üí {current_end.strftime('%Y-%m-%d')}"
            )

            chunk_candles = await self._fetch_candles_chunk(
                symbol, current_start, current_end, interval
            )

            all_candles.extend(chunk_candles)
            current_start = current_end

            # Rate limiting
            await self._rate_limit_delay()

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        all_candles.sort(key=lambda c: c.timestamp)

        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_candles)} —Å–≤–µ—á–µ–π –∏–∑ Bybit API")

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à
        if self.enable_cache:
            self._save_to_cache(cache_key, all_candles)

        return all_candles

    async def _fetch_candles_chunk(
        self,
        symbol: str,
        start: datetime,
        end: datetime,
        interval: str
    ) -> List[Candle]:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å —á–∞–Ω–∫ —Å–≤–µ—á–µ–π —á–µ—Ä–µ–∑ Bybit API.

        API endpoint: GET /v5/market/kline
        Limit: 1000 candles per request
        """
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
            start_ms = int(start.timestamp() * 1000)
            end_ms = int(end.timestamp() * 1000)

            logger.debug(
                f"üì° –ó–∞–ø—Ä–æ—Å –∫ Bybit API: symbol={symbol}, interval={interval}, "
                f"start={start.isoformat()}, end={end.isoformat()}"
            )

            # –ó–∞–ø—Ä–æ—Å –∫ Bybit API
            response = await rest_client.get_kline(
                symbol=symbol,
                interval=interval,
                start=start_ms,
                end=end_ms,
                limit=1000
            )

            logger.debug(f"üì• –û—Ç–≤–µ—Ç API: {response.get('retCode') if response else 'None'}")

            if not response:
                logger.warning(f"–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç API –¥–ª—è {symbol}")
                return []

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞ –æ—Ç–≤–µ—Ç–∞
            if response.get('retCode') != 0:
                logger.error(
                    f"–û—à–∏–±–∫–∞ API Bybit: code={response.get('retCode')}, "
                    f"msg={response.get('retMsg')}"
                )
                return []

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
            result = response.get('result', {})
            klines = result.get('list', [])

            if not klines:
                logger.warning(
                    f"API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Å–≤–µ—á–µ–π –¥–ª—è {symbol} "
                    f"({start.isoformat()} - {end.isoformat()})"
                )
                return []

            # –ü–∞—Ä—Å–∏–Ω–≥ —Å–≤–µ—á–µ–π
            candles = []
            for kline in klines:
                # Bybit kline format: [startTime, open, high, low, close, volume, turnover]
                candle = Candle(
                    timestamp=int(kline[0]),
                    open=float(kline[1]),
                    high=float(kline[2]),
                    low=float(kline[3]),
                    close=float(kline[4]),
                    volume=float(kline[5])
                    # Note: kline[6] is turnover/quote_volume, but Candle model doesn't have this field
                )
                candles.append(candle)

            logger.debug(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(candles)} —Å–≤–µ—á–µ–π")
            return candles

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–≤–µ—á–µ–π {symbol}: {e}", exc_info=True)
            return []

    async def validate_data_quality(
        self,
        candles: List[Candle],
        interval_minutes: int = 1
    ) -> DataQualityReport:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.

        –ü—Ä–æ–≤–µ—Ä–∫–∏:
        1. Gaps (–ø—Ä–æ–ø—É—Å–∫–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏)
        2. Outliers (–∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
        3. Monotonic timestamps
        4. OHLC –ª–æ–≥–∏–∫–∞ (high >= low, open/close –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö [low, high])

        Args:
            candles: –°–ø–∏—Å–æ–∫ —Å–≤–µ—á–µ–π
            interval_minutes: –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–≤–µ—á–µ–π (–º–∏–Ω—É—Ç—ã)

        Returns:
            DataQualityReport —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        report = DataQualityReport()
        report.total_candles = len(candles)

        if not candles:
            report.add_issue("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", severity=100)
            return report

        logger.info(f"üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {len(candles)} —Å–≤–µ—á–µ–π")

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ gaps (–ø—Ä–æ–ø—É—Å–∫–æ–≤)
        gaps = self._detect_gaps(candles, interval_minutes)
        report.gaps_count = len(gaps)
        if gaps:
            report.add_issue(
                f"–ù–∞–π–¥–µ–Ω–æ {len(gaps)} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö",
                severity=min(len(gaps) * 0.5, 20)
            )

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ outliers
        outliers = self._detect_outliers(candles)
        report.outliers_count = len(outliers)
        if outliers:
            report.add_issue(
                f"–ù–∞–π–¥–µ–Ω–æ {len(outliers)} –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π",
                severity=min(len(outliers) * 0.1, 10)
            )

        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏ timestamps
        if not self._is_monotonic(candles):
            report.add_issue("–ù–µ –º–æ–Ω–æ—Ç–æ–Ω–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏", severity=15)

        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ OHLC –ª–æ–≥–∏–∫–∏
        invalid_ohlc = self._validate_ohlc(candles)
        report.invalid_ohlc_count = len(invalid_ohlc)
        if invalid_ohlc:
            report.add_issue(
                f"{len(invalid_ohlc)} —Å–≤–µ—á–µ–π —Å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–º OHLC",
                severity=min(len(invalid_ohlc) * 0.2, 10)
            )

        logger.info(
            f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: quality_score={report.quality_score:.1f}, "
            f"gaps={report.gaps_count}, outliers={report.outliers_count}"
        )

        return report

    def _detect_gaps(
        self,
        candles: List[Candle],
        interval_minutes: int
    ) -> List[Tuple[datetime, datetime]]:
        """–ü–æ–∏—Å–∫ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (gaps) –≤ –¥–∞–Ω–Ω—ã—Ö."""
        gaps = []
        expected_interval_ms = interval_minutes * 60 * 1000

        for i in range(1, len(candles)):
            prev_candle = candles[i - 1]
            current_candle = candles[i]

            time_diff = current_candle.timestamp - prev_candle.timestamp

            # –ï—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –±–æ–ª—å—à–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ (—Å –¥–æ–ø—É—Å–∫–æ–º 10%)
            if time_diff > expected_interval_ms * 1.1:
                gap_start = datetime.fromtimestamp(prev_candle.timestamp / 1000)
                gap_end = datetime.fromtimestamp(current_candle.timestamp / 1000)
                gaps.append((gap_start, gap_end))

        return gaps

    def _detect_outliers(self, candles: List[Candle]) -> List[int]:
        """–ü–æ–∏—Å–∫ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π."""
        if len(candles) < 50:
            return []

        outliers = []

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ IQR (Interquartile Range)
        closes = [c.close for c in candles]

        # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        pct_changes = []
        for i in range(1, len(closes)):
            pct_change = abs((closes[i] - closes[i - 1]) / closes[i - 1]) * 100
            pct_changes.append(pct_change)

        if not pct_changes:
            return outliers

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–≤–∞—Ä—Ç–∏–ª–µ–π
        sorted_changes = sorted(pct_changes)
        q1_idx = len(sorted_changes) // 4
        q3_idx = 3 * len(sorted_changes) // 4

        q1 = sorted_changes[q1_idx]
        q3 = sorted_changes[q3_idx]
        iqr = q3 - q1

        # Outlier = –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ [Q1 - 3*IQR, Q3 + 3*IQR]
        lower_bound = q1 - 3 * iqr
        upper_bound = q3 + 3 * iqr

        for i, pct_change in enumerate(pct_changes):
            if pct_change < lower_bound or pct_change > upper_bound:
                outliers.append(i + 1)  # i+1 —Ç.–∫. pct_changes –Ω–∞ 1 –∫–æ—Ä–æ—á–µ

        return outliers

    def _is_monotonic(self, candles: List[Candle]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏ timestamps."""
        for i in range(1, len(candles)):
            if candles[i].timestamp <= candles[i - 1].timestamp:
                return False
        return True

    def _validate_ohlc(self, candles: List[Candle]) -> List[int]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è OHLC –ª–æ–≥–∏–∫–∏."""
        invalid = []

        for i, candle in enumerate(candles):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: high >= low
            if candle.high < candle.low:
                invalid.append(i)
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: open –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö [low, high]
            if not (candle.low <= candle.open <= candle.high):
                invalid.append(i)
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: close –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö [low, high]
            if not (candle.low <= candle.close <= candle.high):
                invalid.append(i)
                continue

        return invalid

    def _generate_cache_key(
        self,
        symbol: str,
        start: datetime,
        end: datetime,
        interval: str
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞."""
        key_str = f"{symbol}_{start.isoformat()}_{end.isoformat()}_{interval}"
        return hashlib.md5(key_str.encode()).hexdigest()

    def _load_from_cache(self, cache_key: str) -> Optional[List[Candle]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –∫—ç—à–∞."""
        if not self.enable_cache:
            return None

        cache_file = self.cache_dir / f"{cache_key}.pkl"

        if not cache_file.exists():
            return None

        try:
            with open(cache_file, 'rb') as f:
                candles = pickle.load(f)
                logger.debug(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫—ç—à–∞: {len(candles)} —Å–≤–µ—á–µ–π")
                return candles
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞ {cache_key}: {e}")
            return None

    def _save_to_cache(self, cache_key: str, candles: List[Candle]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à."""
        if not self.enable_cache:
            return

        cache_file = self.cache_dir / f"{cache_key}.pkl"

        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(candles, f)
                logger.debug(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∫—ç—à: {len(candles)} —Å–≤–µ—á–µ–π")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫—ç—à {cache_key}: {e}")

    async def _rate_limit_delay(self):
        """–ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è rate limit."""
        now = datetime.now()
        elapsed = (now - self._last_request_time).total_seconds()

        if elapsed < self._request_interval:
            delay = self._request_interval - elapsed
            await asyncio.sleep(delay)

        self._last_request_time = datetime.now()
