# üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ML –º–æ–¥–µ–ª–∏ –¥–ª—è Production Trading

## –û–±–∑–æ—Ä –∏–∑–º–µ–Ω–µ–Ω–∏–π

–î–∞–Ω–Ω—ã–π –ø–∞–∫–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç **industry-standard –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏** –¥–ª—è ML –º–æ–¥–µ–ª–∏ –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞.
–í—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ **—Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤** —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º —Ä–∏—Å–∫–æ–º.

---

## üìä –¶–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏

| –ú–µ—Ç—Ä–∏–∫–∞ | –ë—ã–ª–æ | –ü–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ | Industry Standard |
|---------|------|-------------------|-------------------|
| Accuracy | 33.89% | 55-62% | 55-65% |
| F1 Score | 32.50% | 52-58% | 50-60% |
| Val Loss | 1.219 | <0.7 | <0.6 |
| Train-Val Gap | 0.295 | <0.08 | <0.1 |

---

## üî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø

### 1. Learning Rate: `0.001` ‚Üí `5e-5`

**–ü–æ—á–µ–º—É —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ:**
- –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã –æ—á–µ–Ω—å —à—É–º–Ω—ã–µ
- –í—ã—Å–æ–∫–∏–π LR = –º–æ–¥–µ–ª—å "–ø–µ—Ä–µ—Å–∫–∞–∫–∏–≤–∞–µ—Ç" –æ–ø—Ç–∏–º—É–º—ã
- –ü—Ä–∏ LR=0.001 –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è

```python
# ‚ùå –ë–´–õ–û (–ø–ª–æ—Ö–æ)
learning_rate = 0.001

# ‚úÖ –°–¢–ê–õ–û (–ø—Ä–∞–≤–∏–ª—å–Ω–æ)
learning_rate = 5e-5  # –í 20 —Ä–∞–∑ –º–µ–Ω—å—à–µ!
```

### 2. Batch Size: `64` ‚Üí `256`

**–ü–æ—á–µ–º—É —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ:**
- –ú–∞–ª–µ–Ω—å–∫–∏–π batch = –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
- –û—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –ø—Ä–∏ —à—É–º–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ë–æ–ª—å—à–∏–π batch = –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤

```python
# ‚ùå –ë–´–õ–û (–ø–ª–æ—Ö–æ)
batch_size = 64

# ‚úÖ –°–¢–ê–õ–û (–ø—Ä–∞–≤–∏–ª—å–Ω–æ)
batch_size = 256  # –í 4 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ
```

### 3. Weight Decay: `~0` ‚Üí `0.01`

**–ü–æ—á–µ–º—É —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ:**
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ = overfitting
- –û—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –º–∞–ª–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (~15K samples)
- Weight decay –ø–æ–º–æ–≥–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏

```python
# ‚ùå –ë–´–õ–û (–ø–ª–æ—Ö–æ)
weight_decay = 1e-5  # –ü–æ—á—Ç–∏ –Ω–æ–ª—å

# ‚úÖ –°–¢–ê–õ–û (–ø—Ä–∞–≤–∏–ª—å–Ω–æ)
weight_decay = 0.01  # –ó–Ω–∞—á–∏–º–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
```

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
ml_optimized/
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ optimized_configs.py      # –í—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ hybrid_cnn_lstm_v2.py     # –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ losses.py                 # Loss functions
‚îÇ   ‚îú‚îÄ‚îÄ augmentation.py           # Data augmentation
‚îÇ   ‚îú‚îÄ‚îÄ model_trainer_v2.py       # –£–ª—É—á—à–µ–Ω–Ω—ã–π trainer
‚îÇ   ‚îî‚îÄ‚îÄ class_balancing_v2.py     # Class balancing
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îî‚îÄ‚îÄ optimized_ml_integration.py  # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ run_optimized_training.py    # –°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞
‚îî‚îÄ‚îÄ OPTIMIZATION_GUIDE.md         # –≠—Ç–æ—Ç —Ñ–∞–π–ª
```

---

## üõ† –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é

### –í–∞—Ä–∏–∞–Ω—Ç 1: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (Quick Fix)

–ò–∑–º–µ–Ω–∏—Ç—å **—Ç–æ–ª—å–∫–æ 3 –ø–∞—Ä–∞–º–µ—Ç—Ä–∞** –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º –∫–æ–¥–µ:

```python
# backend/ml_engine/training/model_trainer.py

@dataclass
class TrainerConfig:
    # –ò–ó–ú–ï–ù–ò–¢–¨ –≠–¢–ò 3 –ü–ê–†–ê–ú–ï–¢–†–ê:
    learning_rate: float = 5e-5      # –ë—ã–ª–æ: 0.001
    weight_decay: float = 0.01       # –ë—ã–ª–æ: ~0
    # batch_size –∏–∑–º–µ–Ω–∏—Ç—å –≤ DataConfig!

# backend/ml_engine/training/data_loader.py

@dataclass
class DataConfig:
    batch_size: int = 256            # –ë—ã–ª–æ: 64
```

### –í–∞—Ä–∏–∞–Ω—Ç 2: –ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

1. **–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã** –∏–∑ `ml_optimized/` –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏:

```bash
# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
cp ml_optimized/training/losses.py backend/ml_engine/training/
cp ml_optimized/training/augmentation.py backend/ml_engine/training/
cp ml_optimized/training/class_balancing_v2.py backend/ml_engine/training/
cp ml_optimized/integration/optimized_ml_integration.py backend/ml_engine/integration/
```

2. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:**

```python
from backend.ml_engine.integration.optimized_ml_integration import (
    setup_optimized_training,
    quick_start_training
)

# –ü–æ–ª–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
model_cfg, trainer_cfg, data_cfg, balance_cfg = setup_optimized_training()

# –ò–ª–∏ –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
model, history = quick_start_training(["BTCUSDT", "ETHUSDT"])
```

### –í–∞—Ä–∏–∞–Ω—Ç 3: –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ —Å–∫—Ä–∏–ø—Ç

```bash
python backend/ml_engine/scripts/run_optimized_training.py \
    --symbols BTCUSDT ETHUSDT \
    --days 30 \
    --preset production_small \
    --output-dir checkpoints/optimized
```

---

## üéØ –ü—Ä–µ—Å–µ—Ç—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π

### `production_small` (–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è 7-30 –¥–Ω–µ–π)

```python
{
    "learning_rate": 5e-5,
    "batch_size": 256,
    "weight_decay": 0.01,
    "epochs": 150,
    "dropout": 0.4,
    "focal_gamma": 2.5,
    "mixup_alpha": 0.2,
    "label_smoothing": 0.1
}
```

### `production_large` (–î–ª—è 60+ –¥–Ω–µ–π)

```python
{
    "learning_rate": 1e-4,
    "batch_size": 128,
    "weight_decay": 0.005,
    "epochs": 100,
    "dropout": 0.3,
    "focal_gamma": 2.0,
    "mixup_alpha": 0.1,
    "label_smoothing": 0.05
}
```

### `quick_experiment` (–ë—ã—Å—Ç—Ä—ã–µ —Ç–µ—Å—Ç—ã)

```python
{
    "learning_rate": 1e-4,
    "batch_size": 128,
    "epochs": 30,
    "use_augmentation": False,
    "early_stopping_patience": 10
}
```

### `conservative` (–ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è)

```python
{
    "learning_rate": 3e-5,
    "batch_size": 256,
    "weight_decay": 0.02,
    "dropout": 0.5,
    "focal_gamma": 3.0,
    "label_smoothing": 0.15
}
```

---

## üìã –ß–µ–∫–ª–∏—Å—Ç –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º Production

- [ ] Learning rate ‚â§ 1e-4 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 5e-5)
- [ ] Batch size ‚â• 128 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 256)
- [ ] Weight decay ‚â• 0.001 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 0.01)
- [ ] Focal Loss –≤–∫–ª—é—á—ë–Ω (gamma ‚â• 2.0)
- [ ] Class weights –≤–∫–ª—é—á–µ–Ω—ã
- [ ] Early stopping patience ‚â• 15
- [ ] Validation accuracy > 50%
- [ ] Train-Val loss gap < 0.15

---

## üî¨ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### 1. Loss Functions (`training/losses.py`)

- **LabelSmoothingCrossEntropy**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overconfidence
- **FocalLossV2**: –§–æ–∫—É—Å –Ω–∞ hard examples (gamma=2.5)
- **MultiTaskLossV2**: Direction + Confidence + Return
- **DirectionalAccuracyLoss**: –®—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ

### 2. Data Augmentation (`training/augmentation.py`)

- **MixUp**: –°–º–µ—à–∏–≤–∞–Ω–∏–µ samples (alpha=0.2)
- **Time Masking**: –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
- **Gaussian Noise**: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ (std=0.01)
- **Feature Dropout**: Dropout –æ—Ç–¥–µ–ª—å–Ω—ã—Ö features

### 3. Class Balancing (`training/class_balancing_v2.py`)

- **Adaptive Threshold**: Percentile-based labeling
- **Class Weights**: Balanced / Sqrt / Effective
- **Oversampling**: Random oversampling –¥–ª—è minority

### 4. Model Architecture (`models/hybrid_cnn_lstm_v2.py`)

- **Residual Connections**: –£–ª—É—á—à–µ–Ω–Ω—ã–π gradient flow
- **Multi-Head Attention**: 4 heads –≤–º–µ—Å—Ç–æ 1
- **Layer Normalization**: –õ—É—á—à–µ –¥–ª—è sequences
- **GELU Activation**: –õ—É—á—à–µ —á–µ–º ReLU –¥–ª—è transformers

---

## ‚ö†Ô∏è –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

1. **–û–±—ä—ë–º –¥–∞–Ω–Ω—ã—Ö**: –ü—Ä–∏ < 7 –¥–Ω—è—Ö –¥–∞–Ω–Ω—ã—Ö –¥–∞–∂–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
   –º–æ–∂–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 14 –¥–Ω–µ–π.

2. **Class Imbalance**: –ü—Ä–∏ —Å–∏–ª—å–Ω–æ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ (HOLD > 80%) –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ
   –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π oversampling –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç–µ thresholds –¥–ª—è labeling.

3. **GPU Memory**: –ü—Ä–∏ batch_size=256 –∏ sequence_length=60 —Ç—Ä–µ–±—É–µ—Ç—Å—è
   ~4GB GPU –ø–∞–º—è—Ç–∏. –ù–∞ CPU –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω—ã–º.

---

## üìà –û–∂–∏–¥–∞–µ–º—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è

```
Epoch 1-10:   val_loss ‚Üì –±—ã—Å—Ç—Ä–æ, val_acc ~35-40%
Epoch 10-30:  val_loss ‚Üì —É–º–µ—Ä–µ–Ω–Ω–æ, val_acc ~40-48%
Epoch 30-70:  val_loss ‚Üì –º–µ–¥–ª–µ–Ω–Ω–æ, val_acc ~48-55%
Epoch 70-100: val_loss —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è, val_acc ~55-60%
Epoch 100+:   Fine-tuning, val_acc 58-62%
```

–ï—Å–ª–∏ val_loss –Ω–µ –ø–∞–¥–∞–µ—Ç –ø–æ—Å–ª–µ 20 —ç–ø–æ—Ö:
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ learning rate (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å ~5e-5)
2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ batch size (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å ‚â•128)
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ NaN/Inf

---

## üÜò Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: Val loss –Ω–µ –ø–∞–¥–∞–µ—Ç

```python
# –†–µ—à–µ–Ω–∏–µ 1: –£–º–µ–Ω—å—à–∏—Ç—å learning rate
learning_rate = 1e-5  # –ï—â—ë –º–µ–Ω—å—à–µ

# –†–µ—à–µ–Ω–∏–µ 2: –£–≤–µ–ª–∏—á–∏—Ç—å weight decay
weight_decay = 0.02  # –ë–æ–ª—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏

# –†–µ—à–µ–Ω–∏–µ 3: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
assert not np.isnan(X_train).any()
assert not np.isinf(X_train).any()
```

### –ü—Ä–æ–±–ª–µ–º–∞: Overfitting (train_loss << val_loss)

```python
# –†–µ—à–µ–Ω–∏–µ: –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é
dropout = 0.5  # –ë—ã–ª–æ 0.4
weight_decay = 0.02  # –ë—ã–ª–æ 0.01
mixup_alpha = 0.3  # –ë—ã–ª–æ 0.2
label_smoothing = 0.15  # –ë—ã–ª–æ 0.1
```

### –ü—Ä–æ–±–ª–µ–º–∞: Accuracy ~33% (random)

```python
# –†–µ—à–µ–Ω–∏–µ 1: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å class imbalance
from collections import Counter
print(Counter(y_train))  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å ~—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ

# –†–µ—à–µ–Ω–∏–µ 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å adaptive threshold
use_adaptive_threshold = True
percentile_sell = 0.30  # –ë—ã–ª–æ 0.25
percentile_buy = 0.70   # –ë—ã–ª–æ 0.75
```

---

## üìù –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ

- **v1.0** (—Ç–µ–∫—É—â–∞—è): –ë–∞–∑–æ–≤—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- **v1.1** (–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è): Curriculum Learning
- **v1.2** (–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è): Ensemble –º–æ–¥–µ–ª–µ–π
- **v2.0** (–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è): Transformer architecture

---

## üë§ –ö–æ–Ω—Ç–∞–∫—Ç—ã

–ü—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º –æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å –≤ Issues –∏–ª–∏ —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ PR.

---

*–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–∞ –¥–ª—è –≤–µ—Ä—Å–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –æ—Ç 2024-01-XX*
