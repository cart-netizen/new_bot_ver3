#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è ML –º–æ–¥–µ–ª–∏ - Industry Standard.

–î–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
ML –º–æ–¥–µ–ª–∏ –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —É—á—ë—Ç–æ–º:
- –ú–∞–ª–æ–≥–æ –æ–±—ä—ë–º–∞ –¥–∞–Ω–Ω—ã—Ö (7-30 –¥–Ω–µ–π)
- –í—ã—Å–æ–∫–æ–≥–æ class imbalance (Hold –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç)
- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–æ—Ä–≥–∞—Ö

–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:
1. Learning Rate: 0.001 ‚Üí 5e-5 (–≤ 20 —Ä–∞–∑ –º–µ–Ω—å—à–µ)
2. Batch Size: 64 ‚Üí 256 (–≤ 4 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ)
3. Weight Decay: ~0 ‚Üí 0.01 (—Å–∏–ª—å–Ω–∞—è L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
4. Scheduler: ReduceLROnPlateau ‚Üí CosineAnnealingWarmRestarts
5. Dropout: 0.3 ‚Üí 0.4 (—É–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
6. Focal Gamma: 2.0 ‚Üí 2.5 (—Ñ–æ–∫—É—Å –Ω–∞ hard examples)

–ü—É—Ç—å: backend/ml_engine/configs/optimized_configs.py
"""

from dataclasses import dataclass, field
from typing import List, Tuple, Optional, Dict, Any
from enum import Enum
from pathlib import Path
import torch

# Project root models directory
_PROJECT_ROOT = Path(__file__).parent.parent.parent.parent  # backend/ml_engine/configs -> project_root
_DEFAULT_MODELS_DIR = str(_PROJECT_ROOT / "models")


class LRSchedulerType(str, Enum):
    """–¢–∏–ø—ã Learning Rate Scheduler."""
    REDUCE_ON_PLATEAU = "ReduceLROnPlateau"
    COSINE_ANNEALING = "CosineAnnealing"
    COSINE_WARM_RESTARTS = "CosineAnnealingWarmRestarts"
    ONE_CYCLE = "OneCycleLR"
    EXPONENTIAL = "ExponentialLR"


class ClassBalancingMethod(str, Enum):
    """–ú–µ—Ç–æ–¥—ã –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤."""
    NONE = "none"
    CLASS_WEIGHTS = "class_weights"
    FOCAL_LOSS = "focal_loss"
    FOCAL_WITH_WEIGHTS = "focal_with_weights"
    OVERSAMPLING = "oversampling"
    SMOTE = "smote"
    COMBINED = "combined"  # Focal + Oversampling


# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

@dataclass
class OptimizedModelConfig:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ HybridCNNLSTM.
    
    –ò–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏:
    - –£–º–µ–Ω—å—à–µ–Ω—ã cnn_channels –¥–ª—è –º–∞–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
    - –£–º–µ–Ω—å—à–µ–Ω lstm_hidden (256 ‚Üí 128)
    - –£–≤–µ–ª–∏—á–µ–Ω dropout (0.3 ‚Üí 0.4)
    - –î–æ–±–∞–≤–ª–µ–Ω—ã residual connections
    - –î–æ–±–∞–≤–ª–µ–Ω Multi-Head Attention
    - –î–æ–±–∞–≤–ª–µ–Ω Layer Normalization
    """
    
    # === Input –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    input_features: int = 110  # OrderBook(50) + Candle(25) + Indicator(35)
    sequence_length: int = 60  # 60 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
    
    # === CNN –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–£–ú–ï–ù–¨–®–ï–ù–´ –¥–ª—è –º–∞–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞) ===
    # –ë—ã–ª–æ: (64, 128, 256) - ~500K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    # –°—Ç–∞–ª–æ: (32, 64, 128) - ~150K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    cnn_channels: Tuple[int, ...] = (32, 64, 128)
    cnn_kernel_sizes: Tuple[int, ...] = (3, 5, 7)
    
    # === LSTM –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–£–ú–ï–ù–¨–®–ï–ù–´) ===
    # –ë—ã–ª–æ: 256 hidden, 2 layers
    # –°—Ç–∞–ª–æ: 128 hidden, 2 layers
    lstm_hidden: int = 128
    lstm_layers: int = 2
    lstm_dropout: float = 0.3
    
    # === Attention –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–£–õ–£–ß–®–ï–ù–´) ===
    attention_units: int = 64
    attention_heads: int = 4  # –ù–û–í–û–ï: Multi-Head Attention
    attention_dropout: float = 0.1
    
    # === Output –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    num_classes: int = 3  # DOWN/SELL=0, NEUTRAL/HOLD=1, UP/BUY=2
    
    # === Regularization (–£–°–ò–õ–ï–ù–ê) ===
    # –ë—ã–ª–æ: 0.3
    # –°—Ç–∞–ª–æ: 0.4 (–¥–ª—è –ª—É—á—à–µ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ –º–∞–ª–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ)
    dropout: float = 0.4
    
    # === –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è (–ù–û–í–û–ï) ===
    use_residual: bool = True  # Residual connections –≤ CNN
    use_layer_norm: bool = True  # LayerNorm –≤–º–µ—Å—Ç–æ BatchNorm –≤ LSTM
    use_multi_head_attention: bool = True  # Multi-Head Attention

    # === Memory Optimization ===
    use_gradient_checkpointing: bool = False  # Enable only if OOM (may conflict with mixed precision)

    # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
    init_method: str = "kaiming"  # kaiming, xavier, orthogonal
    
    def get_estimated_params(self) -> int:
        """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
        # CNN params
        cnn_params = sum(
            (self.cnn_channels[i-1] if i > 0 else 1) * ch * k
            for i, (ch, k) in enumerate(zip(self.cnn_channels, self.cnn_kernel_sizes))
        )
        
        # LSTM params (bidirectional)
        lstm_input = self.cnn_channels[-1]
        lstm_params = 4 * self.lstm_hidden * (lstm_input + self.lstm_hidden + 1)
        lstm_params *= self.lstm_layers * 2  # bidirectional
        
        # Attention params
        attn_params = self.lstm_hidden * 2 * self.attention_units * 2
        
        # Head params
        head_params = self.lstm_hidden * 2 * 128 + 128 * self.num_classes
        head_params += self.lstm_hidden * 2 * 64 + 64  # confidence
        head_params += self.lstm_hidden * 2 * 64 + 64  # return
        
        return cnn_params + lstm_params + attn_params + head_params
    
    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
        return {
            'input_features': self.input_features,
            'sequence_length': self.sequence_length,
            'cnn_channels': self.cnn_channels,
            'cnn_kernel_sizes': self.cnn_kernel_sizes,
            'lstm_hidden': self.lstm_hidden,
            'lstm_layers': self.lstm_layers,
            'lstm_dropout': self.lstm_dropout,
            'attention_units': self.attention_units,
            'attention_heads': self.attention_heads,
            'num_classes': self.num_classes,
            'dropout': self.dropout,
            'use_residual': self.use_residual,
            'use_layer_norm': self.use_layer_norm,
            'use_multi_head_attention': self.use_multi_head_attention,
            'estimated_params': self.get_estimated_params()
        }


# ============================================================================
# TRAINER CONFIGURATION
# ============================================================================

@dataclass
class OptimizedTrainerConfig:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è.
    
    –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø:
    1. learning_rate: 0.001 ‚Üí 5e-5 (–≤ 20 —Ä–∞–∑ –º–µ–Ω—å—à–µ!)
    2. batch_size: 64 ‚Üí 256 (–±–æ–ª—å—à–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)
    3. weight_decay: ~0 ‚Üí 0.01 (L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
    4. epochs: 100 ‚Üí 150 (–±–æ–ª—å—à–µ —ç–ø–æ—Ö —Å –º–µ–Ω—å—à–∏–º LR)
    5. scheduler: ReduceLROnPlateau ‚Üí CosineAnnealingWarmRestarts
    """
    
    # === Training –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–´) ===
    epochs: int = 150  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö —Å –º–µ–Ω—å—à–∏–º LR
    
    # –ö–†–ò–¢–ò–ß–ù–û: Learning Rate —É–º–µ–Ω—å—à–µ–Ω –≤ 20 —Ä–∞–∑!
    # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—á–µ–Ω—å —à—É–º–Ω—ã–µ, –≤—ã—Å–æ–∫–∏–π LR = overfitting
    learning_rate: float = 5e-5  # –ë—ã–ª–æ: 0.001
    
    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ LR –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
    min_learning_rate: float = 1e-7
    max_learning_rate: float = 1e-4  # –î–ª—è OneCycleLR
    
    # Batch Size: –±–µ–∑ mixed precision –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–π batch
    batch_size: int = 128  # –°—Ç–∞–±–∏–ª—å–Ω–µ–µ —Å use_mixed_precision=False
    
    # –ö–†–ò–¢–ò–ß–ù–û: Weight Decay –¥–æ–±–∞–≤–ª–µ–Ω!
    # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overfitting
    weight_decay: float = 0.01  # –ë—ã–ª–æ: ~0
    
    # Gradient Clipping (–æ—Å—Ç–∞–≤–ª—è–µ–º)
    grad_clip_value: float = 1.0
    grad_clip_norm: Optional[float] = None  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: clip by norm
    
    # === Early Stopping (–£–í–ï–õ–ò–ß–ï–ù patience) ===
    early_stopping_patience: int = 20  # –ë—ã–ª–æ: 10-15
    early_stopping_delta: float = 1e-4
    early_stopping_metric: str = "val_loss"  # –∏–ª–∏ "val_f1"
    
    # === Learning Rate Scheduler (–ò–ó–ú–ï–ù–Å–ù) ===
    lr_scheduler: LRSchedulerType = LRSchedulerType.COSINE_WARM_RESTARTS
    
    # CosineAnnealingWarmRestarts –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    scheduler_T_0: int = 10  # –ü–µ—Ä–∏–æ–¥ –ø–µ—Ä–≤–æ–≥–æ —Ü–∏–∫–ª–∞
    scheduler_T_mult: int = 2  # –£–º–Ω–æ–∂–∏—Ç–µ–ª—å –ø–µ—Ä–∏–æ–¥–∞
    scheduler_eta_min: float = 1e-7  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR
    
    # ReduceLROnPlateau –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (fallback)
    scheduler_patience: int = 5
    scheduler_factor: float = 0.5
    
    # === Multi-task Learning Weights ===
    direction_loss_weight: float = 1.0
    confidence_loss_weight: float = 0.5
    return_loss_weight: float = 0.3
    
    # === Label Smoothing (–ù–û–í–û–ï) ===
    # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overconfidence
    label_smoothing: float = 0.1  # 0 = off, 0.1-0.2 = recommended
    
    # === Data Augmentation (–ù–û–í–û–ï) ===
    use_augmentation: bool = True
    mixup_alpha: float = 0.2  # MixUp –ø–∞—Ä–∞–º–µ—Ç—Ä (0 = off)
    gaussian_noise_std: float = 0.01  # –®—É–º –∫ features
    time_mask_ratio: float = 0.1  # –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
    feature_dropout_ratio: float = 0.05  # Dropout –æ—Ç–¥–µ–ª—å–Ω—ã—Ö features

    # === Memory Optimization ===
    gradient_accumulation_steps: int = 1  # Accumulate gradients (effective_batch = batch_size * steps)
    use_mixed_precision: bool = False  # FP16 training (–æ—Ç–∫–ª—é—á–µ–Ω–æ - –≤—ã–∑—ã–≤–∞–µ—Ç NaN)
    use_gradient_checkpointing: bool = False  # Enable only if OOM

    # === Checkpoint ===
    checkpoint_dir: str = _DEFAULT_MODELS_DIR  # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ project_root/models
    save_best_only: bool = True
    save_every_n_epochs: int = 10
    
    # === Device ===
    device: str = field(default_factory=lambda: "cuda" if torch.cuda.is_available() else "cpu")
    
    # === Logging ===
    log_interval: int = 10  # Log every N batches
    verbose: bool = True
    use_tqdm: bool = True
    
    # === Reproducibility ===
    seed: int = 42
    deterministic: bool = True
    
    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
        return {
            'epochs': self.epochs,
            'learning_rate': self.learning_rate,
            'batch_size': self.batch_size,
            'weight_decay': self.weight_decay,
            'grad_clip_value': self.grad_clip_value,
            'early_stopping_patience': self.early_stopping_patience,
            'lr_scheduler': self.lr_scheduler.value,
            'label_smoothing': self.label_smoothing,
            'use_augmentation': self.use_augmentation,
            'mixup_alpha': self.mixup_alpha,
            'device': self.device
        }


# ============================================================================
# CLASS BALANCING CONFIGURATION
# ============================================================================

@dataclass
class OptimizedBalancingConfig:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤.
    
    –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø:
    1. focal_gamma: 2.0 ‚Üí 2.5 (–±–æ–ª—å—à–µ —Ñ–æ–∫—É—Å–∞ –Ω–∞ hard examples)
    2. –î–æ–±–∞–≤–ª–µ–Ω oversampling –¥–ª—è minority –∫–ª–∞—Å—Å–æ–≤
    3. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    """

    # === –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ ===
    method: ClassBalancingMethod = ClassBalancingMethod.FOCAL_LOSS  # Focal Loss + Oversampling

    # === Class Weights ===
    # DISABLED when using oversampling - they cause double compensation!
    # Oversampling already balances data physically
    use_class_weights: bool = False  # DISABLED: oversampling is primary balancing method
    class_weight_method: str = "balanced"  # balanced, sqrt, log
    
    # Custom –≤–µ—Å–∞ (–µ—Å–ª–∏ –Ω–µ auto)
    # –¢–∏–ø–∏—á–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: Hold ~70%, Buy ~15%, Sell ~15%
    # –í–µ—Å–∞: –æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã —á–∞—Å—Ç–æ—Ç–µ
    custom_class_weights: Optional[Dict[int, float]] = None
    
    # === Focal Loss –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    use_focal_loss: bool = True
    focal_gamma: float = 1.5  # Reduced from 2.0 - less aggressive with oversampling
    focal_alpha: Optional[List[float]] = None  # Auto-compute if None

    # === Resampling –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    use_oversampling: bool = True   # ENABLED: Physical data balancing for stability
    oversample_ratio: float = 1.0   # Full balance (100% = equal classes)
    
    use_undersampling: bool = False  # –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–∏ –º–∞–ª–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
    undersample_ratio: float = 0.8
    
    use_smote: bool = False  # SMOTE –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å–ø–æ—Ä–µ–Ω
    smote_k_neighbors: int = 5
    
    # === Threshold –¥–ª—è Labeling (–ù–û–í–û–ï) ===
    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π threshold –≤–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ
    adaptive_threshold: bool = True
    threshold_percentile_sell: float = 0.25  # Bottom 25% = Sell
    threshold_percentile_buy: float = 0.75  # Top 25% = Buy
    
    # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π threshold (–µ—Å–ª–∏ adaptive=False)
    fixed_threshold_sell: float = -0.001  # -0.1%
    fixed_threshold_buy: float = 0.001  # +0.1%
    
    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
        return {
            'method': self.method.value,
            'use_class_weights': self.use_class_weights,
            'use_focal_loss': self.use_focal_loss,
            'focal_gamma': self.focal_gamma,
            'use_oversampling': self.use_oversampling,
            'oversample_ratio': self.oversample_ratio,
            'adaptive_threshold': self.adaptive_threshold
        }


# ============================================================================
# DATA CONFIGURATION
# ============================================================================

@dataclass
class OptimizedDataConfig:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö.
    
    –ò–∑–º–µ–Ω–µ–Ω–∏—è:
    - batch_size —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω —Å trainer config
    - –î–æ–±–∞–≤–ª–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Feature Store
    """
    
    # === Storage ===
    storage_path: str = "data/ml_training"
    
    # === Sequence –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    sequence_length: int = 60
    target_horizon: str = "future_direction_60s"
    
    # === Split –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    train_ratio: float = 0.7
    val_ratio: float = 0.15
    test_ratio: float = 0.15
    
    # === DataLoader –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–°–ò–ù–•–†–û–ù–ò–ó–ò–†–û–í–ê–ù–´) ===
    batch_size: int = 128  # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å TrainerConfig
    shuffle: bool = True
    num_workers: int = 4  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è 12GB GPU
    pin_memory: bool = True
    drop_last: bool = True  # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ BatchNorm
    
    # === Feature Store ===
    use_feature_store: bool = True
    feature_store_date_range_days: int = 90
    feature_store_group: str = "training_features"
    fallback_to_legacy: bool = True
    
    # === Preprocessing ===
    normalize_features: bool = True
    clip_outliers: bool = True
    outlier_std: float = 5.0  # Clip values > 5 std
    
    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
        return {
            'sequence_length': self.sequence_length,
            'batch_size': self.batch_size,
            'train_ratio': self.train_ratio,
            'val_ratio': self.val_ratio,
            'test_ratio': self.test_ratio,
            'use_feature_store': self.use_feature_store,
            'feature_store_date_range_days': self.feature_store_date_range_days
        }


# ============================================================================
# PRESET CONFIGURATIONS
# ============================================================================

class ConfigPresets:
    """–ü—Ä–µ—Å–µ—Ç—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤."""
    
    @staticmethod
    def production_small_data() -> Tuple[OptimizedModelConfig, OptimizedTrainerConfig, OptimizedBalancingConfig]:
        """
        –ü—Ä–µ—Å–µ—Ç –¥–ª—è production —Å –º–∞–ª—ã–º –æ–±—ä—ë–º–æ–º –¥–∞–Ω–Ω—ã—Ö (7-30 –¥–Ω–µ–π).
        
        –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
        - –ú–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å (~150K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        - –°–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        - –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π learning rate
        """
        model_config = OptimizedModelConfig(
            cnn_channels=(32, 64, 128),
            lstm_hidden=128,
            dropout=0.4,
            use_residual=True,
            use_layer_norm=True,
            use_multi_head_attention=True
        )
        
        trainer_config = OptimizedTrainerConfig(
            epochs=150,
            learning_rate=5e-5,
            batch_size=128,
            weight_decay=0.01,
            label_smoothing=0.1,
            use_augmentation=True,
            mixup_alpha=0.2
        )
        
        balancing_config = OptimizedBalancingConfig(
            method=ClassBalancingMethod.FOCAL_LOSS,  # –¢–æ–ª—å–∫–æ Focal Loss
            use_class_weights=False,
            use_focal_loss=True,
            focal_gamma=2.5,
            use_oversampling=False  # –û—Ç–∫–ª—é—á–µ–Ω–æ
        )

        return model_config, trainer_config, balancing_config

    @staticmethod
    def production_large_data() -> Tuple[OptimizedModelConfig, OptimizedTrainerConfig, OptimizedBalancingConfig]:
        """
        –ü—Ä–µ—Å–µ—Ç –¥–ª—è production —Å –±–æ–ª—å—à–∏–º –æ–±—ä—ë–º–æ–º –¥–∞–Ω–Ω—ã—Ö (60+ –¥–Ω–µ–π).
        
        –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
        - –ë–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å (~500K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        - –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        - –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π learning rate
        """
        model_config = OptimizedModelConfig(
            cnn_channels=(64, 128, 256),
            lstm_hidden=256,
            dropout=0.3,
            use_residual=True,
            use_layer_norm=True,
            use_multi_head_attention=True
        )
        
        trainer_config = OptimizedTrainerConfig(
            epochs=100,
            learning_rate=1e-4,
            batch_size=128,
            weight_decay=0.005,
            label_smoothing=0.05,
            use_augmentation=True,
            mixup_alpha=0.1
        )
        
        balancing_config = OptimizedBalancingConfig(
            method=ClassBalancingMethod.FOCAL_WITH_WEIGHTS,
            focal_gamma=2.0,
            use_oversampling=True,
            oversample_ratio=0.3
        )
        
        return model_config, trainer_config, balancing_config
    
    @staticmethod
    def quick_experiment() -> Tuple[OptimizedModelConfig, OptimizedTrainerConfig, OptimizedBalancingConfig]:
        """
        –ü—Ä–µ—Å–µ—Ç –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ (5-10 –º–∏–Ω—É—Ç –æ–±—É—á–µ–Ω–∏—è).
        
        –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
        - –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
        - –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        - –ú–µ–Ω—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
        """
        model_config = OptimizedModelConfig(
            cnn_channels=(32, 64),
            lstm_hidden=64,
            lstm_layers=1,
            dropout=0.3,
            use_residual=False,
            use_layer_norm=False,
            use_multi_head_attention=False
        )
        
        trainer_config = OptimizedTrainerConfig(
            epochs=30,
            learning_rate=1e-4,
            batch_size=128,
            weight_decay=0.001,
            label_smoothing=0.0,
            use_augmentation=False,
            early_stopping_patience=10
        )
        
        balancing_config = OptimizedBalancingConfig(
            method=ClassBalancingMethod.FOCAL_LOSS,
            focal_gamma=2.0,
            use_oversampling=False
        )
        
        return model_config, trainer_config, balancing_config
    
    @staticmethod
    def conservative_trading() -> Tuple[OptimizedModelConfig, OptimizedTrainerConfig, OptimizedBalancingConfig]:
        """
        –ü—Ä–µ—Å–µ—Ç –¥–ª—è –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏ (–º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —Ä–∏—Å–∫–∞).
        
        –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
        - –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö
        - –°–∏–ª—å–Ω—ã–π –∞–∫—Ü–µ–Ω—Ç –Ω–∞ precision
        - –ú–µ–Ω—å—à–µ false positives
        """
        model_config = OptimizedModelConfig(
            cnn_channels=(32, 64, 128),
            lstm_hidden=128,
            dropout=0.5,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π dropout
            use_residual=True,
            use_layer_norm=True,
            use_multi_head_attention=True
        )
        
        trainer_config = OptimizedTrainerConfig(
            epochs=200,
            learning_rate=3e-5,  # –ï—â—ë –º–µ–Ω—å—à–µ LR
            batch_size=128,
            weight_decay=0.02,  # –ï—â—ë –±–æ–ª—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
            label_smoothing=0.15,
            use_augmentation=True,
            mixup_alpha=0.3
        )
        
        balancing_config = OptimizedBalancingConfig(
            method=ClassBalancingMethod.FOCAL_WITH_WEIGHTS,
            focal_gamma=3.0,  # –°–∏–ª—å–Ω–µ–µ —Ñ–æ–∫—É—Å –Ω–∞ hard examples
            use_oversampling=True,
            oversample_ratio=0.4,
            # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ thresholds
            threshold_percentile_sell=0.20,
            threshold_percentile_buy=0.80
        )
        
        return model_config, trainer_config, balancing_config


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def get_default_configs() -> Tuple[OptimizedModelConfig, OptimizedTrainerConfig, OptimizedBalancingConfig, OptimizedDataConfig]:
    """–ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
    model_config, trainer_config, balancing_config = ConfigPresets.production_small_data()
    data_config = OptimizedDataConfig()
    return model_config, trainer_config, balancing_config, data_config


def validate_configs(
    model_config: OptimizedModelConfig,
    trainer_config: OptimizedTrainerConfig,
    data_config: OptimizedDataConfig
) -> List[str]:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π.
    
    Returns:
        –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π (–ø—É—Å—Ç–æ–π –µ—Å–ª–∏ –≤—Å—ë OK)
    """
    warnings = []
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ batch_size
    if trainer_config.batch_size != data_config.batch_size:
        warnings.append(
            f"batch_size —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω: trainer={trainer_config.batch_size}, "
            f"data={data_config.batch_size}"
        )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ sequence_length
    if model_config.sequence_length != data_config.sequence_length:
        warnings.append(
            f"sequence_length —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω: model={model_config.sequence_length}, "
            f"data={data_config.sequence_length}"
        )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫ –¥–∞–Ω–Ω—ã–º
    estimated_params = model_config.get_estimated_params()
    if estimated_params > 500000:
        warnings.append(
            f"–ú–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è ({estimated_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤). "
            f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è < 500K –¥–ª—è –º–∞–ª—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤."
        )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ learning rate
    if trainer_config.learning_rate > 1e-4:
        warnings.append(
            f"Learning rate —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π ({trainer_config.learning_rate}). "
            f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 1e-5 - 1e-4 –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
        )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ weight_decay
    if trainer_config.weight_decay < 0.001:
        warnings.append(
            f"Weight decay —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π ({trainer_config.weight_decay}). "
            f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 0.001 - 0.01 –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è overfitting."
        )
    
    return warnings


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    model_cfg, trainer_cfg, balancing_cfg, data_cfg = get_default_configs()
    
    print("=" * 80)
    print("–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò ML –ú–û–î–ï–õ–ò")
    print("=" * 80)
    
    print("\nüìä Model Configuration:")
    for key, value in model_cfg.to_dict().items():
        print(f"  ‚Ä¢ {key}: {value}")
    
    print("\nüéì Trainer Configuration:")
    for key, value in trainer_cfg.to_dict().items():
        print(f"  ‚Ä¢ {key}: {value}")
    
    print("\n‚öñÔ∏è Balancing Configuration:")
    for key, value in balancing_cfg.to_dict().items():
        print(f"  ‚Ä¢ {key}: {value}")
    
    print("\nüìÅ Data Configuration:")
    for key, value in data_cfg.to_dict().items():
        print(f"  ‚Ä¢ {key}: {value}")
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    warnings = validate_configs(model_cfg, trainer_cfg, data_cfg)
    
    if warnings:
        print("\n‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:")
        for warning in warnings:
            print(f"  ‚Ä¢ {warning}")
    else:
        print("\n‚úÖ –í—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã!")
    
    print("\n" + "=" * 80)
