#!/usr/bin/env python3
"""
Training Orchestrator v2 - Industry Standard Integration.

–ì–ª–∞–≤–Ω—ã–π –º–æ–¥—É–ª—å, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã ML —Å–∏—Å—Ç–µ–º—ã:
- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (Feature Store –∏–ª–∏ legacy)
- –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
- –ú–æ–¥–µ–ª—å HybridCNNLSTM v2
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Trainer
- MLflow tracking
- Checkpoint management
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

–ü—É—Ç—å: backend/ml_engine/training_orchestrator_v2.py
"""

import os
import sys
import asyncio
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, Tuple, List
from dataclasses import dataclass, field, asdict
import json

import numpy as np
import torch
from torch.utils.data import DataLoader

from backend.core.logger import get_logger

logger = get_logger(__name__)


# ============================================================================
# CONFIGURATION
# ============================================================================

@dataclass
class OrchestratorConfig:
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Training Orchestrator.
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è.
    """
    
    # === Data Source ===
    use_feature_store: bool = True
    feature_store_days: int = 30  # –î–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö
    fallback_to_legacy: bool = True
    legacy_storage_path: str = "data/ml_training"
    
    # === Symbols ===
    symbols: List[str] = field(default_factory=lambda: ["BTCUSDT", "ETHUSDT"])
    auto_select_symbols: bool = False
    max_symbols: int = 50
    
    # === Model ===
    model_preset: str = "production_small"  # production_small, production_large, quick_experiment
    
    # === Training ===
    trainer_preset: str = "production_small"
    
    # === Balancing ===
    balancing_preset: str = "production"  # production, conservative, aggressive
    
    # === Output ===
    output_dir: str = "models/trained"
    experiment_name: str = "ml_training"
    
    # === MLflow ===
    use_mlflow: bool = True
    mlflow_tracking_uri: str = "mlruns"
    
    # === Device ===
    device: str = "auto"  # auto, cuda, cpu
    
    # === Reproducibility ===
    seed: int = 42
    
    def get_device(self) -> torch.device:
        if self.device == "auto":
            return torch.device("cuda" if torch.cuda.is_available() else "cpu")
        return torch.device(self.device)


# ============================================================================
# TRAINING ORCHESTRATOR V2
# ============================================================================

class TrainingOrchestratorV2:
    """
    Training Orchestrator v2 - –≥–ª–∞–≤–Ω—ã–π –º–æ–¥—É–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏–µ–º.
    
    Workflow:
    1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (Feature Store –∏–ª–∏ legacy)
    2. –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    3. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
    4. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    5. –û–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    6. –û—Ü–µ–Ω–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    7. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
    """
    
    def __init__(self, config: Optional[OrchestratorConfig] = None):
        """
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞
        """
        self.config = config or OrchestratorConfig()
        self.device = self.config.get_device()
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed
        self._set_seed(self.config.seed)
        
        # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.output_dir = Path(self.config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ)
        self.model = None
        self.trainer = None
        self.data_loader = None
        self.balancing_strategy = None
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
        self.training_metadata: Dict[str, Any] = {}
        
        logger.info(
            f"‚úì TrainingOrchestratorV2 initialized:\n"
            f"  ‚Ä¢ Device: {self.device}\n"
            f"  ‚Ä¢ Feature Store: {self.config.use_feature_store}\n"
            f"  ‚Ä¢ Model preset: {self.config.model_preset}\n"
            f"  ‚Ä¢ Trainer preset: {self.config.trainer_preset}\n"
            f"  ‚Ä¢ Output: {self.output_dir}"
        )
    
    def _set_seed(self, seed: int):
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏."""
        import random
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
    
    async def run_training(
        self,
        symbols: Optional[List[str]] = None,
        days: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è.
        
        Args:
            symbols: –°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ (–µ—Å–ª–∏ None - –∏–∑ config)
            days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ None - –∏–∑ config)
        
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        symbols = symbols or self.config.symbols
        days = days or self.config.feature_store_days
        
        logger.info("\n" + "=" * 80)
        logger.info("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø ML –ú–û–î–ï–õ–ò")
        logger.info("=" * 80)
        logger.info(f"–°–∏–º–≤–æ–ª—ã: {symbols}")
        logger.info(f"–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {days} –¥–Ω–µ–π")
        logger.info(f"Device: {self.device}")
        logger.info("=" * 80 + "\n")
        
        start_time = datetime.now()
        
        try:
            # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            logger.info("üì• –≠—Ç–∞–ø 1/6: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            train_loader, val_loader, test_loader, data_stats = await self._load_data(
                symbols, days
            )
            
            if train_loader is None:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # 2. –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
            logger.info("\nüìä –≠—Ç–∞–ø 2/6: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö...")
            data_analysis = self._analyze_data(train_loader, val_loader)
            
            # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
            logger.info("\n‚öñÔ∏è –≠—Ç–∞–ø 3/6: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤...")
            self._setup_balancing(data_analysis)
            
            # 4. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            logger.info("\nüèóÔ∏è –≠—Ç–∞–ø 4/6: –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
            self._create_model(data_analysis)
            
            # 5. –û–±—É—á–µ–Ω–∏–µ
            logger.info("\nüéì –≠—Ç–∞–ø 5/6: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
            training_results = self._train_model(train_loader, val_loader, test_loader)
            
            # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            logger.info("\nüíæ –≠—Ç–∞–ø 6/6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
            save_results = self._save_model(training_results)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç
            end_time = datetime.now()
            total_time = (end_time - start_time).total_seconds()
            
            results = {
                'status': 'success',
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'total_time_seconds': total_time,
                'data_stats': data_stats,
                'data_analysis': data_analysis,
                'training_results': training_results,
                'save_results': save_results,
                'model_path': save_results.get('model_path'),
                'best_metrics': {
                    'val_loss': self.trainer.best_val_loss if self.trainer else None,
                    'val_f1': self.trainer.best_val_f1 if self.trainer else None
                }
            }
            
            self._log_final_results(results)
            
            return results
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            logger.exception("–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏:")
            
            return {
                'status': 'error',
                'error': str(e),
                'start_time': start_time.isoformat(),
                'end_time': datetime.now().isoformat()
            }
    
    async def _load_data(
        self,
        symbols: List[str],
        days: int
    ) -> Tuple[Optional[DataLoader], Optional[DataLoader], Optional[DataLoader], Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Feature Store –∏–ª–∏ legacy."""
        
        data_stats = {
            'symbols': symbols,
            'days': days,
            'source': None,
            'total_samples': 0
        }
        
        # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ Feature Store
        if self.config.use_feature_store:
            try:
                logger.info("–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ Feature Store...")
                
                from backend.ml_engine.feature_store.feature_store import get_feature_store
                from backend.ml_engine.feature_store.feature_schema import DEFAULT_SCHEMA
                from backend.ml_engine.training.data_loader import HistoricalDataLoader, DataConfig
                
                feature_store = get_feature_store()
                await feature_store.initialize()
                
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                end_date = datetime.now()
                start_date = end_date - timedelta(days=days)
                
                features_df = await feature_store.get_training_data(
                    symbols=symbols,
                    start_date=start_date,
                    end_date=end_date
                )
                
                if features_df is not None and len(features_df) > 0:
                    logger.info(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(features_df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ Feature Store")
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataLoaders
                    data_config = DataConfig(
                        batch_size=256,  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π batch size
                        sequence_length=60
                    )
                    
                    data_loader = HistoricalDataLoader(data_config)
                    
                    train_loader, val_loader, test_loader = data_loader.load_from_dataframe(
                        features_df=features_df,
                        feature_columns=DEFAULT_SCHEMA.get_all_feature_columns(),
                        label_column=DEFAULT_SCHEMA.label_column,
                        timestamp_column=DEFAULT_SCHEMA.timestamp_column,
                        apply_resampling=True
                    )
                    
                    data_stats['source'] = 'feature_store'
                    data_stats['total_samples'] = len(features_df)
                    
                    return train_loader, val_loader, test_loader, data_stats
                
            except Exception as e:
                logger.warning(f"Feature Store –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        
        # Fallback –∫ legacy
        if self.config.fallback_to_legacy:
            logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ legacy storage...")
            
            try:
                from backend.ml_engine.training.data_loader import HistoricalDataLoader, DataConfig
                
                data_config = DataConfig(
                    storage_path=self.config.legacy_storage_path,
                    batch_size=256,
                    sequence_length=60
                )
                
                data_loader = HistoricalDataLoader(data_config)
                train_loader, val_loader, test_loader = data_loader.load_and_split(
                    symbols=symbols
                )
                
                if train_loader is not None:
                    data_stats['source'] = 'legacy'
                    data_stats['total_samples'] = len(train_loader.dataset)
                    
                    logger.info(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ legacy: {data_stats['total_samples']} samples")
                    
                    return train_loader, val_loader, test_loader, data_stats
                
            except Exception as e:
                logger.error(f"Legacy –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
        
        return None, None, None, data_stats
    
    def _analyze_data(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ labels
        all_labels = []
        for batch in train_loader:
            all_labels.extend(batch['label'].numpy())
        
        all_labels = np.array(all_labels)
        
        from collections import Counter
        class_dist = Counter(all_labels)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_samples = len(all_labels)
        num_classes = len(class_dist)
        
        # Imbalance ratio
        max_count = max(class_dist.values())
        min_count = min(class_dist.values())
        imbalance_ratio = max_count / max(min_count, 1)
        
        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ samples/params
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
        estimated_params = self._estimate_model_params()
        samples_per_param = total_samples / max(estimated_params, 1)
        
        analysis = {
            'total_samples': total_samples,
            'num_classes': num_classes,
            'class_distribution': dict(class_dist),
            'imbalance_ratio': imbalance_ratio,
            'estimated_params': estimated_params,
            'samples_per_param': samples_per_param,
            'train_batches': len(train_loader),
            'val_batches': len(val_loader),
            'recommendations': []
        }
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if imbalance_ratio > 3:
            analysis['recommendations'].append(
                f"–í—ã—Å–æ–∫–∏–π imbalance ({imbalance_ratio:.1f}x). "
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è Focal Loss —Å gamma>=2.5 –∏ oversampling."
            )
        
        if samples_per_param < 50:
            analysis['recommendations'].append(
                f"–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä ({samples_per_param:.1f}). "
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–º–µ–Ω—å—à–∏—Ç—å –º–æ–¥–µ–ª—å –∏ —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é."
            )
        
        if total_samples < 10000:
            analysis['recommendations'].append(
                "–ú–∞–ª—ã–π –¥–∞—Ç–∞—Å–µ—Ç (<10K samples). "
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å preset 'production_small'."
            )
        
        logger.info(f"\nüìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö:")
        logger.info(f"  ‚Ä¢ Total samples: {total_samples:,}")
        logger.info(f"  ‚Ä¢ Class distribution: {dict(class_dist)}")
        logger.info(f"  ‚Ä¢ Imbalance ratio: {imbalance_ratio:.2f}x")
        logger.info(f"  ‚Ä¢ Samples per param: {samples_per_param:.1f}")
        
        if analysis['recommendations']:
            logger.info(f"\nüìã –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
            for rec in analysis['recommendations']:
                logger.info(f"  ‚Ä¢ {rec}")
        
        return analysis
    
    def _estimate_model_params(self) -> int:
        """–û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏."""
        preset = self.config.model_preset
        
        if preset == "production_small":
            return 150000  # ~150K params
        elif preset == "production_large":
            return 500000  # ~500K params
        elif preset == "quick_experiment":
            return 50000   # ~50K params
        else:
            return 200000  # Default estimate
    
    def _setup_balancing(self, data_analysis: Dict[str, Any]):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏."""
        
        from backend.ml_engine.training.class_balancing_v2 import (
            create_balancing_strategy,
            ClassBalancingConfigV2,
            BalancingMethod
        )
        
        # –í—ã–±–∏—Ä–∞–µ–º preset –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        imbalance = data_analysis.get('imbalance_ratio', 1.0)
        
        if imbalance > 5:
            preset = "aggressive"
        elif imbalance > 2:
            preset = "production"
        else:
            preset = "conservative"
        
        # Override –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –≤ config
        preset = self.config.balancing_preset or preset
        
        self.balancing_strategy = create_balancing_strategy(preset)
        
        logger.info(f"‚úì Balancing strategy: {preset}")
    
    def _create_model(self, data_analysis: Dict[str, Any]):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö."""
        
        from backend.ml_engine.models.hybrid_cnn_lstm_v2 import (
            create_model_v2_from_preset
        )
        
        preset = self.config.model_preset
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä preset –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö
        samples = data_analysis.get('total_samples', 0)
        
        if samples < 20000 and preset == "auto":
            preset = "production_small"
        elif samples >= 20000 and preset == "auto":
            preset = "production_large"
        
        self.model = create_model_v2_from_preset(preset)
        self.model.to(self.device)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
        model_size = self.model.get_model_size()
        logger.info(f"‚úì Model created: {preset}")
        logger.info(f"  ‚Ä¢ Total params: {model_size['total_params']:,}")
        logger.info(f"  ‚Ä¢ Trainable params: {model_size['trainable_params']:,}")
    
    def _train_model(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: Optional[DataLoader]
    ) -> Dict[str, Any]:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏."""
        
        from backend.ml_engine.training.model_trainer_v2 import (
            create_trainer_v2,
            TrainerConfigV2
        )
        
        # –°–æ–∑–¥–∞—ë–º trainer —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        trainer_config = TrainerConfigV2(
            epochs=150,
            learning_rate=5e-5,
            batch_size=256,
            weight_decay=0.01,
            label_smoothing=0.1,
            use_augmentation=True,
            mixup_alpha=0.2,
            focal_gamma=2.5,
            early_stopping_patience=20,
            checkpoint_dir=str(self.output_dir / "checkpoints"),
            device=str(self.device)
        )
        
        self.trainer = create_trainer_v2(self.model, trainer_config)
        
        # –û–±—É—á–µ–Ω–∏–µ
        history = self.trainer.train(
            train_loader=train_loader,
            val_loader=val_loader,
            test_loader=test_loader
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = {
            'epochs_trained': len(history),
            'best_val_loss': self.trainer.best_val_loss,
            'best_val_f1': self.trainer.best_val_f1,
            'final_metrics': history[-1].to_dict() if history else {},
            'history': [m.to_dict() for m in history]
        }
        
        return results
    
    def _save_model(self, training_results: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = f"hybrid_cnn_lstm_v2_{timestamp}"
        model_path = self.output_dir / f"{model_name}.pt"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'config': asdict(self.config),
            'training_results': training_results,
            'timestamp': timestamp
        }, model_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_path = self.output_dir / f"{model_name}_metadata.json"
        
        metadata = {
            'model_name': model_name,
            'timestamp': timestamp,
            'config': asdict(self.config),
            'training_results': {
                'epochs_trained': training_results['epochs_trained'],
                'best_val_loss': training_results['best_val_loss'],
                'best_val_f1': training_results['best_val_f1']
            }
        }
        
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)
        
        logger.info(f"‚úì Model saved: {model_path}")
        logger.info(f"‚úì Metadata saved: {metadata_path}")
        
        return {
            'model_path': str(model_path),
            'metadata_path': str(metadata_path),
            'model_name': model_name
        }
    
    def _log_final_results(self, results: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        
        logger.info("\n" + "=" * 80)
        logger.info("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        logger.info("=" * 80)
        
        if results['status'] == 'success':
            logger.info(f"‚úÖ –°—Ç–∞—Ç—É—Å: –£—Å–ø–µ—Ö")
            logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è: {results['total_time_seconds']:.1f} —Å–µ–∫—É–Ω–¥")
            
            if results.get('best_metrics'):
                logger.info(f"\nüìä –õ—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏:")
                logger.info(f"  ‚Ä¢ Val Loss: {results['best_metrics']['val_loss']:.4f}")
                logger.info(f"  ‚Ä¢ Val F1: {results['best_metrics']['val_f1']:.4f}")
            
            if results.get('model_path'):
                logger.info(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {results['model_path']}")
        else:
            logger.error(f"‚ùå –°—Ç–∞—Ç—É—Å: –û—à–∏–±–∫–∞")
            logger.error(f"–û—à–∏–±–∫–∞: {results.get('error', 'Unknown')}")
        
        logger.info("=" * 80 + "\n")


# ============================================================================
# CLI INTERFACE
# ============================================================================

async def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è CLI."""
    import argparse
    
    parser = argparse.ArgumentParser(description="ML Training Orchestrator v2")
    
    parser.add_argument(
        "--symbols",
        nargs="+",
        default=["BTCUSDT", "ETHUSDT"],
        help="–¢–æ—Ä–≥–æ–≤—ã–µ –ø–∞—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
    )
    parser.add_argument(
        "--days",
        type=int,
        default=30,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö"
    )
    parser.add_argument(
        "--model-preset",
        choices=["production_small", "production_large", "quick_experiment"],
        default="production_small",
        help="–ü—Ä–µ—Å–µ—Ç –º–æ–¥–µ–ª–∏"
    )
    parser.add_argument(
        "--output-dir",
        default="models/trained",
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"
    )
    parser.add_argument(
        "--device",
        choices=["auto", "cuda", "cpu"],
        default="auto",
        help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
    )
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞—ë–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = OrchestratorConfig(
        symbols=args.symbols,
        feature_store_days=args.days,
        model_preset=args.model_preset,
        output_dir=args.output_dir,
        device=args.device
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    orchestrator = TrainingOrchestratorV2(config)
    results = await orchestrator.run_training()
    
    return results


if __name__ == "__main__":
    asyncio.run(main())
