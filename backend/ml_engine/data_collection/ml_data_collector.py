"""
ML Data Collector - —Å–∏—Å—Ç–µ–º–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π.

–°–æ–±–∏—Ä–∞–µ—Ç:
- Feature vectors (112 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
- Market state (orderbook snapshot, –º–µ—Ç—Ä–∏–∫–∏)
- Labels (future price movement, signals)

–§–∞–π–ª: backend/ml_engine/data_collection/ml_data_collector.py
"""

import os
import json
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime
from pathlib import Path
import numpy as np
import pandas as pd

from backend.core.logger import get_logger
from backend.ml_engine.features import FeatureVector
from backend.models.orderbook import OrderBookSnapshot, OrderBookMetrics

logger = get_logger(__name__)


class MLDataCollector:
  """
  –°–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML –æ–±—É—á–µ–Ω–∏—è.

  –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è:
  data/ml_training/
    ‚îú‚îÄ‚îÄ BTCUSDT/
    ‚îÇ   ‚îú‚îÄ‚îÄ features/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2025-01-15_batch_0001.npy  # –ú–∞—Å—Å–∏–≤—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2025-01-15_batch_0002.npy
    ‚îÇ   ‚îú‚îÄ‚îÄ labels/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2025-01-15_batch_0001.npy  # –ú–µ—Ç–∫–∏ (—Ç–∞—Ä–≥–µ—Ç—ã)
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2025-01-15_batch_0002.npy
    ‚îÇ   ‚îî‚îÄ‚îÄ metadata/
    ‚îÇ       ‚îú‚îÄ‚îÄ 2025-01-15_batch_0001.json  # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    ‚îÇ       ‚îî‚îÄ‚îÄ 2025-01-15_batch_0002.json
    ‚îî‚îÄ‚îÄ ETHUSDT/
        ‚îî‚îÄ‚îÄ ...
  """

  def __init__(
      self,
      storage_path: str = "data/ml_training",
      max_samples_per_file: int = 2000,  # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û: 10000 ‚Üí 2000 (~2 MB/—Ñ–∞–π–ª)
      collection_interval: int = 10,
      # auto_save_interval_seconds: int = 40000,# –ö–∞–∂–¥—ã–µ N –∏—Ç–µ—Ä–∞—Ü–∏–π
      max_buffer_memory_mb: int = 200,  # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û: 200 ‚Üí 80 (–∑–∞–ø–∞—Å –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç–∏)
      # –ù–û–í–û–ï: Feature Store integration
      enable_feature_store: bool = True,  # –ó–∞–ø–∏—Å—ã–≤–∞—Ç—å –≤ Feature Store (parquet)
      use_legacy_format: bool = True,     # –ó–∞–ø–∏—Å—ã–≤–∞—Ç—å –≤ legacy —Ñ–æ—Ä–º–∞—Ç (.npy/.json)
      feature_store_group: str = "training_features"
  ):
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä—â–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö.

    Args:
        storage_path: –ü—É—Ç—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (legacy —Ñ–æ—Ä–º–∞—Ç)
        max_samples_per_file: –ú–∞–∫—Å–∏–º—É–º —Å–µ–º–ø–ª–æ–≤ –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ
        collection_interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–±–æ—Ä–∞ (–∫–∞–∂–¥—ã–µ N –∏—Ç–µ—Ä–∞—Ü–∏–π)
        # auto_save_interval_seconds: –ò–Ω—Ç–µ—Ä–≤–∞–ª –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)
        max_buffer_memory_mb: –ú–∞–∫—Å–∏–º—É–º –ø–∞–º—è—Ç–∏ –Ω–∞ –±—É—Ñ–µ—Ä —Å–∏–º–≤–æ–ª–∞ (–ú–ë)
        enable_feature_store: –ó–∞–ø–∏—Å—ã–≤–∞—Ç—å –ª–∏ –≤ Feature Store (parquet)
        use_legacy_format: –ó–∞–ø–∏—Å—ã–≤–∞—Ç—å –ª–∏ –≤ legacy —Ñ–æ—Ä–º–∞—Ç (.npy/.json)
        feature_store_group: –ù–∞–∑–≤–∞–Ω–∏–µ feature group –¥–ª—è Feature Store
    """
    self.storage_path = Path(storage_path)
    self.max_samples_per_file = max_samples_per_file
    self.initial_max_samples_per_file = max_samples_per_file  # –ù–û–í–û–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
    self.collection_interval = collection_interval
    # self.auto_save_interval = auto_save_interval_seconds
    self.max_buffer_memory_mb = max_buffer_memory_mb

    # Feature Store integration
    self.enable_feature_store = enable_feature_store
    self.use_legacy_format = use_legacy_format
    self.feature_store_group = feature_store_group


    # –ë—É—Ñ–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    self.feature_buffers: Dict[str, List[np.ndarray]] = {}
    self.label_buffers: Dict[str, List[Dict[str, Any]]] = {}
    self.metadata_buffers: Dict[str, List[Dict[str, Any]]] = {}

    # –°—á–µ—Ç—á–∏–∫–∏
    self.sample_counts: Dict[str, int] = {}
    self.batch_numbers: Dict[str, int] = {}
    self.last_save_time: Dict[str, float] = {}  # –í—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    self.iteration_counter = 0
    self.last_cleanup_iteration = 0  # –ù–û–í–û–ï: –°—á–µ—Ç—á–∏–∫ –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    self.total_samples_collected = 0
    self.files_written = 0

    # Feature Store (lazy initialization)
    self._feature_store = None

    logger.info(
      f"MLDataCollector –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, storage_path={storage_path}, "
      f"max_samples={max_samples_per_file}, interval={collection_interval}, "
      f"feature_store={'‚úÖ' if enable_feature_store else '‚ùå'}, "
      f"legacy={'‚úÖ' if use_legacy_format else '‚ùå'}"
    )

  async def initialize(self):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""
    try:
      # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
      self.storage_path.mkdir(parents=True, exist_ok=True)
      logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {self.storage_path}")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {e}")
      raise

  def should_collect(self) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω—É–∂–Ω–æ –ª–∏ —Å–æ–±–∏—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —ç—Ç–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏.

    Returns:
        bool: True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å
    """
    self.iteration_counter += 1
    return self.iteration_counter % self.collection_interval == 0

  async def collect_sample(
      self,
      symbol: str,
      feature_vector: FeatureVector,
      orderbook_snapshot: OrderBookSnapshot,
      market_metrics: OrderBookMetrics,
      executed_signal: Optional[Dict[str, Any]] = None
  ):
    """
    –°–±–æ—Ä –æ–¥–Ω–æ–≥–æ —Å–µ–º–ø–ª–∞ –¥–∞–Ω–Ω—ã—Ö.

    Args:
        symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
        feature_vector: –í–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (112 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        orderbook_snapshot: –°–Ω–∏–º–æ–∫ —Å—Ç–∞–∫–∞–Ω–∞
        market_metrics: –†—ã–Ω–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        executed_signal: –ò—Å–ø–æ–ª–Ω–µ–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª (–µ—Å–ª–∏ –µ—Å—Ç—å)
    """
    try:
      # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±—É—Ñ–µ—Ä—ã –¥–ª—è —Å–∏–º–≤–æ–ª–∞
      if symbol not in self.feature_buffers:
        self.feature_buffers[symbol] = []
        self.label_buffers[symbol] = []
        self.metadata_buffers[symbol] = []
        self.sample_counts[symbol] = 0
        self.batch_numbers[symbol] = 1
        self.last_save_time[symbol] = datetime.now().timestamp()

      # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞—Å—Å–∏–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
      features_array = feature_vector.to_array()

      # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫—É (label) –¥–ª—è supervised learning
      # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —Ä–∞—Å—á–µ—Ç–∞ future price movement
      label = self._create_label(
        orderbook_snapshot,
        market_metrics,
        executed_signal
      )

      # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
      metadata = {
        "timestamp": orderbook_snapshot.timestamp,
        "symbol": symbol,
        "mid_price": orderbook_snapshot.mid_price,
        "spread": orderbook_snapshot.spread,
        "imbalance": market_metrics.imbalance,
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏–≥–Ω–∞–ª–µ
        "signal_type": executed_signal.get("type") if executed_signal else None,
        "signal_confidence": executed_signal.get("confidence") if executed_signal else None,
        "signal_strength": executed_signal.get("strength") if executed_signal else None,
        "feature_count": feature_vector.feature_count
      }

      # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±—É—Ñ–µ—Ä—ã
      self.feature_buffers[symbol].append(features_array)
      self.label_buffers[symbol].append(label)
      self.metadata_buffers[symbol].append(metadata)

      self.sample_counts[symbol] += 1
      self.total_samples_collected += 1

      # üîß –ê–î–ê–ü–¢–ò–í–ù–ê–Ø –ü–û–î–°–¢–†–û–ô–ö–ê (–∫–∞–∂–¥—ã–µ 100 —Å–µ–º–ø–ª–æ–≤)
      if self.total_samples_collected % 100 == 0:
        self._adapt_thresholds()

      # üî• –ü–†–û–ê–ö–¢–ò–í–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ü–ê–ú–Ø–¢–ò (–ö–†–ò–¢–ò–ß–ù–û!)
      # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ü–û–°–õ–ï –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–µ–º–ø–ª–∞, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—è —Å–æ—Ö—Ä–∞–Ω–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
      buffer_size = len(self.feature_buffers[symbol])
      buffer_memory_mb = self._calculate_buffer_memory(symbol)
      memory_threshold_mb = self.max_buffer_memory_mb * 0.9  # 90% –ø–æ—Ä–æ–≥

      logger.debug(
        f"{symbol} | –°–æ–±—Ä–∞–Ω —Å–µ–º–ø–ª #{self.sample_counts[symbol]}, "
        f"–±—É—Ñ–µ—Ä: {buffer_size}/{self.max_samples_per_file}, "
        f"–ø–∞–º—è—Ç—å: {buffer_memory_mb:.2f}MB/{self.max_buffer_memory_mb}MB"
      )

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å batch (–ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ò–õ–ò –ø–æ –ø–∞–º—è—Ç–∏)
      should_save = False
      save_reason = ""

      if buffer_size >= self.max_samples_per_file:
        should_save = True
        save_reason = f"–ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç —Å–µ–º–ø–ª–æ–≤ ({buffer_size}/{self.max_samples_per_file})"
      elif buffer_memory_mb >= memory_threshold_mb:
        should_save = True
        save_reason = f"–ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏ ({buffer_memory_mb:.2f}MB/{memory_threshold_mb:.2f}MB)"

      if should_save:
        logger.info(f"{symbol} | üíæ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {save_reason}")
        await self._save_batch(symbol)

    except Exception as e:
      logger.error(f"{symbol} | –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Å–µ–º–ø–ª–∞: {e}")

  def _create_label(
        self,
        orderbook_snapshot: OrderBookSnapshot,
        market_metrics: OrderBookMetrics,
        executed_signal: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
      """
      –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∫–∏ (label) –¥–ª—è supervised learning.

      Future targets (direction, movement) –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –ü–û–ó–ñ–ï
      —á–µ—Ä–µ–∑ preprocessing —Å–∫—Ä–∏–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.

      –í —Ä–µ–∂–∏–º–µ real-time –º—ã –ù–ï –ú–û–ñ–ï–ú –∑–Ω–∞—Ç—å –±—É–¥—É—â—É—é —Ü–µ–Ω—É,
      –ø–æ—ç—Ç–æ–º—É —Å–µ–π—á–∞—Å —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ.

      Args:
          orderbook_snapshot: –°–Ω–∏–º–æ–∫ —Å—Ç–∞–∫–∞–Ω–∞
          market_metrics: –ú–µ—Ç—Ä–∏–∫–∏
          executed_signal: –ò—Å–ø–æ–ª–Ω–µ–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª

      Returns:
          Dict: –ú–µ—Ç–∫–∞ —Å —Ç–µ–∫—É—â–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –∏ –∑–∞–≥–ª—É—à–∫–∞–º–∏ –¥–ª—è future targets
      """
      label = {
        # ===== FUTURE TARGETS (–∑–∞–ø–æ–ª–Ω—è—Ç—Å—è —á–µ—Ä–µ–∑ preprocessing) =====
        # –≠—Ç–∏ –ø–æ–ª—è –±—É–¥—É—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –ü–û–°–õ–ï —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö,
        # –∫–æ–≥–¥–∞ –º—ã –±—É–¥–µ–º –∑–Ω–∞—Ç—å, —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ —Å —Ü–µ–Ω–æ–π —á–µ—Ä–µ–∑ N —Å–µ–∫—É–Ω–¥
        "future_direction_10s": None,  # –ü–æ—Å–ª–µ preprocessing: 0=DOWN, 1=NEUTRAL, 2=UP
        "future_direction_30s": None,
        "future_direction_60s": None,
        "future_movement_10s": None,  # % –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã
        "future_movement_30s": None,
        "future_movement_60s": None,

        # Current state
        "timestamp": orderbook_snapshot.timestamp,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –¥–æ–±–∞–≤–ª–µ–Ω timestamp –¥–ª—è preprocessing
        "current_mid_price": orderbook_snapshot.mid_price,
        "current_imbalance": market_metrics.imbalance,

        # ===== –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–Æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏–≥–Ω–∞–ª–µ =====
        "signal_type": executed_signal.get("type") if executed_signal else None,
        "signal_confidence": executed_signal.get("confidence") if executed_signal else None,
        "signal_strength": executed_signal.get("strength") if executed_signal else None,
      }

      return label

  async def _save_batch(self, symbol: str):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ batch –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Å–∫.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç legacy —Ñ–æ—Ä–º–∞—Ç (.npy/.json) –∏ Feature Store (parquet).

    Args:
        symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
    """
    try:
      if not self.feature_buffers[symbol]:
        return

      # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Feature Store (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
      if self.enable_feature_store:
        await self._save_to_feature_store(symbol)

      # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ legacy —Ñ–æ—Ä–º–∞—Ç (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
      if self.use_legacy_format:
        await self._save_legacy_batch(symbol)

      # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä—ã (–ø–æ—Å–ª–µ –æ–±–æ–∏—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–π)
      buffer_size = len(self.feature_buffers[symbol])
      self.feature_buffers[symbol].clear()
      self.label_buffers[symbol].clear()
      self.metadata_buffers[symbol].clear()

      # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∏—Ä—É–µ–º batch number
      self.batch_numbers[symbol] += 1

      logger.info(
        f"‚úì {symbol} | Batch —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {buffer_size} —Å–µ–º–ø–ª–æ–≤ "
        f"(FS={'‚úÖ' if self.enable_feature_store else '‚ùå'}, "
        f"Legacy={'‚úÖ' if self.use_legacy_format else '‚ùå'})"
      )

    except Exception as e:
      logger.error(f"{symbol} | –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è batch: {e}", exc_info=True)

  async def _save_legacy_batch(self, symbol: str):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ legacy —Ñ–æ—Ä–º–∞—Ç (.npy/.json).

    Args:
        symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
    """
    try:
      # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–∏–º–≤–æ–ª–∞
      symbol_dir = self.storage_path / symbol
      features_dir = symbol_dir / "features"
      labels_dir = symbol_dir / "labels"
      metadata_dir = symbol_dir / "metadata"

      for dir_path in [features_dir, labels_dir, metadata_dir]:
        dir_path.mkdir(parents=True, exist_ok=True)

      # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
      date_str = datetime.now().strftime("%Y-%m-%d")
      batch_num = self.batch_numbers[symbol]
      filename_base = f"{date_str}_batch_{batch_num:04d}"

      # –°–æ—Ö—Ä–∞–Ω—è–µ–º features (numpy)
      features_array = np.array(self.feature_buffers[symbol])
      features_file = features_dir / f"{filename_base}.npy"
      np.save(features_file, features_array)

      # –°–æ—Ö—Ä–∞–Ω—è–µ–º labels (json)
      labels_file = labels_dir / f"{filename_base}.json"
      with open(labels_file, 'w') as f:
        json.dump(self.label_buffers[symbol], f, indent=2)

      # –°–æ—Ö—Ä–∞–Ω—è–µ–º metadata (json)
      metadata_file = metadata_dir / f"{filename_base}.json"
      with open(metadata_file, 'w') as f:
        json.dump({
          "batch_info": {
            "symbol": symbol,
            "batch_number": batch_num,
            "sample_count": len(self.feature_buffers[symbol]),
            "timestamp": datetime.now().isoformat(),
            "feature_shape": features_array.shape
          },
          "samples": self.metadata_buffers[symbol]
        }, f, indent=2)

      self.files_written += 3  # features, labels, metadata

      logger.info(
        f"{symbol} | Legacy batch #{batch_num}: "
        f"{len(self.feature_buffers[symbol])} —Å–µ–º–ø–ª–æ–≤, "
        f"shape={features_array.shape}"
      )

    except Exception as e:
      logger.error(f"{symbol} | –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è legacy batch: {e}", exc_info=True)

  def _get_feature_store(self):
    """Lazy initialization Feature Store"""
    if self._feature_store is None:
      from backend.ml_engine.feature_store.feature_store import get_feature_store
      self._feature_store = get_feature_store()
    return self._feature_store

  async def _save_to_feature_store(self, symbol: str):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Feature Store (parquet —Ñ–æ—Ä–º–∞—Ç).

    Args:
        symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
    """
    try:
      if not self.feature_buffers[symbol]:
        return

      logger.info(f"{symbol} | –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Feature Store...")

      # –ü–æ–ª—É—á–∞–µ–º –±—É—Ñ–µ—Ä—ã
      features_list = self.feature_buffers[symbol]
      labels_list = self.label_buffers[symbol]
      metadata_list = self.metadata_buffers[symbol]

      # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∏–∑ FeatureStoreSchema
      from backend.ml_engine.feature_store.feature_schema import DEFAULT_SCHEMA
      feature_column_names = DEFAULT_SCHEMA.get_all_feature_columns()

      if len(feature_column_names) != 112:
        logger.error(
          f"Feature schema mismatch: expected 112 columns, got {len(feature_column_names)}"
        )
        return

      # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
      rows = []

      for feature_arr, label_dict, meta_dict in zip(features_list, labels_list, metadata_list):
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        row = {
          'symbol': symbol,
          'timestamp': meta_dict['timestamp'],
        }

        # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º 112 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú–ò –ù–ê–ó–í–ê–ù–ò–Ø–ú–ò
        # feature_arr –∏–º–µ–µ—Ç shape (112,)
        if len(feature_arr) != 112:
          logger.warning(
            f"{symbol} | Feature array length mismatch: expected 112, got {len(feature_arr)}"
          )
          continue

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∏–∑ —Å—Ö–µ–º—ã
        for i, feature_name in enumerate(feature_column_names):
          row[feature_name] = feature_arr[i]

        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ (labels)
        # –ú–µ—Ç–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å None (–±—É–¥—É—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –ø–æ–∑–∂–µ —á–µ—Ä–µ–∑ preprocessing)
        row['future_direction_10s'] = label_dict.get('future_direction_10s')
        row['future_direction_30s'] = label_dict.get('future_direction_30s')
        row['future_direction_60s'] = label_dict.get('future_direction_60s')
        row['future_movement_10s'] = label_dict.get('future_movement_10s')
        row['future_movement_30s'] = label_dict.get('future_movement_30s')
        row['future_movement_60s'] = label_dict.get('future_movement_60s')

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º current_mid_price –¥–ª—è preprocessing
        row['current_mid_price'] = label_dict.get('current_mid_price')

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        row['signal_type'] = meta_dict.get('signal_type')
        row['signal_confidence'] = meta_dict.get('signal_confidence')
        row['signal_strength'] = meta_dict.get('signal_strength')

        rows.append(row)

      if not rows:
        logger.warning(f"{symbol} | No valid rows to save")
        return

      # –°–æ–∑–¥–∞–µ–º DataFrame
      df = pd.DataFrame(rows)

      # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ timestamp
      df = df.sort_values('timestamp').reset_index(drop=True)

      # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–æ–Ω–∫–∞—Ö –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
      logger.info(
        f"{symbol} | DataFrame columns: {len(df.columns)}, "
        f"feature columns: {len([c for c in df.columns if c in feature_column_names])}"
      )

      # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ Feature Store
      feature_store = self._get_feature_store()
      success = feature_store.write_offline_features(
        feature_group=self.feature_store_group,
        features=df,
        timestamp_column='timestamp'
      )

      if success:
        logger.info(
          f"‚úì {symbol} | Feature Store: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} —Å–µ–º–ø–ª–æ–≤ –≤ parquet —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–æ–ª–æ–Ω–æ–∫"
        )
      else:
        logger.error(f"{symbol} | –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ Feature Store")

      # CRITICAL: Explicitly delete DataFrame to free memory
      # Pandas does not release memory automatically
      # NOTE: gc.collect() removed - too slow (8 sec per save)
      # GC will collect automatically during periodic cleanup
      del df

    except Exception as e:
      logger.error(f"{symbol} | –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Feature Store: {e}", exc_info=True)

  async def finalize(self):
    """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –±—É—Ñ–µ—Ä–æ–≤."""
    logger.info("–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è MLDataCollector...")

    # –í–ê–ñ–ù–û: –ü—Ä–∏ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–ï –±—É—Ñ–µ—Ä—ã, –¥–∞–∂–µ –º–∞–ª–µ–Ω—å–∫–∏–µ (min_buffer_size=0)
    await self._emergency_save_all_buffers(min_buffer_size=0)

    logger.info(
      f"MLDataCollector —Ñ–∏–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: "
      f"–≤—Å–µ–≥–æ —Å–µ–º–ø–ª–æ–≤={self.total_samples_collected}, "
      f"—Ñ–∞–π–ª–æ–≤={self.files_written}"
    )

  def _calculate_buffer_memory(self, symbol: str) -> float:
    """
    –†–∞—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –±—É—Ñ–µ—Ä–∞ –≤ –ø–∞–º—è—Ç–∏ (–ú–ë).

    Args:
        symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞

    Returns:
        float: –†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –≤ –ú–ë
    """
    if symbol not in self.feature_buffers:
      return 0.0

    try:
      # –†–∞–∑–º–µ—Ä feature –±—É—Ñ–µ—Ä–∞
      features_size = 0
      for arr in self.feature_buffers[symbol]:
        features_size += arr.nbytes

      # –†–∞–∑–º–µ—Ä label –±—É—Ñ–µ—Ä–∞ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
      labels_size = len(self.label_buffers[symbol]) * 200  # ~200 bytes per label

      # –†–∞–∑–º–µ—Ä metadata –±—É—Ñ–µ—Ä–∞ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
      metadata_size = len(self.metadata_buffers[symbol]) * 300  # ~300 bytes per metadata

      total_bytes = features_size + labels_size + metadata_size
      total_mb = total_bytes / (1024 * 1024)

      return total_mb

    except Exception as e:
      logger.error(f"{symbol} | –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞ –±—É—Ñ–µ—Ä–∞: {e}")
      return 0.0

  def _calculate_total_buffer_memory(self) -> float:
    """
    –†–∞—Å—á–µ—Ç –û–ë–©–ï–ì–û —Ä–∞–∑–º–µ—Ä–∞ –í–°–ï–• –±—É—Ñ–µ—Ä–æ–≤ –≤ –ø–∞–º—è—Ç–∏ (–ú–ë).

    Returns:
        float: –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –≤—Å–µ—Ö –±—É—Ñ–µ—Ä–æ–≤ –≤ –ú–ë
    """
    total_mb = 0.0
    for symbol in self.feature_buffers.keys():
      total_mb += self._calculate_buffer_memory(symbol)
    return total_mb

  def _adapt_thresholds(self):
    """
    üîß –ê–î–ê–ü–¢–ò–í–ù–ê–Ø –ü–û–î–°–¢–†–û–ô–ö–ê –ü–û–†–û–ì–û–í.

    –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–∑–º–µ–Ω—è–µ—Ç max_samples_per_file –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ
    –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –í–°–ï–ú–ò –±—É—Ñ–µ—Ä–∞–º–∏.

    –õ–æ–≥–∏–∫–∞:
    - –í—ã—Å–æ–∫–æ–µ –¥–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ (>70% –æ—Ç –ª–∏–º–∏—Ç–∞) ‚Üí —Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–æ 1000
    - –°—Ä–µ–¥–Ω–µ–µ –¥–∞–≤–ª–µ–Ω–∏–µ (>50%) ‚Üí —Å–Ω–∏–∂–∞–µ–º –¥–æ 1500
    - –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫ –±–∞–∑–æ–≤–æ–º—É (2000)

    –¶–ï–õ–¨: –ü—Ä–µ–≤–µ–Ω—Ç–∏–≤–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–µ.
    """
    total_memory_mb = self._calculate_total_buffer_memory()
    symbol_count = len(self.feature_buffers)

    if symbol_count == 0:
      return

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
    total_memory_limit_mb = self.max_buffer_memory_mb * symbol_count
    memory_usage_percent = (total_memory_mb / total_memory_limit_mb) * 100

    old_threshold = self.max_samples_per_file

    # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∞
    if memory_usage_percent > 70:
      # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ‚Üí –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ —Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥
      self.max_samples_per_file = int(self.initial_max_samples_per_file * 0.5)  # 1000
    elif memory_usage_percent > 50:
      # –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ‚Üí —É–º–µ—Ä–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–µ–º
      self.max_samples_per_file = int(self.initial_max_samples_per_file * 0.75)  # 1500
    else:
      # –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ‚Üí –±–∞–∑–æ–≤—ã–π –ø–æ—Ä–æ–≥
      self.max_samples_per_file = self.initial_max_samples_per_file  # 2000

    if old_threshold != self.max_samples_per_file:
      logger.info(
        f"üîß –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∞: max_samples_per_file {old_threshold} ‚Üí {self.max_samples_per_file} "
        f"(–ø–∞–º—è—Ç—å: {total_memory_mb:.1f}MB/{total_memory_limit_mb:.1f}MB, {memory_usage_percent:.1f}%)"
      )

  async def _emergency_save_all_buffers(self, min_buffer_size: int = 50):
    """
    üö® –≠–ö–°–¢–†–ï–ù–ù–û–ï –°–û–•–†–ê–ù–ï–ù–ò–ï –í–°–ï–• –ë–£–§–ï–†–û–í.

    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ø–∞–º—è—Ç–∏ –∏–ª–∏ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–µ.
    –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞—Ä–æ–≥–æ _cleanup_old_buffers(), –°–û–•–†–ê–ù–Ø–ï–¢ –í–°–ï –¥–∞–Ω–Ω—ã–µ,
    –≤–º–µ—Å—Ç–æ –∏—Ö —É—Ä–µ–∑–∞–Ω–∏—è.

    Args:
        min_buffer_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 50)
                        –ë—É—Ñ–µ—Ä—ã –º–µ–Ω—å—à–µ —ç—Ç–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è (–∫—Ä–æ–º–µ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏)

    –ö–†–ò–¢–ò–ß–ù–û: –ù–æ–ª—å –ø–æ—Ç–µ—Ä–∏ –¥–∞–Ω–Ω—ã—Ö!
    """
    import gc

    logger.info(f"üßπ –ü—Ä–æ–≤–µ—Ä–∫–∞ –±—É—Ñ–µ—Ä–æ–≤ ML Data Collector (–º–∏–Ω–∏–º—É–º –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {min_buffer_size})")

    saved_symbols = []
    skipped_symbols = []
    total_saved_samples = 0

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–∏–µ –±—É—Ñ–µ—Ä—ã
    for symbol in list(self.feature_buffers.keys()):
      if self.feature_buffers[symbol]:
        buffer_size = len(self.feature_buffers[symbol])
        buffer_memory = self._calculate_buffer_memory(symbol)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
        if buffer_size >= min_buffer_size:
          # –°–æ—Ö—Ä–∞–Ω—è–µ–º batch
          await self._save_batch(symbol)

          saved_symbols.append(f"{symbol}({buffer_size} —Å–µ–º–ø–ª–æ–≤, {buffer_memory:.2f}MB)")
          total_saved_samples += buffer_size
        else:
          skipped_symbols.append(f"{symbol}({buffer_size})")

    if saved_symbols:
      logger.warning(
        f"üíæ –≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_saved_samples} —Å–µ–º–ø–ª–æ–≤: {', '.join(saved_symbols)}"
      )

    if skipped_symbols:
      logger.info(
        f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω—ã –º–∞–ª–µ–Ω—å–∫–∏–µ –±—É—Ñ–µ—Ä—ã (< {min_buffer_size}): {', '.join(skipped_symbols)}"
      )

    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
    gc.collect()
    logger.info("üßπ –°–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")

  def get_statistics(self) -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.

    Returns:
        Dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–∞–º—è—Ç–∏ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ä–æ–≥–∞—Ö
    """
    symbol_stats = {}
    for symbol in self.sample_counts.keys():
      buffer_memory = self._calculate_buffer_memory(symbol)
      symbol_stats[symbol] = {
        "total_samples": self.sample_counts[symbol],
        "current_batch": self.batch_numbers[symbol],
        "buffer_size": len(self.feature_buffers.get(symbol, [])),
        "buffer_memory_mb": round(buffer_memory, 2),
        "memory_utilization_percent": round(
          (buffer_memory / self.max_buffer_memory_mb) * 100, 1
        ) if self.max_buffer_memory_mb > 0 else 0
      }

    total_memory = self._calculate_total_buffer_memory()

    return {
      "total_samples_collected": self.total_samples_collected,
      "files_written": self.files_written,
      "iteration_counter": self.iteration_counter,
      "collection_interval": self.collection_interval,
      "memory": {
        "total_buffer_memory_mb": round(total_memory, 2),
        "max_buffer_memory_mb_per_symbol": self.max_buffer_memory_mb,
        "current_max_samples_per_file": self.max_samples_per_file,
        "initial_max_samples_per_file": self.initial_max_samples_per_file
      },
      "symbols": symbol_stats
    }