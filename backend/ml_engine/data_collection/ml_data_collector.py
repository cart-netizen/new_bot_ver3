"""
ML Data Collector - ÑÐ¸ÑÑ‚ÐµÐ¼Ð° ÑÐ±Ð¾Ñ€Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ ML Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹.

Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÑ‚:
- Feature vectors (110 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²)
- Market state (orderbook snapshot, Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸)
- Labels (future price movement, signals)

Ð¤Ð°Ð¹Ð»: backend/ml_engine/data_collection/ml_data_collector.py
"""

import os
import json
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime
from pathlib import Path
import numpy as np
import pandas as pd

from backend.core.logger import get_logger
from backend.ml_engine.features import FeatureVector
from backend.models.orderbook import OrderBookSnapshot, OrderBookMetrics

logger = get_logger(__name__)


class MLDataCollector:
  """
  Ð¡Ð±Ð¾Ñ€Ñ‰Ð¸Ðº Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ ML Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ.

  ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ:
  data/ml_training/
    â”œâ”€â”€ BTCUSDT/
    â”‚   â”œâ”€â”€ features/
    â”‚   â”‚   â”œâ”€â”€ 2025-01-15_batch_0001.npy  # ÐœÐ°ÑÑÐ¸Ð²Ñ‹ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
    â”‚   â”‚   â””â”€â”€ 2025-01-15_batch_0002.npy
    â”‚   â”œâ”€â”€ labels/
    â”‚   â”‚   â”œâ”€â”€ 2025-01-15_batch_0001.npy  # ÐœÐµÑ‚ÐºÐ¸ (Ñ‚Ð°Ñ€Ð³ÐµÑ‚Ñ‹)
    â”‚   â”‚   â””â”€â”€ 2025-01-15_batch_0002.npy
    â”‚   â””â”€â”€ metadata/
    â”‚       â”œâ”€â”€ 2025-01-15_batch_0001.json  # ÐœÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
    â”‚       â””â”€â”€ 2025-01-15_batch_0002.json
    â””â”€â”€ ETHUSDT/
        â””â”€â”€ ...
  """

  def __init__(
      self,
      storage_path: str = "data/ml_training",
      max_samples_per_file: int = 2000,  # ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—Ð˜Ð ÐžÐ’ÐÐÐž: 10000 â†’ 2000 (~2 MB/Ñ„Ð°Ð¹Ð»)
      collection_interval: int = 10,
      # auto_save_interval_seconds: int = 40000,# ÐšÐ°Ð¶Ð´Ñ‹Ðµ N Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¹
      max_buffer_memory_mb: int = 200,  # ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—Ð˜Ð ÐžÐ’ÐÐÐž: 200 â†’ 80 (Ð·Ð°Ð¿Ð°Ñ Ð´Ð»Ñ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸)
      # ÐÐžÐ’ÐžÐ•: Feature Store integration
      enable_feature_store: bool = True,  # Ð—Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°Ñ‚ÑŒ Ð² Feature Store (parquet)
      use_legacy_format: bool = True,     # Ð—Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°Ñ‚ÑŒ Ð² legacy Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ (.npy/.json)
      feature_store_group: str = "training_features"
  ):
    """
    Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÐ±Ð¾Ñ€Ñ‰Ð¸ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ….

    Args:
        storage_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… (legacy Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚)
        max_samples_per_file: ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼ ÑÐµÐ¼Ð¿Ð»Ð¾Ð² Ð² Ð¾Ð´Ð½Ð¾Ð¼ Ñ„Ð°Ð¹Ð»Ðµ
        collection_interval: Ð˜Ð½Ñ‚ÐµÑ€Ð²Ð°Ð» ÑÐ±Ð¾Ñ€Ð° (ÐºÐ°Ð¶Ð´Ñ‹Ðµ N Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¹)
        # auto_save_interval_seconds: Ð˜Ð½Ñ‚ÐµÑ€Ð²Ð°Ð» Ð°Ð²Ñ‚Ð¾ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ (ÑÐµÐºÑƒÐ½Ð´Ñ‹)
        max_buffer_memory_mb: ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð½Ð° Ð±ÑƒÑ„ÐµÑ€ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð° (ÐœÐ‘)
        enable_feature_store: Ð—Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°Ñ‚ÑŒ Ð»Ð¸ Ð² Feature Store (parquet)
        use_legacy_format: Ð—Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°Ñ‚ÑŒ Ð»Ð¸ Ð² legacy Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ (.npy/.json)
        feature_store_group: ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ feature group Ð´Ð»Ñ Feature Store
    """
    self.storage_path = Path(storage_path)
    self.max_samples_per_file = max_samples_per_file
    self.initial_max_samples_per_file = max_samples_per_file  # ÐÐžÐ’ÐžÐ•: Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ð¸
    self.collection_interval = collection_interval
    # self.auto_save_interval = auto_save_interval_seconds
    self.max_buffer_memory_mb = max_buffer_memory_mb

    # Feature Store integration
    self.enable_feature_store = enable_feature_store
    self.use_legacy_format = use_legacy_format
    self.feature_store_group = feature_store_group


    # Ð‘ÑƒÑ„ÐµÑ€Ñ‹ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð°
    self.feature_buffers: Dict[str, List[np.ndarray]] = {}
    self.label_buffers: Dict[str, List[Dict[str, Any]]] = {}
    self.metadata_buffers: Dict[str, List[Dict[str, Any]]] = {}

    # Ð¡Ñ‡ÐµÑ‚Ñ‡Ð¸ÐºÐ¸
    self.sample_counts: Dict[str, int] = {}
    self.batch_numbers: Dict[str, int] = {}
    self.last_save_time: Dict[str, float] = {}  # Ð’Ñ€ÐµÐ¼Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ
    self.iteration_counter = 0
    self.last_cleanup_iteration = 0  # ÐÐžÐ’ÐžÐ•: Ð¡Ñ‡ÐµÑ‚Ñ‡Ð¸Ðº Ð´Ð»Ñ Ð¿ÐµÑ€Ð¸Ð¾Ð´Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸

    # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
    self.total_samples_collected = 0
    self.files_written = 0

    # Feature Store (lazy initialization)
    self._feature_store = None

    logger.info(
      f"MLDataCollector Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½, storage_path={storage_path}, "
      f"max_samples={max_samples_per_file}, interval={collection_interval}, "
      f"feature_store={'âœ…' if enable_feature_store else 'âŒ'}, "
      f"legacy={'âœ…' if use_legacy_format else 'âŒ'}"
    )

  async def initialize(self):
    """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ð°."""
    try:
      # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
      self.storage_path.mkdir(parents=True, exist_ok=True)
      logger.info(f"Ð¡Ð¾Ð·Ð´Ð°Ð½Ð° Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ð°: {self.storage_path}")

    except Exception as e:
      logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ð°: {e}")
      raise

  def should_collect(self) -> bool:
    """
    ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½ÑƒÐ¶Ð½Ð¾ Ð»Ð¸ ÑÐ¾Ð±Ð¸Ñ€Ð°Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² ÑÑ‚Ð¾Ð¹ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸.

    Returns:
        bool: True ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾ ÑÐ¾Ð±Ñ€Ð°Ñ‚ÑŒ
    """
    self.iteration_counter += 1
    return self.iteration_counter % self.collection_interval == 0

  async def collect_sample(
      self,
      symbol: str,
      feature_vector: FeatureVector,
      orderbook_snapshot: OrderBookSnapshot,
      market_metrics: OrderBookMetrics,
      executed_signal: Optional[Dict[str, Any]] = None
  ):
    """
    Ð¡Ð±Ð¾Ñ€ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÑÐµÐ¼Ð¿Ð»Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ….

    Args:
        symbol: Ð¢Ð¾Ñ€Ð³Ð¾Ð²Ð°Ñ Ð¿Ð°Ñ€Ð°
        feature_vector: Ð’ÐµÐºÑ‚Ð¾Ñ€ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² (110 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²)
        orderbook_snapshot: Ð¡Ð½Ð¸Ð¼Ð¾Ðº ÑÑ‚Ð°ÐºÐ°Ð½Ð°
        market_metrics: Ð Ñ‹Ð½Ð¾Ñ‡Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
        executed_signal: Ð˜ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð½Ñ‹Ð¹ ÑÐ¸Ð³Ð½Ð°Ð» (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ)
    """
    try:
      # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€Ñ‹ Ð´Ð»Ñ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð°
      if symbol not in self.feature_buffers:
        self.feature_buffers[symbol] = []
        self.label_buffers[symbol] = []
        self.metadata_buffers[symbol] = []
        self.sample_counts[symbol] = 0
        self.batch_numbers[symbol] = 1
        self.last_save_time[symbol] = datetime.now().timestamp()

      # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¼Ð°ÑÑÐ¸Ð² Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
      features_array = feature_vector.to_array()

      # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¼ÐµÑ‚ÐºÑƒ (label) Ð´Ð»Ñ supervised learning
      # Ð—Ð´ÐµÑÑŒ Ð½ÑƒÐ¶Ð½Ð¾ Ð±ÑƒÐ´ÐµÑ‚ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð»Ð¾Ð³Ð¸ÐºÑƒ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð° future price movement
      label = self._create_label(
        orderbook_snapshot,
        market_metrics,
        executed_signal
      )

      # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
      metadata = {
        "timestamp": orderbook_snapshot.timestamp,
        "symbol": symbol,
        "mid_price": orderbook_snapshot.mid_price,
        "spread": orderbook_snapshot.spread,
        "imbalance": market_metrics.imbalance,
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ð¾Ð»Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ ÑÐ¸Ð³Ð½Ð°Ð»Ðµ
        "signal_type": executed_signal.get("type") if executed_signal else None,
        "signal_confidence": executed_signal.get("confidence") if executed_signal else None,
        "signal_strength": executed_signal.get("strength") if executed_signal else None,
        "feature_count": feature_vector.feature_count
      }

      # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð² Ð±ÑƒÑ„ÐµÑ€Ñ‹
      self.feature_buffers[symbol].append(features_array)
      self.label_buffers[symbol].append(label)
      self.metadata_buffers[symbol].append(metadata)

      self.sample_counts[symbol] += 1
      self.total_samples_collected += 1

      # ðŸ”§ ÐÐ”ÐÐŸÐ¢Ð˜Ð’ÐÐÐ¯ ÐŸÐžÐ”Ð¡Ð¢Ð ÐžÐ™ÐšÐ (ÐºÐ°Ð¶Ð´Ñ‹Ðµ 100 ÑÐµÐ¼Ð¿Ð»Ð¾Ð²)
      if self.total_samples_collected % 100 == 0:
        self._adapt_thresholds()

      # ðŸ”¥ ÐŸÐ ÐžÐÐšÐ¢Ð˜Ð’ÐÐÐ¯ ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ ÐŸÐÐœÐ¯Ð¢Ð˜ (ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž!)
      # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐŸÐžÐ¡Ð›Ð• Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ ÑÐµÐ¼Ð¿Ð»Ð°, Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€ÑƒÑ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ñ…
      buffer_size = len(self.feature_buffers[symbol])
      buffer_memory_mb = self._calculate_buffer_memory(symbol)
      memory_threshold_mb = self.max_buffer_memory_mb * 0.9  # 90% Ð¿Ð¾Ñ€Ð¾Ð³

      logger.debug(
        f"{symbol} | Ð¡Ð¾Ð±Ñ€Ð°Ð½ ÑÐµÐ¼Ð¿Ð» #{self.sample_counts[symbol]}, "
        f"Ð±ÑƒÑ„ÐµÑ€: {buffer_size}/{self.max_samples_per_file}, "
        f"Ð¿Ð°Ð¼ÑÑ‚ÑŒ: {buffer_memory_mb:.2f}MB/{self.max_buffer_memory_mb}MB"
      )

      # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½ÑƒÐ¶Ð½Ð¾ Ð»Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ batch (Ð¿Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ñƒ Ð˜Ð›Ð˜ Ð¿Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸)
      should_save = False
      save_reason = ""

      if buffer_size >= self.max_samples_per_file:
        should_save = True
        save_reason = f"Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐµÐ½ Ð»Ð¸Ð¼Ð¸Ñ‚ ÑÐµÐ¼Ð¿Ð»Ð¾Ð² ({buffer_size}/{self.max_samples_per_file})"
      elif buffer_memory_mb >= memory_threshold_mb:
        should_save = True
        save_reason = f"Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐµÐ½ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð¿Ð°Ð¼ÑÑ‚Ð¸ ({buffer_memory_mb:.2f}MB/{memory_threshold_mb:.2f}MB)"

      if should_save:
        logger.info(f"{symbol} | ðŸ’¾ ÐÐ²Ñ‚Ð¾ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ: {save_reason}")
        await self._save_batch(symbol)

    except Exception as e:
      logger.error(f"{symbol} | ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ±Ð¾Ñ€Ð° ÑÐµÐ¼Ð¿Ð»Ð°: {e}")

  def _create_label(
        self,
        orderbook_snapshot: OrderBookSnapshot,
        market_metrics: OrderBookMetrics,
        executed_signal: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
      """
      Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¼ÐµÑ‚ÐºÐ¸ (label) Ð´Ð»Ñ supervised learning.

      Future targets (direction, movement) Ð±ÑƒÐ´ÑƒÑ‚ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ ÐŸÐžÐ—Ð–Ð•
      Ñ‡ÐµÑ€ÐµÐ· preprocessing ÑÐºÑ€Ð¸Ð¿Ñ‚, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ ÑÐ¾Ð±Ñ€Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ.

      Ð’ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ real-time Ð¼Ñ‹ ÐÐ• ÐœÐžÐ–Ð•Ðœ Ð·Ð½Ð°Ñ‚ÑŒ Ð±ÑƒÐ´ÑƒÑ‰ÑƒÑŽ Ñ†ÐµÐ½Ñƒ,
      Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ ÑÐµÐ¹Ñ‡Ð°Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚ÐµÐºÑƒÑ‰ÐµÐµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ.

      Args:
          orderbook_snapshot: Ð¡Ð½Ð¸Ð¼Ð¾Ðº ÑÑ‚Ð°ÐºÐ°Ð½Ð°
          market_metrics: ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸
          executed_signal: Ð˜ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð½Ñ‹Ð¹ ÑÐ¸Ð³Ð½Ð°Ð»

      Returns:
          Dict: ÐœÐµÑ‚ÐºÐ° Ñ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸ÐµÐ¼ Ð¸ Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ°Ð¼Ð¸ Ð´Ð»Ñ future targets
      """
      label = {
        # ===== FUTURE TARGETS (Ð·Ð°Ð¿Ð¾Ð»Ð½ÑÑ‚ÑÑ Ñ‡ÐµÑ€ÐµÐ· preprocessing) =====
        # Ð­Ñ‚Ð¸ Ð¿Ð¾Ð»Ñ Ð±ÑƒÐ´ÑƒÑ‚ Ñ€Ð°ÑÑÑ‡Ð¸Ñ‚Ð°Ð½Ñ‹ ÐŸÐžÐ¡Ð›Ð• ÑÐ±Ð¾Ñ€Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…,
        # ÐºÐ¾Ð³Ð´Ð° Ð¼Ñ‹ Ð±ÑƒÐ´ÐµÐ¼ Ð·Ð½Ð°Ñ‚ÑŒ, Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð¾ Ñ Ñ†ÐµÐ½Ð¾Ð¹ Ñ‡ÐµÑ€ÐµÐ· N ÑÐµÐºÑƒÐ½Ð´
        "future_direction_10s": None,  # 1=up, 0=neutral, -1=down
        "future_direction_30s": None,
        "future_direction_60s": None,
        "future_movement_10s": None,  # % Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ñ†ÐµÐ½Ñ‹
        "future_movement_30s": None,
        "future_movement_60s": None,

        # Current state
        "timestamp": orderbook_snapshot.timestamp,  # Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ timestamp Ð´Ð»Ñ preprocessing
        "current_mid_price": orderbook_snapshot.mid_price,
        "current_imbalance": market_metrics.imbalance,

        # ===== Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐ˜Ð•: Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð’Ð¡Ð® Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ ÑÐ¸Ð³Ð½Ð°Ð»Ðµ =====
        "signal_type": executed_signal.get("type") if executed_signal else None,
        "signal_confidence": executed_signal.get("confidence") if executed_signal else None,
        "signal_strength": executed_signal.get("strength") if executed_signal else None,
      }

      return label

  async def _save_batch(self, symbol: str):
    """
    Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ batch Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½Ð° Ð´Ð¸ÑÐº.
    ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ legacy Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ (.npy/.json) Ð¸ Feature Store (parquet).

    Args:
        symbol: Ð¢Ð¾Ñ€Ð³Ð¾Ð²Ð°Ñ Ð¿Ð°Ñ€Ð°
    """
    try:
      if not self.feature_buffers[symbol]:
        return

      # 1. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² Feature Store (ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾)
      if self.enable_feature_store:
        await self._save_to_feature_store(symbol)

      # 2. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² legacy Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ (ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾)
      if self.use_legacy_format:
        await self._save_legacy_batch(symbol)

      # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€Ñ‹ (Ð¿Ð¾ÑÐ»Ðµ Ð¾Ð±Ð¾Ð¸Ñ… ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ð¹)
      buffer_size = len(self.feature_buffers[symbol])
      self.feature_buffers[symbol].clear()
      self.label_buffers[symbol].clear()
      self.metadata_buffers[symbol].clear()

      # Ð˜Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ batch number
      self.batch_numbers[symbol] += 1

      logger.info(
        f"âœ“ {symbol} | Batch ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½: {buffer_size} ÑÐµÐ¼Ð¿Ð»Ð¾Ð² "
        f"(FS={'âœ…' if self.enable_feature_store else 'âŒ'}, "
        f"Legacy={'âœ…' if self.use_legacy_format else 'âŒ'})"
      )

    except Exception as e:
      logger.error(f"{symbol} | ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ batch: {e}", exc_info=True)

  async def _save_legacy_batch(self, symbol: str):
    """
    Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² legacy Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ (.npy/.json).

    Args:
        symbol: Ð¢Ð¾Ñ€Ð³Ð¾Ð²Ð°Ñ Ð¿Ð°Ñ€Ð°
    """
    try:
      # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ Ð´Ð»Ñ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð°
      symbol_dir = self.storage_path / symbol
      features_dir = symbol_dir / "features"
      labels_dir = symbol_dir / "labels"
      metadata_dir = symbol_dir / "metadata"

      for dir_path in [features_dir, labels_dir, metadata_dir]:
        dir_path.mkdir(parents=True, exist_ok=True)

      # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð¼Ñ Ñ„Ð°Ð¹Ð»Ð°
      date_str = datetime.now().strftime("%Y-%m-%d")
      batch_num = self.batch_numbers[symbol]
      filename_base = f"{date_str}_batch_{batch_num:04d}"

      # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ features (numpy)
      features_array = np.array(self.feature_buffers[symbol])
      features_file = features_dir / f"{filename_base}.npy"
      np.save(features_file, features_array)

      # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ labels (json)
      labels_file = labels_dir / f"{filename_base}.json"
      with open(labels_file, 'w') as f:
        json.dump(self.label_buffers[symbol], f, indent=2)

      # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ metadata (json)
      metadata_file = metadata_dir / f"{filename_base}.json"
      with open(metadata_file, 'w') as f:
        json.dump({
          "batch_info": {
            "symbol": symbol,
            "batch_number": batch_num,
            "sample_count": len(self.feature_buffers[symbol]),
            "timestamp": datetime.now().isoformat(),
            "feature_shape": features_array.shape
          },
          "samples": self.metadata_buffers[symbol]
        }, f, indent=2)

      self.files_written += 3  # features, labels, metadata

      logger.info(
        f"{symbol} | Legacy batch #{batch_num}: "
        f"{len(self.feature_buffers[symbol])} ÑÐµÐ¼Ð¿Ð»Ð¾Ð², "
        f"shape={features_array.shape}"
      )

    except Exception as e:
      logger.error(f"{symbol} | ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ legacy batch: {e}", exc_info=True)

  def _get_feature_store(self):
    """Lazy initialization Feature Store"""
    if self._feature_store is None:
      from backend.ml_engine.feature_store.feature_store import get_feature_store
      self._feature_store = get_feature_store()
    return self._feature_store

  async def _save_to_feature_store(self, symbol: str):
    """
    Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Feature Store (parquet Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚).

    Args:
        symbol: Ð¢Ð¾Ñ€Ð³Ð¾Ð²Ð°Ñ Ð¿Ð°Ñ€Ð°
    """
    try:
      if not self.feature_buffers[symbol]:
        return

      logger.info(f"{symbol} | Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² Feature Store...")

      # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€Ñ‹
      features_list = self.feature_buffers[symbol]
      labels_list = self.label_buffers[symbol]
      metadata_list = self.metadata_buffers[symbol]

      # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² DataFrame
      rows = []

      for feature_arr, label_dict, meta_dict in zip(features_list, labels_list, metadata_list):
        # Ð‘Ð°Ð·Ð¾Ð²Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ
        row = {
          'symbol': symbol,
          'timestamp': meta_dict['timestamp'],
          'mid_price': meta_dict['mid_price'],
        }

        # Ð Ð°ÑÐ¿Ð°ÐºÐ¾Ð²Ñ‹Ð²Ð°ÐµÐ¼ 110 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
        # feature_arr Ð¸Ð¼ÐµÐµÑ‚ shape (110,)
        for i in range(len(feature_arr)):
          row[f'feature_{i:03d}'] = feature_arr[i]

        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¼ÐµÑ‚ÐºÐ¸ (labels)
        # ÐœÐµÑ‚ÐºÐ¸ Ð¼Ð¾Ð³ÑƒÑ‚ Ð±Ñ‹Ñ‚ÑŒ None (Ð±ÑƒÐ´ÑƒÑ‚ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ñ‹ Ð¿Ð¾Ð·Ð¶Ðµ Ñ‡ÐµÑ€ÐµÐ· preprocessing)
        row['future_direction_10s'] = label_dict.get('future_direction_10s')
        row['future_direction_30s'] = label_dict.get('future_direction_30s')
        row['future_direction_60s'] = label_dict.get('future_direction_60s')
        row['future_movement_10s'] = label_dict.get('future_movement_10s')
        row['future_movement_30s'] = label_dict.get('future_movement_30s')
        row['future_movement_60s'] = label_dict.get('future_movement_60s')

        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ current_mid_price Ð´Ð»Ñ preprocessing
        row['current_mid_price'] = label_dict.get('current_mid_price')

        # ÐœÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¸Ð³Ð½Ð°Ð»Ð° (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
        row['signal_type'] = meta_dict.get('signal_type')
        row['signal_confidence'] = meta_dict.get('signal_confidence')
        row['signal_strength'] = meta_dict.get('signal_strength')

        rows.append(row)

      # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ DataFrame
      df = pd.DataFrame(rows)

      # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ timestamp
      df = df.sort_values('timestamp').reset_index(drop=True)

      # Ð—Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÐ¼ Ð² Feature Store
      feature_store = self._get_feature_store()
      success = feature_store.write_offline_features(
        feature_group=self.feature_store_group,
        features=df,
        timestamp_column='timestamp'
      )

      if success:
        logger.info(
          f"âœ“ {symbol} | Feature Store: ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾ {len(df)} ÑÐµÐ¼Ð¿Ð»Ð¾Ð² Ð² parquet"
        )
      else:
        logger.error(f"{symbol} | ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð² Feature Store")

    except Exception as e:
      logger.error(f"{symbol} | ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð² Feature Store: {e}", exc_info=True)

  async def finalize(self):
    """Ð¤Ð¸Ð½Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ - ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… Ð¾ÑÑ‚Ð°Ð²ÑˆÐ¸Ñ…ÑÑ Ð±ÑƒÑ„ÐµÑ€Ð¾Ð²."""
    logger.info("Ð¤Ð¸Ð½Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ MLDataCollector...")

    for symbol in self.feature_buffers.keys():
      if self.feature_buffers[symbol]:
        await self._save_batch(symbol)
        logger.info(f"{symbol} | Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ batch ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½")

    logger.info(
      f"MLDataCollector Ñ„Ð¸Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½: "
      f"Ð²ÑÐµÐ³Ð¾ ÑÐµÐ¼Ð¿Ð»Ð¾Ð²={self.total_samples_collected}, "
      f"Ñ„Ð°Ð¹Ð»Ð¾Ð²={self.files_written}"
    )

  def _calculate_buffer_memory(self, symbol: str) -> float:
    """
    Ð Ð°ÑÑ‡ÐµÑ‚ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° Ð±ÑƒÑ„ÐµÑ€Ð° Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸ (ÐœÐ‘).

    Args:
        symbol: Ð¢Ð¾Ñ€Ð³Ð¾Ð²Ð°Ñ Ð¿Ð°Ñ€Ð°

    Returns:
        float: Ð Ð°Ð·Ð¼ÐµÑ€ Ð±ÑƒÑ„ÐµÑ€Ð° Ð² ÐœÐ‘
    """
    if symbol not in self.feature_buffers:
      return 0.0

    try:
      # Ð Ð°Ð·Ð¼ÐµÑ€ feature Ð±ÑƒÑ„ÐµÑ€Ð°
      features_size = 0
      for arr in self.feature_buffers[symbol]:
        features_size += arr.nbytes

      # Ð Ð°Ð·Ð¼ÐµÑ€ label Ð±ÑƒÑ„ÐµÑ€Ð° (Ð¿Ñ€Ð¸Ð±Ð»Ð¸Ð·Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾)
      labels_size = len(self.label_buffers[symbol]) * 200  # ~200 bytes per label

      # Ð Ð°Ð·Ð¼ÐµÑ€ metadata Ð±ÑƒÑ„ÐµÑ€Ð° (Ð¿Ñ€Ð¸Ð±Ð»Ð¸Ð·Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾)
      metadata_size = len(self.metadata_buffers[symbol]) * 300  # ~300 bytes per metadata

      total_bytes = features_size + labels_size + metadata_size
      total_mb = total_bytes / (1024 * 1024)

      return total_mb

    except Exception as e:
      logger.error(f"{symbol} | ÐžÑˆÐ¸Ð±ÐºÐ° Ñ€Ð°ÑÑ‡ÐµÑ‚Ð° Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° Ð±ÑƒÑ„ÐµÑ€Ð°: {e}")
      return 0.0

  def _calculate_total_buffer_memory(self) -> float:
    """
    Ð Ð°ÑÑ‡ÐµÑ‚ ÐžÐ‘Ð©Ð•Ð“Ðž Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° Ð’Ð¡Ð•Ð¥ Ð±ÑƒÑ„ÐµÑ€Ð¾Ð² Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸ (ÐœÐ‘).

    Returns:
        float: ÐžÐ±Ñ‰Ð¸Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð²ÑÐµÑ… Ð±ÑƒÑ„ÐµÑ€Ð¾Ð² Ð² ÐœÐ‘
    """
    total_mb = 0.0
    for symbol in self.feature_buffers.keys():
      total_mb += self._calculate_buffer_memory(symbol)
    return total_mb

  def _adapt_thresholds(self):
    """
    ðŸ”§ ÐÐ”ÐÐŸÐ¢Ð˜Ð’ÐÐÐ¯ ÐŸÐžÐ”Ð¡Ð¢Ð ÐžÐ™ÐšÐ ÐŸÐžÐ ÐžÐ“ÐžÐ’.

    Ð”Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¸Ð·Ð¼ÐµÐ½ÑÐµÑ‚ max_samples_per_file Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾
    Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð’Ð¡Ð•ÐœÐ˜ Ð±ÑƒÑ„ÐµÑ€Ð°Ð¼Ð¸.

    Ð›Ð¾Ð³Ð¸ÐºÐ°:
    - Ð’Ñ‹ÑÐ¾ÐºÐ¾Ðµ Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð°Ð¼ÑÑ‚Ð¸ (>70% Ð¾Ñ‚ Ð»Ð¸Ð¼Ð¸Ñ‚Ð°) â†’ ÑÐ½Ð¸Ð¶Ð°ÐµÐ¼ Ð¿Ð¾Ñ€Ð¾Ð³ Ð´Ð¾ 1000
    - Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ (>50%) â†’ ÑÐ½Ð¸Ð¶Ð°ÐµÐ¼ Ð´Ð¾ 1500
    - ÐÐ¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ â†’ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ðº Ð±Ð°Ð·Ð¾Ð²Ð¾Ð¼Ñƒ (2000)

    Ð¦Ð•Ð›Ð¬: ÐŸÑ€ÐµÐ²ÐµÐ½Ñ‚Ð¸Ð²Ð½Ð¾Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐµ.
    """
    total_memory_mb = self._calculate_total_buffer_memory()
    symbol_count = len(self.feature_buffers)

    if symbol_count == 0:
      return

    # Ð Ð°ÑÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð´Ð»Ñ Ð²ÑÐµÑ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
    total_memory_limit_mb = self.max_buffer_memory_mb * symbol_count
    memory_usage_percent = (total_memory_mb / total_memory_limit_mb) * 100

    old_threshold = self.max_samples_per_file

    # ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð¿Ð¾Ð´ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°
    if memory_usage_percent > 70:
      # ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ â†’ Ð°Ð³Ñ€ÐµÑÑÐ¸Ð²Ð½Ð¾ ÑÐ½Ð¸Ð¶Ð°ÐµÐ¼ Ð¿Ð¾Ñ€Ð¾Ð³
      self.max_samples_per_file = int(self.initial_max_samples_per_file * 0.5)  # 1000
    elif memory_usage_percent > 50:
      # Ð’Ñ‹ÑÐ¾ÐºÐ¾Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ â†’ ÑƒÐ¼ÐµÑ€ÐµÐ½Ð½Ð¾ ÑÐ½Ð¸Ð¶Ð°ÐµÐ¼
      self.max_samples_per_file = int(self.initial_max_samples_per_file * 0.75)  # 1500
    else:
      # ÐÐ¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ â†’ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ñ€Ð¾Ð³
      self.max_samples_per_file = self.initial_max_samples_per_file  # 2000

    if old_threshold != self.max_samples_per_file:
      logger.info(
        f"ðŸ”§ ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð¿Ð¾Ð´ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°: max_samples_per_file {old_threshold} â†’ {self.max_samples_per_file} "
        f"(Ð¿Ð°Ð¼ÑÑ‚ÑŒ: {total_memory_mb:.1f}MB/{total_memory_limit_mb:.1f}MB, {memory_usage_percent:.1f}%)"
      )

  async def _emergency_save_all_buffers(self):
    """
    ðŸš¨ Ð­ÐšÐ¡Ð¢Ð Ð•ÐÐÐžÐ• Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ˜Ð• Ð’Ð¡Ð•Ð¥ Ð‘Ð£Ð¤Ð•Ð ÐžÐ’.

    Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð¿Ñ€Ð¸ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¼ Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐµÐ½Ð¸Ð¸ Ð¿Ð°Ð¼ÑÑ‚Ð¸.
    Ð’ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð¸Ðµ Ð¾Ñ‚ ÑÑ‚Ð°Ñ€Ð¾Ð³Ð¾ _cleanup_old_buffers(), Ð¡ÐžÐ¥Ð ÐÐÐ¯Ð•Ð¢ Ð’Ð¡Ð• Ð´Ð°Ð½Ð½Ñ‹Ðµ,
    Ð²Ð¼ÐµÑÑ‚Ð¾ Ð¸Ñ… ÑƒÑ€ÐµÐ·Ð°Ð½Ð¸Ñ.

    ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž: ÐÐ¾Ð»ÑŒ Ð¿Ð¾Ñ‚ÐµÑ€Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…!
    """
    import gc

    logger.warning("ðŸš¨ Ð­ÐšÐ¡Ð¢Ð Ð•ÐÐÐžÐ• Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ˜Ð• Ð’Ð¡Ð•Ð¥ Ð‘Ð£Ð¤Ð•Ð ÐžÐ’")

    saved_symbols = []
    total_saved_samples = 0

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð’Ð¡Ð• Ð±ÑƒÑ„ÐµÑ€Ñ‹ Ð´Ð»Ñ Ð’Ð¡Ð•Ð¥ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
    for symbol in list(self.feature_buffers.keys()):
      if self.feature_buffers[symbol]:
        buffer_size = len(self.feature_buffers[symbol])
        buffer_memory = self._calculate_buffer_memory(symbol)

        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ batch
        await self._save_batch(symbol)

        saved_symbols.append(f"{symbol}({buffer_size} ÑÐµÐ¼Ð¿Ð»Ð¾Ð², {buffer_memory:.2f}MB)")
        total_saved_samples += buffer_size

    if saved_symbols:
      logger.warning(
        f"ðŸ’¾ Ð­ÐºÑÑ‚Ñ€ÐµÐ½Ð½Ð¾ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾ {total_saved_samples} ÑÐµÐ¼Ð¿Ð»Ð¾Ð²: {', '.join(saved_symbols)}"
      )

    # ÐŸÑ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ ÑÐ±Ð¾Ñ€ÐºÐ° Ð¼ÑƒÑÐ¾Ñ€Ð°
    gc.collect()
    logger.info("ðŸ§¹ Ð¡Ð±Ð¾Ñ€ÐºÐ° Ð¼ÑƒÑÐ¾Ñ€Ð° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð°")

  def get_statistics(self) -> Dict[str, Any]:
    """
    ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ ÑÐ±Ð¾Ñ€Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ….

    Returns:
        Dict: Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÐµÐ¹ Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð¸ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¿Ð¾Ñ€Ð¾Ð³Ð°Ñ…
    """
    symbol_stats = {}
    for symbol in self.sample_counts.keys():
      buffer_memory = self._calculate_buffer_memory(symbol)
      symbol_stats[symbol] = {
        "total_samples": self.sample_counts[symbol],
        "current_batch": self.batch_numbers[symbol],
        "buffer_size": len(self.feature_buffers.get(symbol, [])),
        "buffer_memory_mb": round(buffer_memory, 2),
        "memory_utilization_percent": round(
          (buffer_memory / self.max_buffer_memory_mb) * 100, 1
        ) if self.max_buffer_memory_mb > 0 else 0
      }

    total_memory = self._calculate_total_buffer_memory()

    return {
      "total_samples_collected": self.total_samples_collected,
      "files_written": self.files_written,
      "iteration_counter": self.iteration_counter,
      "collection_interval": self.collection_interval,
      "memory": {
        "total_buffer_memory_mb": round(total_memory, 2),
        "max_buffer_memory_mb_per_symbol": self.max_buffer_memory_mb,
        "current_max_samples_per_file": self.max_samples_per_file,
        "initial_max_samples_per_file": self.initial_max_samples_per_file
      },
      "symbols": symbol_stats
    }