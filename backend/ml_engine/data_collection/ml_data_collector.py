"""
ML Data Collector - —Å–∏—Å—Ç–µ–º–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π.

–°–æ–±–∏—Ä–∞–µ—Ç:
- Feature vectors (110 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
- Market state (orderbook snapshot, –º–µ—Ç—Ä–∏–∫–∏)
- Labels (future price movement, signals)

–§–∞–π–ª: backend/ml_engine/data_collection/ml_data_collector.py
"""

import os
import json
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime
from pathlib import Path
import numpy as np

from core.logger import get_logger
from ml_engine.features import FeatureVector
from models.orderbook import OrderBookSnapshot, OrderBookMetrics

logger = get_logger(__name__)


class MLDataCollector:
  """
  –°–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML –æ–±—É—á–µ–Ω–∏—è.

  –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è:
  data/ml_training/
    ‚îú‚îÄ‚îÄ BTCUSDT/
    ‚îÇ   ‚îú‚îÄ‚îÄ features/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2025-01-15_batch_0001.npy  # –ú–∞—Å—Å–∏–≤—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2025-01-15_batch_0002.npy
    ‚îÇ   ‚îú‚îÄ‚îÄ labels/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2025-01-15_batch_0001.npy  # –ú–µ—Ç–∫–∏ (—Ç–∞—Ä–≥–µ—Ç—ã)
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2025-01-15_batch_0002.npy
    ‚îÇ   ‚îî‚îÄ‚îÄ metadata/
    ‚îÇ       ‚îú‚îÄ‚îÄ 2025-01-15_batch_0001.json  # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    ‚îÇ       ‚îî‚îÄ‚îÄ 2025-01-15_batch_0002.json
    ‚îî‚îÄ‚îÄ ETHUSDT/
        ‚îî‚îÄ‚îÄ ...
  """

  def __init__(
      self,
      storage_path: str = "data/ml_training",
      max_samples_per_file: int = 10000,
      collection_interval: int = 10,
      # auto_save_interval_seconds: int = 40000,# –ö–∞–∂–¥—ã–µ N –∏—Ç–µ—Ä–∞—Ü–∏–π
      max_buffer_memory_mb: int = 200  # –ù–û–í–û–ï: –ú–∞–∫—Å–∏–º—É–º –ú–ë –Ω–∞ —Å–∏–º–≤–æ–ª
  ):
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä—â–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö.

    Args:
        storage_path: –ü—É—Ç—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        max_samples_per_file: –ú–∞–∫—Å–∏–º—É–º —Å–µ–º–ø–ª–æ–≤ –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ
        collection_interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–±–æ—Ä–∞ (–∫–∞–∂–¥—ã–µ N –∏—Ç–µ—Ä–∞—Ü–∏–π)
        # auto_save_interval_seconds: –ò–Ω—Ç–µ—Ä–≤–∞–ª –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)
        max_buffer_memory_mb: –ú–∞–∫—Å–∏–º—É–º –ø–∞–º—è—Ç–∏ –Ω–∞ –±—É—Ñ–µ—Ä —Å–∏–º–≤–æ–ª–∞ (–ú–ë)
    """
    self.storage_path = Path(storage_path)
    self.max_samples_per_file = max_samples_per_file
    self.collection_interval = collection_interval
    # self.auto_save_interval = auto_save_interval_seconds
    self.max_buffer_memory_mb = max_buffer_memory_mb


    # –ë—É—Ñ–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    self.feature_buffers: Dict[str, List[np.ndarray]] = {}
    self.label_buffers: Dict[str, List[Dict[str, Any]]] = {}
    self.metadata_buffers: Dict[str, List[Dict[str, Any]]] = {}

    # –°—á–µ—Ç—á–∏–∫–∏
    self.sample_counts: Dict[str, int] = {}
    self.batch_numbers: Dict[str, int] = {}
    self.last_save_time: Dict[str, float] = {}  # –í—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    self.iteration_counter = 0
    self.last_cleanup_iteration = 0  # –ù–û–í–û–ï: –°—á–µ—Ç—á–∏–∫ –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    self.total_samples_collected = 0
    self.files_written = 0

    logger.info(
      f"MLDataCollector –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, storage_path={storage_path}, "
      f"max_samples={max_samples_per_file}, interval={collection_interval}, "
      # f"auto_save={auto_save_interval_seconds}s, max_buffer_mem={max_buffer_memory_mb}MB"
      # f"auto_save={auto_save_interval_seconds}s"
      f"interval={collection_interval}"
    )

  async def initialize(self):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""
    try:
      # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
      self.storage_path.mkdir(parents=True, exist_ok=True)
      logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {self.storage_path}")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {e}")
      raise

  def should_collect(self) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω—É–∂–Ω–æ –ª–∏ —Å–æ–±–∏—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —ç—Ç–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏.

    Returns:
        bool: True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å
    """
    self.iteration_counter += 1
    return self.iteration_counter % self.collection_interval == 0

  async def collect_sample(
      self,
      symbol: str,
      feature_vector: FeatureVector,
      orderbook_snapshot: OrderBookSnapshot,
      market_metrics: OrderBookMetrics,
      executed_signal: Optional[Dict[str, Any]] = None
  ):
    """
    –°–±–æ—Ä –æ–¥–Ω–æ–≥–æ —Å–µ–º–ø–ª–∞ –¥–∞–Ω–Ω—ã—Ö.

    Args:
        symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
        feature_vector: –í–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (110 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        orderbook_snapshot: –°–Ω–∏–º–æ–∫ —Å—Ç–∞–∫–∞–Ω–∞
        market_metrics: –†—ã–Ω–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        executed_signal: –ò—Å–ø–æ–ª–Ω–µ–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª (–µ—Å–ª–∏ –µ—Å—Ç—å)
    """
    try:
      # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±—É—Ñ–µ—Ä—ã –¥–ª—è —Å–∏–º–≤–æ–ª–∞
      if symbol not in self.feature_buffers:
        self.feature_buffers[symbol] = []
        self.label_buffers[symbol] = []
        self.metadata_buffers[symbol] = []
        self.sample_counts[symbol] = 0
        self.batch_numbers[symbol] = 1
        self.last_save_time[symbol] = datetime.now().timestamp()

      # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞—Å—Å–∏–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
      features_array = feature_vector.to_array()

      # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫—É (label) –¥–ª—è supervised learning
      # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —Ä–∞—Å—á–µ—Ç–∞ future price movement
      label = self._create_label(
        orderbook_snapshot,
        market_metrics,
        executed_signal
      )

      # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
      metadata = {
        "timestamp": orderbook_snapshot.timestamp,
        "symbol": symbol,
        "mid_price": orderbook_snapshot.mid_price,
        "spread": orderbook_snapshot.spread,
        "imbalance": market_metrics.imbalance,
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏–≥–Ω–∞–ª–µ
        "signal_type": executed_signal.get("type") if executed_signal else None,
        "signal_confidence": executed_signal.get("confidence") if executed_signal else None,
        "signal_strength": executed_signal.get("strength") if executed_signal else None,
        "feature_count": feature_vector.feature_count
      }

      # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±—É—Ñ–µ—Ä—ã
      self.feature_buffers[symbol].append(features_array)
      self.label_buffers[symbol].append(label)
      self.metadata_buffers[symbol].append(metadata)

      self.sample_counts[symbol] += 1
      self.total_samples_collected += 1

      logger.debug(
        f"{symbol} | –°–æ–±—Ä–∞–Ω —Å–µ–º–ø–ª #{self.sample_counts[symbol]}, "
        f"–±—É—Ñ–µ—Ä: {len(self.feature_buffers[symbol])}/{self.max_samples_per_file}"
      )

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å batch
      if len(self.feature_buffers[symbol]) >= self.max_samples_per_file:
        await self._save_batch(symbol)

    except Exception as e:
      logger.error(f"{symbol} | –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Å–µ–º–ø–ª–∞: {e}")

  def _create_label(
        self,
        orderbook_snapshot: OrderBookSnapshot,
        market_metrics: OrderBookMetrics,
        executed_signal: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
      """
      –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∫–∏ (label) –¥–ª—è supervised learning.

      Future targets (direction, movement) –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –ü–û–ó–ñ–ï
      —á–µ—Ä–µ–∑ preprocessing —Å–∫—Ä–∏–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.

      –í —Ä–µ–∂–∏–º–µ real-time –º—ã –ù–ï –ú–û–ñ–ï–ú –∑–Ω–∞—Ç—å –±—É–¥—É—â—É—é —Ü–µ–Ω—É,
      –ø–æ—ç—Ç–æ–º—É —Å–µ–π—á–∞—Å —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ.

      Args:
          orderbook_snapshot: –°–Ω–∏–º–æ–∫ —Å—Ç–∞–∫–∞–Ω–∞
          market_metrics: –ú–µ—Ç—Ä–∏–∫–∏
          executed_signal: –ò—Å–ø–æ–ª–Ω–µ–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª

      Returns:
          Dict: –ú–µ—Ç–∫–∞ —Å —Ç–µ–∫—É—â–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –∏ –∑–∞–≥–ª—É—à–∫–∞–º–∏ –¥–ª—è future targets
      """
      label = {
        # ===== FUTURE TARGETS (–∑–∞–ø–æ–ª–Ω—è—Ç—Å—è —á–µ—Ä–µ–∑ preprocessing) =====
        # –≠—Ç–∏ –ø–æ–ª—è –±—É–¥—É—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –ü–û–°–õ–ï —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö,
        # –∫–æ–≥–¥–∞ –º—ã –±—É–¥–µ–º –∑–Ω–∞—Ç—å, —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ —Å —Ü–µ–Ω–æ–π —á–µ—Ä–µ–∑ N —Å–µ–∫—É–Ω–¥
        "future_direction_10s": None,  # 1=up, 0=neutral, -1=down
        "future_direction_30s": None,
        "future_direction_60s": None,
        "future_movement_10s": None,  # % –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã
        "future_movement_30s": None,
        "future_movement_60s": None,

        # Current state
        "timestamp": orderbook_snapshot.timestamp,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –¥–æ–±–∞–≤–ª–µ–Ω timestamp –¥–ª—è preprocessing
        "current_mid_price": orderbook_snapshot.mid_price,
        "current_imbalance": market_metrics.imbalance,

        # ===== –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–Æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏–≥–Ω–∞–ª–µ =====
        "signal_type": executed_signal.get("type") if executed_signal else None,
        "signal_confidence": executed_signal.get("confidence") if executed_signal else None,
        "signal_strength": executed_signal.get("strength") if executed_signal else None,
      }

      return label

  async def _save_batch(self, symbol: str):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ batch –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Å–∫.

    Args:
        symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
    """
    try:
      if not self.feature_buffers[symbol]:
        return

      # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–∏–º–≤–æ–ª–∞
      symbol_dir = self.storage_path / symbol
      features_dir = symbol_dir / "features"
      labels_dir = symbol_dir / "labels"
      metadata_dir = symbol_dir / "metadata"

      for dir_path in [features_dir, labels_dir, metadata_dir]:
        dir_path.mkdir(parents=True, exist_ok=True)

      # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
      date_str = datetime.now().strftime("%Y-%m-%d")
      batch_num = self.batch_numbers[symbol]
      filename_base = f"{date_str}_batch_{batch_num:04d}"

      # –°–æ—Ö—Ä–∞–Ω—è–µ–º features (numpy)
      features_array = np.array(self.feature_buffers[symbol])
      features_file = features_dir / f"{filename_base}.npy"
      np.save(features_file, features_array)

      # –°–æ—Ö—Ä–∞–Ω—è–µ–º labels (numpy)
      # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º labels –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—Å–∏–≤
      labels_file = labels_dir / f"{filename_base}.json"
      with open(labels_file, 'w') as f:
        json.dump(self.label_buffers[symbol], f, indent=2)

      # –°–æ—Ö—Ä–∞–Ω—è–µ–º metadata (json)
      metadata_file = metadata_dir / f"{filename_base}.json"
      with open(metadata_file, 'w') as f:
        json.dump({
          "batch_info": {
            "symbol": symbol,
            "batch_number": batch_num,
            "sample_count": len(self.feature_buffers[symbol]),
            "timestamp": datetime.now().isoformat(),
            "feature_shape": features_array.shape
          },
          "samples": self.metadata_buffers[symbol]
        }, f, indent=2)

      self.files_written += 3  # features, labels, metadata

      logger.info(
        f"{symbol} | –°–æ—Ö—Ä–∞–Ω–µ–Ω batch #{batch_num}: "
        f"{len(self.feature_buffers[symbol])} —Å–µ–º–ø–ª–æ–≤, "
        f"features_shape={features_array.shape}"
      )

      # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä—ã
      self.feature_buffers[symbol].clear()
      self.label_buffers[symbol].clear()
      self.metadata_buffers[symbol].clear()

      # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∏—Ä—É–µ–º batch number
      self.batch_numbers[symbol] += 1

    except Exception as e:
      logger.error(f"{symbol} | –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è batch: {e}")

  async def finalize(self):
    """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –±—É—Ñ–µ—Ä–æ–≤."""
    logger.info("–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è MLDataCollector...")

    for symbol in self.feature_buffers.keys():
      if self.feature_buffers[symbol]:
        await self._save_batch(symbol)
        logger.info(f"{symbol} | –§–∏–Ω–∞–ª—å–Ω—ã–π batch —Å–æ—Ö—Ä–∞–Ω–µ–Ω")

    logger.info(
      f"MLDataCollector —Ñ–∏–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: "
      f"–≤—Å–µ–≥–æ —Å–µ–º–ø–ª–æ–≤={self.total_samples_collected}, "
      f"—Ñ–∞–π–ª–æ–≤={self.files_written}"
    )

  def _calculate_buffer_memory(self, symbol: str) -> float:
    """
    –†–∞—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –±—É—Ñ–µ—Ä–∞ –≤ –ø–∞–º—è—Ç–∏ (–ú–ë).

    Args:
        symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞

    Returns:
        float: –†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –≤ –ú–ë
    """
    if symbol not in self.feature_buffers:
      return 0.0

    try:
      # –†–∞–∑–º–µ—Ä feature –±—É—Ñ–µ—Ä–∞
      features_size = 0
      for arr in self.feature_buffers[symbol]:
        features_size += arr.nbytes

      # –†–∞–∑–º–µ—Ä label –±—É—Ñ–µ—Ä–∞ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
      labels_size = len(self.label_buffers[symbol]) * 200  # ~200 bytes per label

      # –†–∞–∑–º–µ—Ä metadata –±—É—Ñ–µ—Ä–∞ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
      metadata_size = len(self.metadata_buffers[symbol]) * 300  # ~300 bytes per metadata

      total_bytes = features_size + labels_size + metadata_size
      total_mb = total_bytes / (1024 * 1024)

      return total_mb

    except Exception as e:
      logger.error(f"{symbol} | –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞ –±—É—Ñ–µ—Ä–∞: {e}")
      return 0.0

  def _cleanup_old_buffers(self):
    """
    –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –±—É—Ñ–µ—Ä–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —ç–ª–µ–º–µ–Ω—Ç–æ–≤.
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏.
    """
    import gc

    cleaned_symbols = []

    for symbol in list(self.feature_buffers.keys()):
      buffer_size = len(self.feature_buffers[symbol])

      # –ï—Å–ª–∏ –±—É—Ñ–µ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 100 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100
      if buffer_size > 100:
        self.feature_buffers[symbol] = self.feature_buffers[symbol][-100:]
        self.label_buffers[symbol] = self.label_buffers[symbol][-100:]
        self.metadata_buffers[symbol] = self.metadata_buffers[symbol][-100:]
        cleaned_symbols.append(f"{symbol}({buffer_size}‚Üí100)")

    if cleaned_symbols:
      logger.warning(
        f"üßπ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –±—É—Ñ–µ—Ä–æ–≤: {', '.join(cleaned_symbols)}"
      )

    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
    gc.collect()
    logger.info("üßπ –°–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")

  def get_statistics(self) -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.

    Returns:
        Dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    """
    symbol_stats = {}
    for symbol in self.sample_counts.keys():
      symbol_stats[symbol] = {
        "total_samples": self.sample_counts[symbol],
        "current_batch": self.batch_numbers[symbol],
        "buffer_size": len(self.feature_buffers.get(symbol, []))
      }

    return {
      "total_samples_collected": self.total_samples_collected,
      "files_written": self.files_written,
      "iteration_counter": self.iteration_counter,
      "collection_interval": self.collection_interval,
      "symbols": symbol_stats
    }