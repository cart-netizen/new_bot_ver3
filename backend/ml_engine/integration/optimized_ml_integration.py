#!/usr/bin/env python3
"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π ML —Å–∏—Å—Ç–µ–º—ã.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–≤—è–∑—ã–≤–∞–µ—Ç –≤—Å–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π
–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –±–æ—Ç–∞. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è:
1. –°–æ–∑–¥–∞–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
2. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
3. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏

–ü—É—Ç—å: backend/ml_engine/integration/optimized_ml_integration.py
"""

import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
from dataclasses import dataclass, asdict
from collections import Counter

from backend.core.logger import get_logger

logger = get_logger(__name__)


# ============================================================================
# OPTIMIZED HYPERPARAMETERS
# ============================================================================

@dataclass
class OptimizedHyperparameters:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã - Industry Standard.
    
    –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:
    
    | –ü–∞—Ä–∞–º–µ—Ç—Ä       | –ë—ã–ª–æ    | –°—Ç–∞–ª–æ   | –ü—Ä–∏—á–∏–Ω–∞                    |
    |----------------|---------|---------|----------------------------|
    | learning_rate  | 0.001   | 5e-5    | –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —à—É–º–Ω—ã–µ   |
    | batch_size     | 64      | 256     | –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤    |
    | weight_decay   | ~0      | 0.01    | L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è           |
    | focal_gamma    | 2.0     | 2.5     | –§–æ–∫—É—Å –Ω–∞ hard examples     |
    | dropout        | 0.3     | 0.4     | –õ—É—á—à–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è       |
    | epochs         | 100     | 150     | –ë–æ–ª—å—à–µ —ç–ø–æ—Ö —Å –º–µ–Ω—å—à–∏–º LR   |
    """
    
    # === –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–º–µ–Ω—è—Ç—å –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å) ===
    learning_rate: float = 5e-5      # –ù–ï 0.001!
    batch_size: int = 256            # –ù–ï 64!
    weight_decay: float = 0.01       # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    
    # === Training ===
    epochs: int = 150
    early_stopping_patience: int = 20
    grad_clip_value: float = 1.0
    
    # === Scheduler ===
    scheduler_type: str = "cosine_warm_restarts"
    scheduler_T_0: int = 10
    scheduler_T_mult: int = 2
    scheduler_eta_min: float = 1e-7
    
    # === Label Smoothing ===
    label_smoothing: float = 0.1
    
    # === Data Augmentation ===
    use_augmentation: bool = True
    mixup_alpha: float = 0.2
    gaussian_noise_std: float = 0.01
    
    # === Class Balancing ===
    use_focal_loss: bool = True
    focal_gamma: float = 2.5
    use_class_weights: bool = True
    
    # === Model Architecture ===
    cnn_channels: Tuple[int, ...] = (32, 64, 128)
    lstm_hidden: int = 128
    dropout: float = 0.4
    use_residual: bool = True
    use_layer_norm: bool = True
    
    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å."""
        return asdict(self)
    
    def apply_to_trainer_config(self, trainer_config) -> None:
        """
        –ü—Ä–∏–º–µ–Ω–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É TrainerConfig.
        
        Args:
            trainer_config: –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π TrainerConfig –¥–ª—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        trainer_config.learning_rate = self.learning_rate
        trainer_config.weight_decay = self.weight_decay
        trainer_config.epochs = self.epochs
        trainer_config.early_stopping_patience = self.early_stopping_patience
        trainer_config.grad_clip_value = self.grad_clip_value
        
        # Scheduler
        if hasattr(trainer_config, 'lr_scheduler'):
            trainer_config.lr_scheduler = self.scheduler_type
        
        logger.info(f"‚úì –ü—Ä–∏–º–µ–Ω–µ–Ω—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    
    def apply_to_data_config(self, data_config) -> None:
        """
        –ü—Ä–∏–º–µ–Ω–∏—Ç—å batch_size –∫ DataConfig.
        
        Args:
            data_config: –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π DataConfig –¥–ª—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        data_config.batch_size = self.batch_size
        logger.info(f"‚úì Batch size –∏–∑–º–µ–Ω—ë–Ω –Ω–∞ {self.batch_size}")
    
    def apply_to_balancing_config(self, balancing_config) -> None:
        """
        –ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏.
        
        Args:
            balancing_config: –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π ClassBalancingConfig
        """
        balancing_config.use_focal_loss = self.use_focal_loss
        balancing_config.focal_gamma = self.focal_gamma
        balancing_config.use_class_weights = self.use_class_weights
        
        logger.info(f"‚úì –ü—Ä–∏–º–µ–Ω–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤")


# ============================================================================
# INTEGRATION FUNCTIONS
# ============================================================================

def create_optimized_model_config():
    """
    –°–æ–∑–¥–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏.

    Returns:
        ModelConfigV2 —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    # UPDATED: –ò—Å–ø–æ–ª—å–∑—É–µ–º v2 –≤–µ—Ä—Å–∏—é
    from backend.ml_engine.models.hybrid_cnn_lstm_v2 import ModelConfigV2 as ModelConfig

    return ModelConfig(
        input_features=110,
        sequence_length=60,
        cnn_channels=(32, 64, 128),  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –º–∞–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        lstm_hidden=128,             # –£–º–µ–Ω—å—à–µ–Ω–æ
        lstm_layers=2,
        attention_units=64,
        num_classes=3,
        dropout=0.4                  # –£–≤–µ–ª–∏—á–µ–Ω–æ
    )


def create_optimized_trainer_config(checkpoint_dir: str = "checkpoints/models"):
    """
    –°–æ–∑–¥–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é trainer.

    Args:
        checkpoint_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è checkpoints

    Returns:
        TrainerConfigV2 —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    # UPDATED: –ò—Å–ø–æ–ª—å–∑—É–µ–º v2 –≤–µ—Ä—Å–∏—é
    from backend.ml_engine.training.model_trainer_v2 import TrainerConfigV2 as TrainerConfig

    device = "cuda" if torch.cuda.is_available() else "cpu"

    return TrainerConfig(
        epochs=150,
        learning_rate=5e-5,          # –ö–†–ò–¢–ò–ß–ù–û!
        weight_decay=0.01,           # –ö–†–ò–¢–ò–ß–ù–û!
        grad_clip_value=1.0,
        early_stopping_patience=20,
        label_smoothing=0.1,
        scheduler_type="cosine_warm_restarts",
        scheduler_T_0=10,
        scheduler_T_mult=2,
        checkpoint_dir=checkpoint_dir,
        save_best_only=True,
        device=device
    )


def create_optimized_data_config(storage_path: str = "data/ml_training"):
    """
    –°–æ–∑–¥–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö.
    
    Args:
        storage_path: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
    
    Returns:
        DataConfig —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    from backend.ml_engine.training.data_loader import DataConfig
    
    return DataConfig(
        storage_path=storage_path,
        sequence_length=60,
        target_horizon="future_direction_60s",
        train_ratio=0.7,
        val_ratio=0.15,
        test_ratio=0.15,
        batch_size=256,              # –ö–†–ò–¢–ò–ß–ù–û!
        shuffle=True,
        num_workers=4
    )


def create_optimized_balancing_config():
    """
    –°–æ–∑–¥–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤.
    
    Returns:
        ClassBalancingConfig —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    from backend.ml_engine.training.class_balancing import ClassBalancingConfig
    
    return ClassBalancingConfig(
        use_class_weights=True,
        use_focal_loss=True,
        focal_gamma=2.5,             # –ö–†–ò–¢–ò–ß–ù–û: —É–≤–µ–ª–∏—á–µ–Ω–æ —Å 2.0
        use_oversampling=True,
        oversample_ratio=0.5,
        use_undersampling=False,
        undersample_ratio=0.8
    )


def setup_optimized_training():
    """
    –ü–æ–ª–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è.
    
    Returns:
        Tuple –∏–∑ (model_config, trainer_config, data_config, balancing_config)
    """
    model_config = create_optimized_model_config()
    trainer_config = create_optimized_trainer_config()
    data_config = create_optimized_data_config()
    balancing_config = create_optimized_balancing_config()
    
    logger.info("\n" + "=" * 80)
    logger.info("–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø")
    logger.info("=" * 80)
    logger.info("Model:")
    logger.info(f"  ‚Ä¢ CNN channels: {model_config.cnn_channels}")
    logger.info(f"  ‚Ä¢ LSTM hidden: {model_config.lstm_hidden}")
    logger.info(f"  ‚Ä¢ Dropout: {model_config.dropout}")
    logger.info("\nTraining:")
    logger.info(f"  ‚Ä¢ Learning rate: {trainer_config.learning_rate}")
    logger.info(f"  ‚Ä¢ Batch size: {data_config.batch_size}")
    logger.info(f"  ‚Ä¢ Weight decay: {trainer_config.weight_decay}")
    logger.info(f"  ‚Ä¢ Epochs: {trainer_config.epochs}")
    logger.info("\nClass Balancing:")
    logger.info(f"  ‚Ä¢ Focal Loss: gamma={balancing_config.focal_gamma}")
    logger.info(f"  ‚Ä¢ Class weights: {balancing_config.use_class_weights}")
    logger.info(f"  ‚Ä¢ Oversampling: ratio={balancing_config.oversample_ratio}")
    logger.info("=" * 80 + "\n")
    
    return model_config, trainer_config, data_config, balancing_config


# ============================================================================
# LOSS FUNCTION SETUP
# ============================================================================

def create_optimized_loss(
    train_labels: np.ndarray,
    device: str = "cpu"
) -> nn.Module:
    """
    –°–æ–∑–¥–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é loss function.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Focal Loss —Å Label Smoothing –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –≤–µ—Å–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤.
    
    Args:
        train_labels: –ú–µ—Ç–∫–∏ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –≤–µ—Å–æ–≤
        device: Device –¥–ª—è tensors
    
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è loss function
    """
    from backend.ml_engine.training.class_balancing import ClassBalancingStrategy, ClassBalancingConfig
    
    # –†–∞—Å—á—ë—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤
    class_counts = Counter(train_labels)
    num_classes = len(class_counts)
    total = len(train_labels)
    
    # Balanced weights
    weights = np.array([
        total / (num_classes * class_counts.get(i, 1))
        for i in range(num_classes)
    ], dtype=np.float32)
    
    # Normalize
    weights = weights * num_classes / weights.sum()
    
    logger.info(f"Class weights: {weights}")
    
    # –°–æ–∑–¥–∞—ë–º Focal Loss
    from backend.ml_engine.training.class_balancing import FocalLoss
    
    focal_loss = FocalLoss(
        gamma=2.5,
        alpha=torch.tensor(weights, device=device),
        reduction='mean'
    )
    
    return focal_loss


# ============================================================================
# SCHEDULER SETUP
# ============================================================================

def create_optimized_scheduler(
    optimizer: torch.optim.Optimizer,
    scheduler_type: str = "cosine_warm_restarts",
    **kwargs
) -> torch.optim.lr_scheduler._LRScheduler:
    """
    –°–æ–∑–¥–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LR scheduler.
    
    Args:
        optimizer: Optimizer
        scheduler_type: –¢–∏–ø scheduler
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π scheduler
    """
    if scheduler_type == "cosine_warm_restarts":
        return torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer,
            T_0=kwargs.get("T_0", 10),
            T_mult=kwargs.get("T_mult", 2),
            eta_min=kwargs.get("eta_min", 1e-7)
        )
    
    elif scheduler_type == "reduce_on_plateau":
        return torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            patience=kwargs.get("patience", 5),
            factor=kwargs.get("factor", 0.5),
            min_lr=kwargs.get("min_lr", 1e-7)
        )
    
    elif scheduler_type == "one_cycle":
        return torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=kwargs.get("max_lr", 1e-4),
            total_steps=kwargs.get("total_steps", 1000),
            pct_start=0.3,
            anneal_strategy='cos'
        )
    
    else:
        return torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=kwargs.get("T_max", 100),
            eta_min=kwargs.get("eta_min", 1e-7)
        )


# ============================================================================
# MIGRATION HELPERS
# ============================================================================

def migrate_existing_config(existing_config: Any) -> Any:
    """
    –ú–∏–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º.
    
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –æ–±–Ω–æ–≤–ª—è—è —Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.
    
    Args:
        existing_config: –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–ª—é–±–æ–≥–æ —Ç–∏–ø–∞)
    
    Returns:
        –û–±–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    """
    optimized = OptimizedHyperparameters()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    if hasattr(existing_config, 'learning_rate'):
        if existing_config.learning_rate >= 0.001:
            logger.warning(
                f"‚ö†Ô∏è Learning rate —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π: {existing_config.learning_rate} ‚Üí {optimized.learning_rate}"
            )
            existing_config.learning_rate = optimized.learning_rate
    
    if hasattr(existing_config, 'batch_size'):
        if existing_config.batch_size < 128:
            logger.warning(
                f"‚ö†Ô∏è Batch size —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π: {existing_config.batch_size} ‚Üí {optimized.batch_size}"
            )
            existing_config.batch_size = optimized.batch_size
    
    if hasattr(existing_config, 'weight_decay'):
        if existing_config.weight_decay < 0.001:
            logger.warning(
                f"‚ö†Ô∏è Weight decay —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π: {existing_config.weight_decay} ‚Üí {optimized.weight_decay}"
            )
            existing_config.weight_decay = optimized.weight_decay
    
    return existing_config


def validate_config(config: Any) -> List[str]:
    """
    –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º.
    
    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    
    Returns:
        –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
    """
    warnings = []
    
    if hasattr(config, 'learning_rate'):
        if config.learning_rate > 1e-4:
            warnings.append(
                f"Learning rate ({config.learning_rate}) —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π. "
                f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 1e-5 - 1e-4."
            )
    
    if hasattr(config, 'batch_size'):
        if config.batch_size < 128:
            warnings.append(
                f"Batch size ({config.batch_size}) —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π. "
                f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 128-256."
            )
    
    if hasattr(config, 'weight_decay'):
        if config.weight_decay < 0.001:
            warnings.append(
                f"Weight decay ({config.weight_decay}) —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π. "
                f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 0.001-0.01."
            )
    
    if hasattr(config, 'focal_gamma'):
        if config.focal_gamma < 2.0:
            warnings.append(
                f"Focal gamma ({config.focal_gamma}) –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º. "
                f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 2.0-3.0."
            )
    
    return warnings


# ============================================================================
# QUICK START
# ============================================================================

def quick_start_training(
    symbols: List[str],
    data_path: str = "data/ml_training",
    output_dir: str = "checkpoints/optimized"
):
    """
    –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
    
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        from backend.ml_engine.integration.optimized_ml_integration import quick_start_training
        quick_start_training(["BTCUSDT", "ETHUSDT"])
    
    Args:
        symbols: –°–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä
        data_path: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    # UPDATED: –ò—Å–ø–æ–ª—å–∑—É–µ–º v2 –≤–µ—Ä—Å–∏–∏
    from backend.ml_engine.models.hybrid_cnn_lstm_v2 import HybridCNNLSTMv2 as HybridCNNLSTM
    from backend.ml_engine.training.model_trainer_v2 import ModelTrainerV2 as ModelTrainer
    from backend.ml_engine.training.data_loader import HistoricalDataLoader

    # 1. –ü–æ–ª—É—á–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    model_config, trainer_config, data_config, balancing_config = setup_optimized_training()

    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç–∏
    data_config.storage_path = data_path
    trainer_config.checkpoint_dir = output_dir

    # 2. –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
    model = HybridCNNLSTM(model_config)
    logger.info(f"‚úì –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")

    # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    data_loader = HistoricalDataLoader(data_config, balancing_config)
    result = data_loader.load_and_prepare(symbols, apply_resampling=True)
    dataloaders = result['dataloaders']
    logger.info(f"‚úì –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

    # 4. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º trainer —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π (v2 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç create_trainer_v2)
    from backend.ml_engine.training.model_trainer_v2 import create_trainer_v2
    trainer = create_trainer_v2(model, trainer_config)

    # 5. –û–±—É—á–∞–µ–º
    history = trainer.train(dataloaders['train'], dataloaders['val'])

    logger.info(f"‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    logger.info(f"  ‚Ä¢ –≠–ø–æ—Ö: {len(history)}")
    logger.info(f"  ‚Ä¢ Best val_loss: {trainer.best_val_loss:.4f}")

    return model, history


# ============================================================================
# EXAMPLE
# ============================================================================

if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print("=" * 80)
    print("OPTIMIZED ML INTEGRATION")
    print("=" * 80)
    
    # –ü–æ–ª—É—á–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    params = OptimizedHyperparameters()
    
    print("\nüìä –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    for key, value in params.to_dict().items():
        print(f"  ‚Ä¢ {key}: {value}")
    
    # Setup –ø–æ–ª–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    print("\nüîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
    model_cfg, trainer_cfg, data_cfg, balance_cfg = setup_optimized_training()
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    print("\n‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π...")
    warnings = validate_config(trainer_cfg)
    if warnings:
        print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:")
        for w in warnings:
            print(f"  ‚Ä¢ {w}")
    else:
        print("–í—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã!")
    
    print("\n" + "=" * 80)
