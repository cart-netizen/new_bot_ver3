#!/usr/bin/env python3
"""
TLOB - Transformer for Limit Order Book.

–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞–∫–∞–Ω–∞ –æ—Ä–¥–µ—Ä–æ–≤ (LOB).
–û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ "Transformers for Limit Order Books" (2023).

–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
1. Spatial CNN: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç–∞–∫–∞–Ω–∞
2. Temporal Transformer: –ó–∞—Ö–≤–∞—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
3. Multi-scale Analysis: –ê–Ω–∞–ª–∏–∑ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö
4. HFT-optimized: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
- Raw LOB tensor: (batch, sequence, levels, 4)
- 4 –∫–∞–Ω–∞–ª–∞: bid_price, bid_volume, ask_price, ask_volume

–ü—É—Ç—å: backend/ml_engine/models/tlob_transformer.py
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Tuple, Optional, List
from dataclasses import dataclass

from backend.core.logger import get_logger

logger = get_logger(__name__)


# ============================================================================
# CONFIGURATION
# ============================================================================

@dataclass
class TLOBConfig:
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è TLOB –º–æ–¥–µ–ª–∏.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞–∫–∞–Ω–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç.
    """

    # === Input –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    num_levels: int = 20  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Ä–æ–≤–Ω–µ–π —Å—Ç–∞–∫–∞–Ω–∞
    num_features: int = 4  # bid_price, bid_vol, ask_price, ask_vol
    sequence_length: int = 60  # –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ

    # === Spatial CNN ===
    cnn_channels: Tuple[int, ...] = (32, 64, 128)
    cnn_kernel_sizes: Tuple[int, ...] = (3, 3, 3)
    cnn_dropout: float = 0.2

    # === Temporal Transformer ===
    embed_dim: int = 128
    num_layers: int = 4
    num_heads: int = 4
    mlp_ratio: float = 2.0
    dropout: float = 0.1
    attention_dropout: float = 0.1

    # === Multi-scale ===
    use_multi_scale: bool = True
    scales: Tuple[int, ...] = (1, 5, 15)  # 1 = –∫–∞–∂–¥—ã–π —à–∞–≥, 5 = –∫–∞–∂–¥—ã–µ 5, 15 = –∫–∞–∂–¥—ã–µ 15

    # === Output ===
    num_classes: int = 3  # SELL, HOLD, BUY
    prediction_horizons: Tuple[int, ...] = (10, 30, 60)  # –ì–æ—Ä–∏–∑–æ–Ω—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —à–∞–≥–∞—Ö

    # === Regularization ===
    drop_path_rate: float = 0.1
    layer_norm_eps: float = 1e-6

    # === Auxiliary Features ===
    use_auxiliary_features: bool = True  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (spread, imbalance)


# ============================================================================
# BUILDING BLOCKS
# ============================================================================

class SpatialCNNEncoder(nn.Module):
    """
    Spatial CNN –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç–∞–∫–∞–Ω–∞.

    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É LOB:
    - –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ: —É—Ä–æ–≤–Ω–∏ —Ü–µ–Ω (bid/ask levels)
    - –ö–∞–Ω–∞–ª—ã: price + volume –¥–ª—è bid –∏ ask
    """

    def __init__(self, config: TLOBConfig):
        super().__init__()

        self.config = config

        # –ù–∞—á–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: (num_levels, num_features) = (20, 4)
        in_channels = config.num_features

        layers = []
        for i, (out_channels, kernel_size) in enumerate(
            zip(config.cnn_channels, config.cnn_kernel_sizes)
        ):
            layers.extend([
                nn.Conv1d(
                    in_channels=in_channels,
                    out_channels=out_channels,
                    kernel_size=kernel_size,
                    padding=kernel_size // 2
                ),
                nn.BatchNorm1d(out_channels),
                nn.GELU(),
                nn.Dropout(config.cnn_dropout)
            ])
            in_channels = out_channels

        self.encoder = nn.Sequential(*layers)

        # Global pooling –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        self.global_pool = nn.AdaptiveAvgPool1d(1)

        # –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–∞
        self.output_dim = config.cnn_channels[-1]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (batch, num_levels, num_features)

        Returns:
            (batch, output_dim) - –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        """
        # Transpose –¥–ª—è Conv1d: (batch, features, levels)
        x = x.transpose(1, 2)

        # CNN encoding
        x = self.encoder(x)

        # Global pooling
        x = self.global_pool(x).squeeze(-1)

        return x


class TemporalTransformerBlock(nn.Module):
    """
    Transformer –±–ª–æ–∫ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.
    """

    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        mlp_ratio: float = 2.0,
        dropout: float = 0.1,
        attention_dropout: float = 0.1,
        drop_path: float = 0.0,
        layer_norm_eps: float = 1e-6
    ):
        super().__init__()

        self.norm1 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)

        self.attn = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            dropout=attention_dropout,
            batch_first=True
        )

        self.norm2 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)

        hidden_dim = int(embed_dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, embed_dim),
            nn.Dropout(dropout)
        )

        self.drop_path = nn.Identity()
        if drop_path > 0:
            self.drop_path = DropPath(drop_path)

    def forward(
        self,
        x: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: (batch, seq_len, embed_dim)
            attn_mask: Optional attention mask

        Returns:
            output: (batch, seq_len, embed_dim)
            attention_weights: (batch, seq_len, seq_len)
        """
        # Self-attention with residual
        x_norm = self.norm1(x)
        attn_out, attn_weights = self.attn(
            x_norm, x_norm, x_norm,
            attn_mask=attn_mask,
            need_weights=True
        )
        x = x + self.drop_path(attn_out)

        # MLP with residual
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x, attn_weights


class DropPath(nn.Module):
    """Stochastic Depth."""

    def __init__(self, drop_prob: float = 0.0):
        super().__init__()
        self.drop_prob = drop_prob

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.drop_prob == 0.0 or not self.training:
            return x

        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = x.new_empty(shape).bernoulli_(keep_prob)

        return x.div(keep_prob) * random_tensor


class MultiScaleEncoder(nn.Module):
    """
    Multi-scale encoding –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö.
    """

    def __init__(self, config: TLOBConfig, input_dim: int):
        super().__init__()

        self.config = config
        self.scales = config.scales

        # Encoder –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞
        self.scale_encoders = nn.ModuleDict()

        for scale in self.scales:
            self.scale_encoders[f"scale_{scale}"] = nn.Sequential(
                nn.Linear(input_dim, config.embed_dim),
                nn.LayerNorm(config.embed_dim),
                nn.GELU(),
                nn.Dropout(config.dropout)
            )

        # Fusion layer
        self.fusion = nn.Sequential(
            nn.Linear(config.embed_dim * len(self.scales), config.embed_dim),
            nn.LayerNorm(config.embed_dim),
            nn.GELU()
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (batch, seq_len, input_dim)

        Returns:
            (batch, seq_len, embed_dim)
        """
        B, T, D = x.shape
        scale_outputs = []

        for scale in self.scales:
            # Subsample –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            if scale > 1:
                indices = torch.arange(0, T, scale, device=x.device)
                x_scaled = x[:, indices, :]

                # Upsample –æ–±—Ä–∞—Ç–Ω–æ
                x_scaled = F.interpolate(
                    x_scaled.transpose(1, 2),
                    size=T,
                    mode='linear',
                    align_corners=False
                ).transpose(1, 2)
            else:
                x_scaled = x

            # Encode
            encoded = self.scale_encoders[f"scale_{scale}"](x_scaled)
            scale_outputs.append(encoded)

        # Concatenate –∏ fusion
        x = torch.cat(scale_outputs, dim=-1)
        x = self.fusion(x)

        return x


class AuxiliaryFeatureEncoder(nn.Module):
    """
    Encoder –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ LOB (spread, imbalance, etc).
    """

    def __init__(self, config: TLOBConfig, output_dim: int):
        super().__init__()

        # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ LOB –¥–∞–Ω–Ω—ã—Ö
        # spread, mid_price_change, imbalance_5, imbalance_10, volume_ratio
        num_aux_features = 8

        self.encoder = nn.Sequential(
            nn.Linear(num_aux_features, 32),
            nn.GELU(),
            nn.Linear(32, output_dim)
        )

    def compute_auxiliary_features(self, lob: torch.Tensor) -> torch.Tensor:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ LOB.

        Args:
            lob: (batch, seq_len, num_levels, 4) - [bid_price, bid_vol, ask_price, ask_vol]

        Returns:
            (batch, seq_len, num_aux_features)
        """
        B, T, L, _ = lob.shape

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–Ω–∞–ª—ã
        bid_prices = lob[:, :, :, 0]  # (B, T, L)
        bid_volumes = lob[:, :, :, 1]
        ask_prices = lob[:, :, :, 2]
        ask_volumes = lob[:, :, :, 3]

        # Best bid/ask
        best_bid = bid_prices[:, :, 0]  # (B, T)
        best_ask = ask_prices[:, :, 0]

        # Spread
        spread = best_ask - best_bid
        mid_price = (best_bid + best_ask) / 2
        relative_spread = spread / (mid_price + 1e-8)

        # Mid price change
        mid_price_change = torch.zeros_like(mid_price)
        mid_price_change[:, 1:] = (mid_price[:, 1:] - mid_price[:, :-1]) / (mid_price[:, :-1] + 1e-8)

        # Imbalance at different levels
        bid_vol_5 = bid_volumes[:, :, :5].sum(dim=-1)
        ask_vol_5 = ask_volumes[:, :, :5].sum(dim=-1)
        imbalance_5 = (bid_vol_5 - ask_vol_5) / (bid_vol_5 + ask_vol_5 + 1e-8)

        bid_vol_10 = bid_volumes[:, :, :10].sum(dim=-1)
        ask_vol_10 = ask_volumes[:, :, :10].sum(dim=-1)
        imbalance_10 = (bid_vol_10 - ask_vol_10) / (bid_vol_10 + ask_vol_10 + 1e-8)

        # Volume ratio
        total_bid_vol = bid_volumes.sum(dim=-1)
        total_ask_vol = ask_volumes.sum(dim=-1)
        volume_ratio = total_bid_vol / (total_ask_vol + 1e-8)

        # VWAP distance
        weighted_bid = (bid_prices * bid_volumes).sum(dim=-1) / (total_bid_vol + 1e-8)
        weighted_ask = (ask_prices * ask_volumes).sum(dim=-1) / (total_ask_vol + 1e-8)
        vwap_spread = (weighted_ask - weighted_bid) / (mid_price + 1e-8)

        # Stack features
        aux_features = torch.stack([
            relative_spread,
            mid_price_change,
            imbalance_5,
            imbalance_10,
            volume_ratio.clamp(-5, 5),  # Clip outliers
            vwap_spread,
            torch.log1p(total_bid_vol),
            torch.log1p(total_ask_vol)
        ], dim=-1)

        return aux_features

    def forward(self, lob: torch.Tensor) -> torch.Tensor:
        """
        Args:
            lob: (batch, seq_len, num_levels, 4)

        Returns:
            (batch, seq_len, output_dim)
        """
        aux_features = self.compute_auxiliary_features(lob)
        return self.encoder(aux_features)


# ============================================================================
# MAIN MODEL
# ============================================================================

class TLOBTransformer(nn.Module):
    """
    TLOB - Transformer for Limit Order Book.

    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    1. Spatial CNN: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç–∞–∫–∞–Ω–∞
    2. Multi-scale Encoding: –ê–Ω–∞–ª–∏–∑ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö
    3. Temporal Transformer: –ó–∞—Ö–≤–∞—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    4. Multi-horizon Prediction: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞—Ö

    Input:
        Raw LOB: (batch, sequence, levels, 4)
        4 channels: [bid_price, bid_volume, ask_price, ask_volume]

    Output:
        - direction_logits: (batch, num_classes)
        - confidence: (batch, 1)
        - expected_return: (batch, 1)
        - horizon_predictions: Dict[horizon, predictions] (optional)
    """

    def __init__(self, config: TLOBConfig):
        super().__init__()

        self.config = config

        # ==================== SPATIAL CNN ====================
        self.spatial_encoder = SpatialCNNEncoder(config)
        spatial_dim = self.spatial_encoder.output_dim

        # ==================== AUXILIARY FEATURES ====================
        if config.use_auxiliary_features:
            self.aux_encoder = AuxiliaryFeatureEncoder(config, spatial_dim)
            combined_dim = spatial_dim * 2
        else:
            self.aux_encoder = None
            combined_dim = spatial_dim

        # ==================== MULTI-SCALE ENCODING ====================
        if config.use_multi_scale:
            self.multi_scale = MultiScaleEncoder(config, combined_dim)
            embed_dim = config.embed_dim
        else:
            self.multi_scale = None
            embed_dim = combined_dim
            self.projection = nn.Linear(combined_dim, config.embed_dim)

        # ==================== POSITIONAL ENCODING ====================
        self.pos_embed = nn.Parameter(
            torch.randn(1, config.sequence_length, config.embed_dim) * 0.02
        )

        # ==================== TEMPORAL TRANSFORMER ====================
        dpr = [
            config.drop_path_rate * i / (config.num_layers - 1)
            for i in range(config.num_layers)
        ]

        self.transformer_blocks = nn.ModuleList([
            TemporalTransformerBlock(
                embed_dim=config.embed_dim,
                num_heads=config.num_heads,
                mlp_ratio=config.mlp_ratio,
                dropout=config.dropout,
                attention_dropout=config.attention_dropout,
                drop_path=dpr[i],
                layer_norm_eps=config.layer_norm_eps
            )
            for i in range(config.num_layers)
        ])

        self.norm = nn.LayerNorm(config.embed_dim, eps=config.layer_norm_eps)

        # ==================== OUTPUT HEADS ====================

        # Main classification head
        self.direction_head = nn.Sequential(
            nn.Linear(config.embed_dim, config.embed_dim // 2),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.embed_dim // 2, config.num_classes)
        )

        # Confidence head
        self.confidence_head = nn.Sequential(
            nn.Linear(config.embed_dim, config.embed_dim // 4),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.embed_dim // 4, 1),
            nn.Sigmoid()
        )

        # Return prediction head
        self.return_head = nn.Sequential(
            nn.Linear(config.embed_dim, config.embed_dim // 4),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.embed_dim // 4, 1)
        )

        # Multi-horizon heads (optional)
        self.horizon_heads = nn.ModuleDict()
        for horizon in config.prediction_horizons:
            self.horizon_heads[f"horizon_{horizon}"] = nn.Sequential(
                nn.Linear(config.embed_dim, config.embed_dim // 4),
                nn.GELU(),
                nn.Dropout(config.dropout),
                nn.Linear(config.embed_dim // 4, config.num_classes)
            )

        # ==================== INITIALIZATION ====================
        self._init_weights()

        # Logging
        model_size = self.get_model_size()
        logger.info(
            f"‚úì TLOBTransformer initialized: "
            f"params={model_size['total_params']:,}, "
            f"levels={config.num_levels}, "
            f"embed_dim={config.embed_dim}, "
            f"layers={config.num_layers}"
        )

    def _init_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤."""
        for name, module in self.named_modules():
            if isinstance(module, nn.Linear):
                nn.init.trunc_normal_(module.weight, std=0.02)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Conv1d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out')
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(
        self,
        x: torch.Tensor,
        return_attention: bool = False,
        return_horizons: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass.

        Args:
            x: (batch, sequence_length, num_levels, num_features)
               –∏–ª–∏ (batch, sequence_length, num_levels * num_features) - –±—É–¥–µ—Ç reshape
            return_attention: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ attention weights
            return_horizons: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤

        Returns:
            Dict —Å –≤—ã—Ö–æ–¥–∞–º–∏
        """
        B = x.size(0)
        T = x.size(1)

        # Reshape –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if x.dim() == 3:
            # (B, T, L*F) -> (B, T, L, F)
            x = x.reshape(B, T, self.config.num_levels, self.config.num_features)

        # ==================== SPATIAL ENCODING ====================
        # Process each timestep through spatial CNN
        spatial_features = []

        for t in range(T):
            lob_t = x[:, t, :, :]  # (B, L, F)
            spatial_t = self.spatial_encoder(lob_t)  # (B, spatial_dim)
            spatial_features.append(spatial_t)

        spatial_out = torch.stack(spatial_features, dim=1)  # (B, T, spatial_dim)

        # ==================== AUXILIARY FEATURES ====================
        if self.aux_encoder is not None:
            aux_out = self.aux_encoder(x)  # (B, T, spatial_dim)
            combined = torch.cat([spatial_out, aux_out], dim=-1)
        else:
            combined = spatial_out

        # ==================== MULTI-SCALE ENCODING ====================
        if self.multi_scale is not None:
            temporal_in = self.multi_scale(combined)
        else:
            temporal_in = self.projection(combined)

        # ==================== POSITIONAL ENCODING ====================
        temporal_in = temporal_in + self.pos_embed[:, :T, :]

        # ==================== TEMPORAL TRANSFORMER ====================
        attention_weights = []

        for block in self.transformer_blocks:
            temporal_in, attn = block(temporal_in)
            if return_attention:
                attention_weights.append(attn)

        temporal_out = self.norm(temporal_in)

        # ==================== POOLING ====================
        # Use last timestep + mean
        pooled = temporal_out[:, -1, :] + temporal_out.mean(dim=1)

        # ==================== OUTPUT HEADS ====================
        direction_logits = self.direction_head(pooled)
        confidence = self.confidence_head(pooled)
        expected_return = self.return_head(pooled)

        outputs = {
            'direction_logits': direction_logits,
            'confidence': confidence,
            'expected_return': expected_return
        }

        if return_attention:
            outputs['attention_weights'] = attention_weights

        if return_horizons:
            outputs['horizon_predictions'] = {}
            for horizon in self.config.prediction_horizons:
                outputs['horizon_predictions'][horizon] = \
                    self.horizon_heads[f"horizon_{horizon}"](pooled)

        return outputs

    def predict(
        self,
        x: torch.Tensor,
        temperature: float = 1.0,
        confidence_threshold: float = 0.0
    ) -> Dict[str, torch.Tensor]:
        """
        Inference.

        Args:
            x: Input LOB data
            temperature: Temperature scaling
            confidence_threshold: Minimum confidence

        Returns:
            Dict —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
        """
        self.eval()

        with torch.no_grad():
            outputs = self.forward(x, return_attention=False, return_horizons=False)

            direction_logits = outputs['direction_logits'] / temperature
            direction_probs = F.softmax(direction_logits, dim=-1)
            direction = torch.argmax(direction_probs, dim=-1)

            confidence = outputs['confidence'].squeeze(-1)
            expected_return = outputs['expected_return'].squeeze(-1)

            should_trade = confidence >= confidence_threshold

            return {
                'direction': direction,
                'direction_probs': direction_probs,
                'confidence': confidence,
                'expected_return': expected_return,
                'should_trade': should_trade
            }

    def get_model_size(self) -> Dict[str, int]:
        """–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(
            p.numel() for p in self.parameters() if p.requires_grad
        )

        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'non_trainable_params': total_params - trainable_params
        }


# ============================================================================
# FACTORY FUNCTIONS
# ============================================================================

def create_tlob_transformer(
    config: Optional[TLOBConfig] = None
) -> TLOBTransformer:
    """–°–æ–∑–¥–∞–µ—Ç TLOB –º–æ–¥–µ–ª—å."""
    if config is None:
        config = TLOBConfig()

    return TLOBTransformer(config)


def create_tlob_from_preset(preset: str = "base") -> TLOBTransformer:
    """
    –°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –ø—Ä–µ—Å–µ—Ç–∞.

    –ü—Ä–µ—Å–µ—Ç—ã:
        - tiny: –î–ª—è –±—ã—Å—Ç—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
        - base: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        - hft: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è HFT (–º–µ–Ω—å—à–µ latency)
    """
    if preset == "tiny":
        config = TLOBConfig(
            num_levels=10,
            cnn_channels=(16, 32, 64),
            embed_dim=64,
            num_layers=2,
            num_heads=2
        )

    elif preset == "base":
        config = TLOBConfig(
            num_levels=20,
            cnn_channels=(32, 64, 128),
            embed_dim=128,
            num_layers=4,
            num_heads=4
        )

    elif preset == "hft":
        config = TLOBConfig(
            num_levels=10,
            sequence_length=30,
            cnn_channels=(32, 64),
            embed_dim=64,
            num_layers=2,
            num_heads=2,
            use_multi_scale=False,
            dropout=0.05
        )

    else:
        raise ValueError(f"Unknown preset: {preset}")

    return create_tlob_transformer(config)


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("TLOB TRANSFORMER TEST")
    print("=" * 80)

    # Create config
    config = TLOBConfig(
        num_levels=20,
        sequence_length=60,
        embed_dim=128,
        num_layers=4,
        num_heads=4
    )

    print(f"\nüìã Config:")
    print(f"   ‚Ä¢ Levels: {config.num_levels}")
    print(f"   ‚Ä¢ Sequence: {config.sequence_length}")
    print(f"   ‚Ä¢ Embed dim: {config.embed_dim}")
    print(f"   ‚Ä¢ Layers: {config.num_layers}")
    print(f"   ‚Ä¢ Multi-scale: {config.use_multi_scale}")

    # Create model
    model = create_tlob_transformer(config)

    # Test input: (batch, sequence, levels, 4)
    batch_size = 32
    x = torch.randn(
        batch_size,
        config.sequence_length,
        config.num_levels,
        config.num_features
    )

    print(f"\nüìä Input shape: {x.shape}")

    print("\nüîÑ Forward pass...")
    outputs = model(x, return_attention=False, return_horizons=True)

    print("\nüì§ Outputs:")
    for key, value in outputs.items():
        if isinstance(value, torch.Tensor):
            print(f"   ‚Ä¢ {key}: {value.shape}")
        elif isinstance(value, dict):
            print(f"   ‚Ä¢ {key}:")
            for k, v in value.items():
                print(f"      - {k}: {v.shape}")

    # Inference
    print("\nüéØ Inference...")
    predictions = model.predict(x, temperature=1.0, confidence_threshold=0.5)

    print("\nüìä Predictions:")
    for key, value in predictions.items():
        if isinstance(value, torch.Tensor):
            print(f"   ‚Ä¢ {key}: {value.shape}")

    # Model info
    print("\nüìà Model info:")
    size_info = model.get_model_size()
    for key, value in size_info.items():
        print(f"   ‚Ä¢ {key}: {value:,}")

    print("\n" + "=" * 80)
    print("‚úÖ TLOB Transformer test passed!")
    print("=" * 80)
