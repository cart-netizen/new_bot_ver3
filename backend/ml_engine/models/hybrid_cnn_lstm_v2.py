#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–∏–±—Ä–∏–¥–Ω–∞—è CNN-LSTM –º–æ–¥–µ–ª—å v2 - Industry Standard.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏:
1. Residual Connections –≤ CNN –±–ª–æ–∫–∞—Ö
2. Multi-Head Temporal Attention (–≤–º–µ—Å—Ç–æ single-head)
3. Layer Normalization (–≤–º–µ—Å—Ç–æ BatchNorm –≤ LSTM —á–∞—Å—Ç–∏)
4. Improved Weight Initialization (orthogonal –¥–ª—è LSTM)
5. Stochastic Depth (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
6. Squeeze-and-Excitation blocks (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:
- –ú–æ–¥–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –º–∞–ª—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (7-30 –¥–Ω–µ–π)
- –°–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ dropout, layer norm
- Backward compatible —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º API

–ü—É—Ç—å: backend/ml_engine/models/hybrid_cnn_lstm_v2.py
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.checkpoint import checkpoint
from typing import Dict, Tuple, Optional, List
from dataclasses import dataclass, field

from backend.core.logger import get_logger

logger = get_logger(__name__)


# ============================================================================
# CONFIGURATION
# ============================================================================

@dataclass
class ModelConfigV2:
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ v2.
    
    –°–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å –±–∞–∑–æ–≤–æ–π ModelConfig, –Ω–æ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.
    """
    # === Input –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    # input_features: —Ç–æ–ª—å–∫–æ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏, –º–æ–¥–µ–ª—å –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏
    # –ü–æ—Å–ª–µ preprocessing_v2: 112 base + 60 lagged + 15 derived = 187 features
    input_features: int = 187
    sequence_length: int = 60
    
    # === CNN –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    cnn_channels: Tuple[int, ...] = (32, 64, 128)
    cnn_kernel_sizes: Tuple[int, ...] = (3, 5, 7)
    
    # === LSTM –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    lstm_hidden: int = 128
    lstm_layers: int = 2
    lstm_dropout: float = 0.3
    
    # === Attention –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    attention_units: int = 64
    attention_heads: int = 4  # Multi-Head Attention
    attention_dropout: float = 0.1
    
    # === Output –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    num_classes: int = 3
    
    # === Regularization ===
    dropout: float = 0.4
    
    # === –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è ===
    use_residual: bool = True
    use_layer_norm: bool = True
    use_multi_head_attention: bool = True
    use_se_block: bool = False  # Squeeze-and-Excitation
    stochastic_depth_prob: float = 0.0  # 0 = off

    # === Memory optimization ===
    use_gradient_checkpointing: bool = False  # Enable only if OOM (may cause NaN with mixed precision)

    # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
    init_method: str = "kaiming"  # kaiming, xavier, orthogonal


# ============================================================================
# BUILDING BLOCKS
# ============================================================================

class ResidualConvBlock(nn.Module):
    """
    CNN –±–ª–æ–∫ —Å Residual Connection.
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    input ‚Üí Conv ‚Üí BN ‚Üí ReLU ‚Üí Dropout ‚Üí output
        ‚Üì                                    ‚Üë
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ skip connection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    Skip connection –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 1x1 conv –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç.
    """
    
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        dropout: float = 0.3,
        use_residual: bool = True,
        stochastic_depth_prob: float = 0.0
    ):
        super().__init__()
        
        self.use_residual = use_residual
        self.stochastic_depth_prob = stochastic_depth_prob
        
        # Main path
        self.conv = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            padding=kernel_size // 2  # same padding
        )
        self.batch_norm = nn.BatchNorm1d(out_channels)
        self.activation = nn.GELU()  # GELU –≤–º–µ—Å—Ç–æ ReLU (–ª—É—á—à–µ –¥–ª—è transformers)
        self.dropout = nn.Dropout(dropout)
        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)
        
        # Skip connection (projection –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã–µ)
        if use_residual:
            if in_channels != out_channels:
                self.skip = nn.Sequential(
                    nn.Conv1d(in_channels, out_channels, kernel_size=1),
                    nn.MaxPool1d(kernel_size=2, stride=2)
                )
            else:
                self.skip = nn.MaxPool1d(kernel_size=2, stride=2)
        else:
            self.skip = None
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.
        
        Args:
            x: (batch, channels, sequence)
        
        Returns:
            (batch, out_channels, sequence // 2)
        """
        # Main path
        out = self.conv(x)
        out = self.batch_norm(out)
        out = self.activation(out)
        out = self.dropout(out)
        out = self.pool(out)
        
        # Residual connection
        if self.use_residual and self.skip is not None:
            identity = self.skip(x)
            
            # Stochastic depth (drop path during training)
            if self.training and self.stochastic_depth_prob > 0:
                if torch.rand(1).item() < self.stochastic_depth_prob:
                    return identity
            
            out = out + identity
        
        return out


class SqueezeExcitation(nn.Module):
    """
    Squeeze-and-Excitation block –¥–ª—è channel attention.
    
    –ü–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤–∑–≤–µ—à–∏–≤–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–æ–≤.
    """
    
    def __init__(self, channels: int, reduction: int = 4):
        super().__init__()
        
        reduced_channels = max(channels // reduction, 8)
        
        self.se = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(channels, reduced_channels),
            nn.ReLU(),
            nn.Linear(reduced_channels, channels),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (batch, channels, sequence)
        
        Returns:
            (batch, channels, sequence) - rescaled
        """
        weights = self.se(x).unsqueeze(-1)  # (batch, channels, 1)
        return x * weights


class MultiHeadTemporalAttention(nn.Module):
    """
    Multi-Head Attention –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.
    
    –û—Ç–ª–∏—á–∏—è –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ Multi-Head Attention:
    - Temporal positional encoding
    - Global average pooling –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ context
    - Causal masking –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
    """
    
    def __init__(
        self,
        hidden_size: int,
        num_heads: int = 4,
        dropout: float = 0.1,
        use_layer_norm: bool = True
    ):
        super().__init__()
        
        assert hidden_size % num_heads == 0, \
            f"hidden_size ({hidden_size}) –¥–æ–ª–∂–µ–Ω –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ num_heads ({num_heads})"
        
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.scale = self.head_dim ** -0.5
        
        # Query, Key, Value projections
        self.q_proj = nn.Linear(hidden_size, hidden_size)
        self.k_proj = nn.Linear(hidden_size, hidden_size)
        self.v_proj = nn.Linear(hidden_size, hidden_size)
        self.out_proj = nn.Linear(hidden_size, hidden_size)
        
        # Dropout
        self.attn_dropout = nn.Dropout(dropout)
        self.proj_dropout = nn.Dropout(dropout)
        
        # Layer Normalization
        if use_layer_norm:
            self.layer_norm = nn.LayerNorm(hidden_size)
        else:
            self.layer_norm = None
        
        # Temporal positional encoding (learnable)
        self.max_seq_len = 256
        self.pos_embedding = nn.Parameter(
            torch.randn(1, self.max_seq_len, hidden_size) * 0.02
        )
    
    def forward(
        self,
        x: torch.Tensor,
        return_weights: bool = False
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Forward pass.
        
        Args:
            x: (batch, seq_len, hidden_size)
            return_weights: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ attention weights
        
        Returns:
            context: (batch, hidden_size) - –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            attention_weights: (batch, num_heads, seq_len) –µ—Å–ª–∏ return_weights=True
        """
        B, T, C = x.shape
        
        # –î–æ–±–∞–≤–ª—è–µ–º positional encoding
        if T <= self.max_seq_len:
            x = x + self.pos_embedding[:, :T, :]
        
        # Pre-LayerNorm (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        if self.layer_norm is not None:
            x_norm = self.layer_norm(x)
        else:
            x_norm = x
        
        # Multi-head projection
        # (B, T, C) -> (B, T, num_heads, head_dim) -> (B, num_heads, T, head_dim)
        q = self.q_proj(x_norm).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x_norm).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x_norm).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)

        # Use memory-efficient attention (Flash Attention) if available (PyTorch 2.0+)
        # This is O(n) memory instead of O(n¬≤) for the attention matrix
        use_flash_attention = hasattr(F, 'scaled_dot_product_attention') and not return_weights

        if use_flash_attention:
            # Flash Attention - memory efficient, but doesn't return weights
            dropout_p = self.attn_dropout.p if self.training else 0.0
            out = F.scaled_dot_product_attention(
                q, k, v,
                dropout_p=dropout_p,
                is_causal=False  # Not causal for full sequence attention
            )
            attn = None  # Weights not available with Flash Attention
        else:
            # Standard attention - needed when return_weights=True
            attn = (q @ k.transpose(-2, -1)) * self.scale
            attn = F.softmax(attn, dim=-1)
            attn = self.attn_dropout(attn)
            out = attn @ v
        
        # Reshape: (B, T, hidden_size)
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        out = self.out_proj(out)
        out = self.proj_dropout(out)
        
        # Residual connection
        out = x + out
        
        # Global context: mean pooling + last timestep (ensemble)
        context = out.mean(dim=1) + out[:, -1, :]
        
        if return_weights and attn is not None:
            # Average attention across heads
            avg_attn = attn.mean(dim=1)  # (B, T, T)
            # Attention –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π timestep
            last_attn = avg_attn[:, -1, :]  # (B, T)
            return context, last_attn

        return context, None


class SimpleAttentionLayer(nn.Module):
    """
    –ü—Ä–æ—Å—Ç–æ–π Attention –º–µ—Ö–∞–Ω–∏–∑–º (backward compatible).
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ—Å–ª–∏ use_multi_head_attention=False.
    """
    
    def __init__(self, hidden_size: int, attention_units: int):
        super().__init__()
        
        self.attention = nn.Sequential(
            nn.Linear(hidden_size, attention_units),
            nn.Tanh(),
            nn.Linear(attention_units, 1)
        )
    
    def forward(
        self,
        lstm_output: torch.Tensor,
        return_weights: bool = False
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Forward pass.
        
        Args:
            lstm_output: (batch, sequence, hidden_size)
        
        Returns:
            context: (batch, hidden_size)
            attention_weights: (batch, sequence) –µ—Å–ª–∏ return_weights=True
        """
        # Attention scores
        attention_scores = self.attention(lstm_output)  # (batch, seq, 1)
        attention_weights = F.softmax(attention_scores, dim=1)  # (batch, seq, 1)
        
        # Weighted sum
        context = torch.sum(attention_weights * lstm_output, dim=1)  # (batch, hidden_size)
        
        if return_weights:
            return context, attention_weights.squeeze(-1)
        
        return context, None


class LSTMWithLayerNorm(nn.Module):
    """
    LSTM —Å Layer Normalization –Ω–∞ –≤—ã—Ö–æ–¥–µ.
    
    Layer Normalization –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å sequences —á–µ–º Batch Normalization.
    """
    
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int = 2,
        dropout: float = 0.3,
        bidirectional: bool = True,
        use_layer_norm: bool = True
    ):
        super().__init__()
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=bidirectional
        )
        
        output_size = hidden_size * (2 if bidirectional else 1)
        
        if use_layer_norm:
            self.layer_norm = nn.LayerNorm(output_size)
        else:
            self.layer_norm = None
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (batch, seq_len, input_size)
        
        Returns:
            (batch, seq_len, hidden_size * 2)
        """
        lstm_out, _ = self.lstm(x)
        
        if self.layer_norm is not None:
            lstm_out = self.layer_norm(lstm_out)
        
        lstm_out = self.dropout(lstm_out)
        
        return lstm_out


# ============================================================================
# MAIN MODEL
# ============================================================================

class HybridCNNLSTMv2(nn.Module):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–∏–±—Ä–∏–¥–Ω–∞—è CNN-LSTM –º–æ–¥–µ–ª—å v2.
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    1. CNN –±–ª–æ–∫–∏ —Å Residual Connections
    2. BiLSTM —Å Layer Normalization
    3. Multi-Head Temporal Attention
    4. Multi-task heads (direction, confidence, return)
    
    –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å:
    - API —Å–æ–≤–º–µ—Å—Ç–∏–º —Å –±–∞–∑–æ–≤–æ–π HybridCNNLSTM
    - –ú–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤–µ—Å–∞ –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (—á–∞—Å—Ç–∏—á–Ω–æ)
    """
    
    def __init__(self, config: ModelConfigV2):
        super().__init__()
        
        self.config = config
        
        # ==================== CNN –ë–õ–û–ö–ò ====================
        cnn_blocks = []
        in_channels = 1  # –ù–∞—á–∏–Ω–∞–µ–º —Å 1 –∫–∞–Ω–∞–ª–∞
        
        for i, (out_channels, kernel_size) in enumerate(
            zip(config.cnn_channels, config.cnn_kernel_sizes)
        ):
            # Stochastic depth —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —Å –≥–ª—É–±–∏–Ω–æ–π
            sd_prob = config.stochastic_depth_prob * (i + 1) / len(config.cnn_channels)
            
            cnn_blocks.append(
                ResidualConvBlock(
                    in_channels=in_channels,
                    out_channels=out_channels,
                    kernel_size=kernel_size,
                    dropout=config.dropout,
                    use_residual=config.use_residual,
                    stochastic_depth_prob=sd_prob
                )
            )
            
            # Squeeze-and-Excitation –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ CNN –±–ª–æ–∫–∞
            if config.use_se_block:
                cnn_blocks.append(SqueezeExcitation(out_channels))
            
            in_channels = out_channels
        
        self.cnn_blocks = nn.ModuleList(cnn_blocks)
        
        # –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ CNN
        cnn_output_size = config.cnn_channels[-1]
        # Sequence —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –≤ 2^n —Ä–∞–∑ –∏–∑-–∑–∞ pooling
        n_pooling = len(config.cnn_channels)
        if config.use_se_block:
            n_pooling = len(config.cnn_channels)  # SE –Ω–µ –º–µ–Ω—è–µ—Ç —Ä–∞–∑–º–µ—Ä
        
        # ==================== LSTM ====================
        self.lstm = LSTMWithLayerNorm(
            input_size=cnn_output_size,
            hidden_size=config.lstm_hidden,
            num_layers=config.lstm_layers,
            dropout=config.lstm_dropout,
            bidirectional=True,
            use_layer_norm=config.use_layer_norm
        )
        
        lstm_output_size = config.lstm_hidden * 2  # Bidirectional
        
        # ==================== ATTENTION ====================
        if config.use_multi_head_attention:
            self.attention = MultiHeadTemporalAttention(
                hidden_size=lstm_output_size,
                num_heads=config.attention_heads,
                dropout=config.attention_dropout,
                use_layer_norm=config.use_layer_norm
            )
        else:
            self.attention = SimpleAttentionLayer(
                hidden_size=lstm_output_size,
                attention_units=config.attention_units
            )
        
        # ==================== OUTPUT HEADS ====================
        
        # Direction classifier (BUY/HOLD/SELL)
        self.direction_head = nn.Sequential(
            nn.Linear(lstm_output_size, 128),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Dropout(config.dropout * 0.5),
            nn.Linear(64, config.num_classes)
        )
        
        # Confidence regressor (0-1)
        self.confidence_head = nn.Sequential(
            nn.Linear(lstm_output_size, 64),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        # Expected return regressor
        self.return_head = nn.Sequential(
            nn.Linear(lstm_output_size, 64),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(64, 1)
        )
        
        # ==================== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ====================
        self._initialize_weights()
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        model_size = self.get_model_size()
        logger.info(
            f"‚úì –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ HybridCNNLSTMv2: "
            f"params={model_size['total_params']:,}, "
            f"trainable={model_size['trainable_params']:,}"
        )
        logger.info(
            f"  ‚Ä¢ CNN: {config.cnn_channels}, residual={config.use_residual}"
        )
        logger.info(
            f"  ‚Ä¢ LSTM: hidden={config.lstm_hidden}, layers={config.lstm_layers}, "
            f"layer_norm={config.use_layer_norm}"
        )
        logger.info(
            f"  ‚Ä¢ Attention: heads={config.attention_heads}, "
            f"multi_head={config.use_multi_head_attention}"
        )
    
    def _initialize_weights(self):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤."""
        for name, module in self.named_modules():
            if isinstance(module, nn.Conv1d):
                # Kaiming –¥–ª—è Conv
                nn.init.kaiming_normal_(
                    module.weight,
                    mode='fan_out',
                    nonlinearity='relu'
                )
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            
            elif isinstance(module, nn.Linear):
                # Xavier –¥–ª—è Linear
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            
            elif isinstance(module, nn.LSTM):
                # Orthogonal –¥–ª—è LSTM (–ª—É—á—à–µ –¥–ª—è gradient flow)
                for param_name, param in module.named_parameters():
                    if 'weight_ih' in param_name:
                        nn.init.xavier_uniform_(param)
                    elif 'weight_hh' in param_name:
                        nn.init.orthogonal_(param)
                    elif 'bias' in param_name:
                        nn.init.zeros_(param)
                        # Forget gate bias = 1 (–ø–æ–º–æ–≥–∞–µ—Ç –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å)
                        n = param.size(0)
                        param.data[n // 4:n // 2].fill_(1.0)
            
            elif isinstance(module, (nn.BatchNorm1d, nn.LayerNorm)):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(
        self,
        x: torch.Tensor,
        return_attention: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass.
        
        Args:
            x: (batch, sequence_length, input_features)
            return_attention: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ attention weights
        
        Returns:
            Dict —Å –≤—ã—Ö–æ–¥–∞–º–∏:
                - direction_logits: (batch, num_classes)
                - confidence: (batch, 1)
                - expected_return: (batch, 1)
                - attention_weights: (batch, sequence) –µ—Å–ª–∏ return_attention=True
        """
        batch_size = x.size(0)
        use_checkpointing = self.config.use_gradient_checkpointing and self.training

        # ==================== CNN PROCESSING ====================
        # (batch, sequence, features) -> (batch, 1, sequence * features)
        x = x.reshape(batch_size, 1, -1)

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ CNN –±–ª–æ–∫–∏ (—Å gradient checkpointing –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        for block in self.cnn_blocks:
            if use_checkpointing:
                x = checkpoint(block, x, use_reentrant=False)
            else:
                x = block(x)

        # ==================== LSTM PROCESSING ====================
        # (batch, channels, reduced_sequence) -> (batch, reduced_sequence, channels)
        x = x.transpose(1, 2)

        # BiLSTM —Å LayerNorm (—Å gradient checkpointing –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        if use_checkpointing:
            lstm_out = checkpoint(self.lstm, x, use_reentrant=False)
        else:
            lstm_out = self.lstm(x)  # (batch, reduced_sequence, lstm_hidden * 2)

        # ==================== ATTENTION ====================
        # Attention –Ω–µ checkpoint'–∏–º —Ç.–∫. —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º Flash Attention
        context, attention_weights = self.attention(
            lstm_out,
            return_weights=return_attention
        )
        
        # ==================== OUTPUT HEADS ====================
        direction_logits = self.direction_head(context)
        confidence = self.confidence_head(context)
        expected_return = self.return_head(context)
        
        outputs = {
            'direction_logits': direction_logits,
            'confidence': confidence,
            'expected_return': expected_return
        }
        
        if return_attention and attention_weights is not None:
            outputs['attention_weights'] = attention_weights
        
        return outputs
    
    def predict(
        self,
        x: torch.Tensor,
        temperature: float = 1.0,
        confidence_threshold: float = 0.0
    ) -> Dict[str, torch.Tensor]:
        """
        Inference —Å temperature scaling –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ confidence.
        
        Args:
            x: (batch, sequence_length, input_features)
            temperature: Temperature –¥–ª—è softmax –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
            confidence_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π confidence –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        
        Returns:
            Dict —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏:
                - direction: (batch,) –∫–ª–∞—Å—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                - direction_probs: (batch, num_classes) –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                - confidence: (batch,) —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
                - expected_return: (batch,) –æ–∂–∏–¥–∞–µ–º–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
                - should_trade: (batch,) bool - —Ç–æ—Ä–≥–æ–≤–∞—Ç—å –ª–∏ (confidence > threshold)
        """
        self.eval()
        
        with torch.no_grad():
            outputs = self.forward(x, return_attention=False)
            
            # Temperature scaling –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
            direction_logits = outputs['direction_logits'] / temperature
            direction_probs = F.softmax(direction_logits, dim=-1)
            direction = torch.argmax(direction_probs, dim=-1)
            
            confidence = outputs['confidence'].squeeze(-1)
            expected_return = outputs['expected_return'].squeeze(-1)
            
            # –§–∏–ª—å—Ç—Ä –ø–æ confidence
            should_trade = confidence >= confidence_threshold
            
            return {
                'direction': direction,
                'direction_probs': direction_probs,
                'confidence': confidence,
                'expected_return': expected_return,
                'should_trade': should_trade
            }
    
    def get_model_size(self) -> Dict[str, int]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(
            p.numel() for p in self.parameters() if p.requires_grad
        )
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'non_trainable_params': total_params - trainable_params
        }
    
    def get_layer_info(self) -> List[Dict[str, any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ª–æ—è—Ö –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏."""
        info = []
        
        for name, module in self.named_modules():
            if isinstance(module, (nn.Conv1d, nn.Linear, nn.LSTM)):
                params = sum(p.numel() for p in module.parameters())
                info.append({
                    'name': name,
                    'type': type(module).__name__,
                    'params': params
                })
        
        return info


# ============================================================================
# FACTORY FUNCTIONS
# ============================================================================

def create_model_v2(config: Optional[ModelConfigV2] = None) -> HybridCNNLSTMv2:
    """
    –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ v2.
    
    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (None = default)
    
    Returns:
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    """
    if config is None:
        config = ModelConfigV2()
    
    model = HybridCNNLSTMv2(config)
    
    return model


def create_model_v2_from_preset(preset: str = "production_small") -> HybridCNNLSTMv2:
    """
    –°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å –∏–∑ –ø—Ä–µ—Å–µ—Ç–∞.
    
    –ü—Ä–µ—Å–µ—Ç—ã:
        - production_small: –î–ª—è –º–∞–ª—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (7-30 –¥–Ω–µ–π)
        - production_large: –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (60+ –¥–Ω–µ–π)
        - quick_experiment: –î–ª—è –±—ã—Å—Ç—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
        - conservative: –î–ª—è –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏
    """
    if preset == "production_small":
        config = ModelConfigV2(
            cnn_channels=(32, 64, 128),
            lstm_hidden=128,
            dropout=0.4,
            use_residual=True,
            use_layer_norm=True,
            use_multi_head_attention=True
        )
    
    elif preset == "production_large":
        config = ModelConfigV2(
            cnn_channels=(64, 128, 256),
            lstm_hidden=256,
            dropout=0.3,
            use_residual=True,
            use_layer_norm=True,
            use_multi_head_attention=True
        )
    
    elif preset == "quick_experiment":
        config = ModelConfigV2(
            cnn_channels=(32, 64),
            lstm_hidden=64,
            lstm_layers=1,
            dropout=0.3,
            use_residual=False,
            use_layer_norm=False,
            use_multi_head_attention=False
        )
    
    elif preset == "conservative":
        config = ModelConfigV2(
            cnn_channels=(32, 64, 128),
            lstm_hidden=128,
            dropout=0.5,
            use_residual=True,
            use_layer_norm=True,
            use_multi_head_attention=True,
            use_se_block=True
        )
    
    else:
        raise ValueError(f"Unknown preset: {preset}")
    
    return create_model_v2(config)


# ============================================================================
# BACKWARD COMPATIBILITY
# ============================================================================

def load_from_v1_checkpoint(
    model_v2: HybridCNNLSTMv2,
    checkpoint_path: str,
    strict: bool = False
) -> Tuple[List[str], List[str]]:
    """
    –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ –∏–∑ checkpoint v1 –≤ –º–æ–¥–µ–ª—å v2.
    
    Args:
        model_v2: –ú–æ–¥–µ–ª—å v2
        checkpoint_path: –ü—É—Ç—å –∫ checkpoint v1
        strict: –¢—Ä–µ–±–æ–≤–∞—Ç—å —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–µ—Å–æ–≤
    
    Returns:
        (missing_keys, unexpected_keys)
    """
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
    else:
        state_dict = checkpoint
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –∏–º—ë–Ω
    missing, unexpected = model_v2.load_state_dict(state_dict, strict=strict)
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –≤–µ—Å–∞ –∏–∑ v1 checkpoint: {checkpoint_path}")
    if missing:
        logger.warning(f"Missing keys: {missing}")
    if unexpected:
        logger.warning(f"Unexpected keys: {unexpected}")
    
    return missing, unexpected


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å —Å default –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    print("=" * 80)
    print("–¢–ï–°–¢ HybridCNNLSTMv2")
    print("=" * 80)
    
    config = ModelConfigV2(
        cnn_channels=(32, 64, 128),
        lstm_hidden=128,
        use_residual=True,
        use_layer_norm=True,
        use_multi_head_attention=True
    )
    
    model = create_model_v2(config)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    batch_size = 32
    sequence_length = 60
    input_features = 110
    
    x = torch.randn(batch_size, sequence_length, input_features)
    
    print(f"\nüìä Input shape: {x.shape}")
    
    # Forward pass
    print("\nüîÑ Forward pass...")
    outputs = model(x, return_attention=True)
    
    print("\nüì§ Outputs:")
    for key, value in outputs.items():
        print(f"  ‚Ä¢ {key}: {value.shape}")
    
    # Inference
    print("\nüéØ Inference...")
    predictions = model.predict(x, temperature=1.0, confidence_threshold=0.5)
    
    print("\nüìä Predictions:")
    for key, value in predictions.items():
        if isinstance(value, torch.Tensor):
            print(f"  ‚Ä¢ {key}: {value.shape}")
    
    # Model info
    print("\nüìà Model info:")
    size_info = model.get_model_size()
    for key, value in size_info.items():
        print(f"  ‚Ä¢ {key}: {value:,}")
    
    # Layer info
    print("\nüìã Layer info (top params):")
    layers = model.get_layer_info()
    layers_sorted = sorted(layers, key=lambda x: x['params'], reverse=True)[:5]
    for layer in layers_sorted:
        print(f"  ‚Ä¢ {layer['name']}: {layer['type']}, params={layer['params']:,}")
    
    print("\n" + "=" * 80)
    print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã!")
    print("=" * 80)
