#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π Model Trainer v2 - Industry Standard.

–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏
2. Data Augmentation (MixUp, Time Masking, etc.)
3. CosineAnnealingWarmRestarts scheduler
4. Label Smoothing
5. Gradient Accumulation
6. Mixed Precision Training (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
7. –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–µ—Ç—Ä–∏–∫–∏
8. Checkpoint management
9. Reproducibility (seed fixing)

–ü—É—Ç—å: backend/ml_engine/training/model_trainer_v2.py
"""

import os
import random
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional, Tuple, List, Any, Union
from collections import deque
from dataclasses import dataclass, asdict, is_dataclass

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
    classification_report
)

from backend.core.logger import get_logger

# Project root models directory
_PROJECT_ROOT = Path(__file__).parent.parent.parent.parent  # backend/ml_engine/training -> project_root
_DEFAULT_MODELS_DIR = str(_PROJECT_ROOT / "models")

# –õ–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã (–±—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã)
# from backend.ml_engine.configs.optimized_configs import (
#     OptimizedTrainerConfig,
#     OptimizedBalancingConfig
# )
# from backend.ml_engine.training.losses import (
#     LossFactory,
#     MultiTaskLossV2,
#     compute_class_weights
# )
# from backend.ml_engine.training.augmentation import (
#     AugmentationPipeline,
#     AugmentationConfig,
#     MixUp
# )

logger = get_logger(__name__)


# ============================================================================
# CONFIGURATION
# ============================================================================

@dataclass
class TrainerConfigV2:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è v2.
    
    –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–ê–†–ê–ú–ï–¢–†–´:
    - learning_rate: 5e-5 (–ù–ï 0.001!)
    - batch_size: 256
    - weight_decay: 0.01
    """
    
    # === Training ===
    epochs: int = 150
    learning_rate: float = 5e-5  # –ö–†–ò–¢–ò–ß–ù–û: –í 20 —Ä–∞–∑ –º–µ–Ω—å—à–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ!
    min_learning_rate: float = 1e-7
    batch_size: int = 64  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è oversampled dataset (–±—ã–ª–æ 128)
    weight_decay: float = 0.01  # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è

    # === Gradient ===
    grad_clip_value: float = 1.0
    gradient_accumulation_steps: int = 4  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch = 64*4 = 256
    
    # === Early Stopping ===
    early_stopping_patience: int = 20
    early_stopping_delta: float = 1e-4
    early_stopping_metric: str = "val_loss"  # val_loss, val_f1, val_accuracy
    
    # === Scheduler ===
    scheduler_type: str = "cosine_warm_restarts"  # cosine_warm_restarts, reduce_on_plateau
    scheduler_T_0: int = 10  # –ü–µ—Ä–∏–æ–¥ –ø–µ—Ä–≤–æ–≥–æ —Ü–∏–∫–ª–∞
    scheduler_T_mult: int = 2  # –£–º–Ω–æ–∂–∏—Ç–µ–ª—å
    scheduler_eta_min: float = 1e-7
    scheduler_patience: int = 5  # –î–ª—è ReduceOnPlateau
    scheduler_factor: float = 0.5
    
    # === Label Smoothing ===
    label_smoothing: float = 0.1  # Restored: helps with overconfidence

    # === Data Augmentation ===
    use_augmentation: bool = True  # Restored: helps with generalization
    mixup_alpha: float = 0.2  # Restored
    mixup_prob: float = 0.3  # Reduced from 0.5 for stability
    gaussian_noise_std: float = 0.01
    time_mask_ratio: float = 0.1

    # === Multi-task Loss Weights ===
    direction_loss_weight: float = 1.0
    confidence_loss_weight: float = 0.5
    return_loss_weight: float = 0.3

    # === Class Balancing ===
    # Strategy: Oversampling (data balance) + Focal Loss (hard examples)
    # NO class_weights - they cause double compensation with oversampling!
    use_class_weights: bool = False  # DISABLED: oversampling already balances data
    use_focal_loss: bool = True      # Focuses on hard examples
    focal_gamma: float = 1.5         # Moderate focus with balanced data
    
    # === Mixed Precision ===
    use_mixed_precision: bool = False  # –í–†–ï–ú–ï–ù–ù–û –û–¢–ö–õ–Æ–ß–ï–ù–û –∏–∑-–∑–∞ NaN loss (RTX 3060 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç!)

    # === Checkpoint ===
    checkpoint_dir: str = _DEFAULT_MODELS_DIR  # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ project_root/models
    save_best_only: bool = True
    save_every_n_epochs: int = 10

    # === Device ===
    device: str = "auto"  # auto, cuda, cpu
    
    # === Logging ===
    log_interval: int = 10
    verbose: bool = True
    use_tqdm: bool = True
    
    # === Reproducibility ===
    seed: int = 42
    deterministic: bool = False  # False –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏, True –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    def get_device(self) -> torch.device:
        """–ü–æ–ª—É—á–∏—Ç—å device."""
        if self.device == "auto":
            return torch.device("cuda" if torch.cuda.is_available() else "cpu")
        return torch.device(self.device)


# ============================================================================
# EARLY STOPPING
# ============================================================================

class EarlyStopping:
    """Early Stopping —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫."""
    
    def __init__(
        self,
        patience: int = 10,
        delta: float = 1e-4,
        mode: str = "min"
    ):
        """
        Args:
            patience: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
            delta: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
            mode: 'min' –¥–ª—è loss, 'max' –¥–ª—è accuracy/f1
        """
        self.patience = patience
        self.delta = delta
        self.mode = mode
        
        self.counter = 0
        self.best_score = None
        self.early_stop = False
    
    def __call__(self, score: float) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω—É–∂–Ω–æ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ.
        
        Args:
            score: –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        
        Returns:
            True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
        """
        if self.best_score is None:
            self.best_score = score
            return False
        
        if self.mode == "min":
            improved = score < self.best_score - self.delta
        else:
            improved = score > self.best_score + self.delta
        
        if improved:
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        
        return self.early_stop
    
    def reset(self):
        """–°–±—Ä–æ—Å–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ."""
        self.counter = 0
        self.best_score = None
        self.early_stop = False


# ============================================================================
# EPOCH METRICS
# ============================================================================

@dataclass
class EpochMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ —ç–ø–æ—Ö–∏."""
    epoch: int
    train_loss: float
    val_loss: float
    train_accuracy: float
    val_accuracy: float
    val_precision: float
    val_recall: float
    val_f1: float
    learning_rate: float
    epoch_time: float
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


# ============================================================================
# MODEL TRAINER V2
# ============================================================================

class ModelTrainerV2:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π Model Trainer v2.
    
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    - Data Augmentation (MixUp, noise, masking)
    - CosineAnnealingWarmRestarts scheduler
    - Label Smoothing + Focal Loss
    - Gradient clipping –∏ accumulation
    - Mixed Precision Training
    """
    
    def __init__(
        self,
        model: nn.Module,
        config: TrainerConfigV2
    ):
        """
        Args:
            model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        self.model = model
        self.config = config
        self.device = config.get_device()
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ device
        self.model.to(self.device)

        # –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
        if self.device.type == "cuda":
            torch.cuda.empty_cache()

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        if config.deterministic:
            self._set_seed(config.seed)
        
        # Loss function (–±—É–¥–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ train())
        self.criterion = None
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # Scheduler
        self.scheduler = self._create_scheduler()
        
        # Early Stopping
        mode = "min" if config.early_stopping_metric == "val_loss" else "max"
        self.early_stopping = EarlyStopping(
            patience=config.early_stopping_patience,
            delta=config.early_stopping_delta,
            mode=mode
        )
        
        # Mixed Precision with safer settings
        if config.use_mixed_precision:
            self.scaler = GradScaler(
                init_scale=2.**10,  # –ú–µ–Ω—å—à–∏–π –Ω–∞—á–∞–ª—å–Ω—ã–π scale (–±—ã–ª–æ 2^16)
                growth_interval=1000  # –†–µ–∂–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º scale
            )
        else:
            self.scaler = None
        
        # Checkpoint directory
        self.checkpoint_dir = Path(config.checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # Training history
        self.history: List[EpochMetrics] = []
        self.best_val_loss = float('inf')
        self.best_val_f1 = 0.0
        
        # MixUp
        if config.use_augmentation and config.mixup_alpha > 0:
            from backend.ml_engine.training.augmentation import MixUp
            self.mixup = MixUp(alpha=config.mixup_alpha)
        else:
            self.mixup = None
        
        logger.info(
            f"‚úì ModelTrainerV2 –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:\n"
            f"  ‚Ä¢ Device: {self.device}\n"
            f"  ‚Ä¢ Learning rate: {config.learning_rate}\n"
            f"  ‚Ä¢ Batch size: {config.batch_size}\n"
            f"  ‚Ä¢ Weight decay: {config.weight_decay}\n"
            f"  ‚Ä¢ Scheduler: {config.scheduler_type}\n"
            f"  ‚Ä¢ Label smoothing: {config.label_smoothing}\n"
            f"  ‚Ä¢ MixUp: {config.mixup_alpha if self.mixup else 'disabled'}\n"
            f"  ‚Ä¢ Mixed precision: {config.use_mixed_precision}"
        )
    
    def _set_seed(self, seed: int):
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏."""
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
    
    def _create_scheduler(self) -> optim.lr_scheduler._LRScheduler:
        """–°–æ–∑–¥–∞—Ç—å learning rate scheduler."""
        if self.config.scheduler_type == "cosine_warm_restarts":
            return optim.lr_scheduler.CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=self.config.scheduler_T_0,
                T_mult=self.config.scheduler_T_mult,
                eta_min=self.config.scheduler_eta_min
            )
        elif self.config.scheduler_type == "reduce_on_plateau":
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                patience=self.config.scheduler_patience,
                factor=self.config.scheduler_factor,
                min_lr=self.config.min_learning_rate
            )
        elif self.config.scheduler_type == "one_cycle":
            # –ù—É–∂–Ω–æ –∑–Ω–∞—Ç—å total_steps, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –≤ train()
            return None
        else:
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.epochs,
                eta_min=self.config.min_learning_rate
            )
    
    def _setup_loss_function(self, train_labels: np.ndarray):
        """
        –ù–∞—Å—Ç—Ä–æ–∏—Ç—å loss function –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            train_labels: –ú–µ—Ç–∫–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –≤–µ—Å–æ–≤
        """
        from backend.ml_engine.training.losses import (
            LossFactory,
            compute_class_weights
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        class_weights = None
        if self.config.use_class_weights:
            class_weights = compute_class_weights(
                train_labels,
                method="balanced"
            )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø loss
        if self.config.use_focal_loss:
            if self.config.label_smoothing > 0:
                loss_type = "focal_smooth"
            else:
                loss_type = "focal"
        else:
            if self.config.label_smoothing > 0:
                loss_type = "ce_smooth"
            else:
                loss_type = "ce"
        
        # –°–æ–∑–¥–∞—ë–º multi-task loss
        self.criterion = LossFactory.create_multi_task_loss(
            direction_loss_type=loss_type,
            num_classes=3,  # BUY, HOLD, SELL
            class_weights=class_weights,
            gamma=self.config.focal_gamma,
            label_smoothing=self.config.label_smoothing,
            direction_weight=self.config.direction_loss_weight,
            confidence_weight=self.config.confidence_loss_weight,
            return_weight=self.config.return_loss_weight,
            device=str(self.device)
        )
        
        logger.info(
            f"‚úì Loss function –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {loss_type}, "
            f"focal_gamma={self.config.focal_gamma}, "
            f"label_smoothing={self.config.label_smoothing}"
        )
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: Optional[DataLoader] = None
    ) -> List[EpochMetrics]:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.
        
        Args:
            train_loader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            test_loader: DataLoader –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è (list of EpochMetrics)
        """
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º loss –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö
        if self.criterion is None:
            logger.info("–ê–Ω–∞–ª–∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ loss...")
            all_labels = []
            for batch in train_loader:
                labels = batch['label'].numpy()
                all_labels.extend(labels)
            self._setup_loss_function(np.array(all_labels))
        
        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ tqdm.write() –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
        tqdm.write("\n" + "=" * 80)
        tqdm.write("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
        tqdm.write("=" * 80)
        tqdm.write(f"–≠–ø–æ—Ö: {self.config.epochs}")
        tqdm.write(f"Train batches: {len(train_loader)}")
        tqdm.write(f"Val batches: {len(val_loader)}")
        tqdm.write(f"Device: {self.device}")
        tqdm.write("=" * 80 + "\n")
        
        # Progress bar –¥–ª—è —ç–ø–æ—Ö
        # position=0 –∏ dynamic_ncols=False –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≤ –≤–µ–±-–∫–æ–Ω—Å–æ–ª–∏
        epoch_pbar = tqdm(
            range(self.config.epochs),
            desc="Training",
            unit="epoch",
            disable=not self.config.use_tqdm,
            position=0,
            leave=True,
            dynamic_ncols=False,
            ncols=100,
            mininterval=1.0  # –û–±–Ω–æ–≤–ª—è—Ç—å –Ω–µ —á–∞—â–µ —Ä–∞–∑–∞ –≤ —Å–µ–∫—É–Ω–¥—É
        )
        
        for epoch in epoch_pbar:
            epoch_start = time.time()
            
            # Training
            train_loss, train_acc = self._train_epoch(
                train_loader,
                epoch_num=epoch + 1
            )
            
            # Validation
            val_loss, val_metrics = self._validate_epoch(
                val_loader,
                epoch_num=epoch + 1
            )
            
            # Scheduler step
            current_lr = self.optimizer.param_groups[0]['lr']
            if self.config.scheduler_type == "reduce_on_plateau":
                # ReduceLROnPlateau –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –º–µ—Ç—Ä–∏–∫—É (float)
                self.scheduler.step(val_loss)  # type: ignore[arg-type]
            else:
                self.scheduler.step()
            
            # –ú–µ—Ç—Ä–∏–∫–∏ —ç–ø–æ—Ö–∏
            epoch_time = time.time() - epoch_start
            metrics = EpochMetrics(
                epoch=epoch + 1,
                train_loss=train_loss,
                val_loss=val_loss,
                train_accuracy=train_acc,
                val_accuracy=val_metrics['accuracy'],
                val_precision=val_metrics['precision'],
                val_recall=val_metrics['recall'],
                val_f1=val_metrics['f1'],
                learning_rate=current_lr,
                epoch_time=epoch_time
            )
            self.history.append(metrics)
            
            # Update progress bar
            epoch_pbar.set_postfix({
                'train_loss': f'{train_loss:.4f}',
                'val_loss': f'{val_loss:.4f}',
                'val_acc': f'{val_metrics["accuracy"]:.4f}',
                'val_f1': f'{val_metrics["f1"]:.4f}'
            })
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ tqdm.write() –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
            if self.config.verbose:
                tqdm.write(
                    f"[Epoch {epoch + 1}/{self.config.epochs}] "
                    f"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, "
                    f"val_acc={val_metrics['accuracy']:.4f}, "
                    f"val_f1={val_metrics['f1']:.4f}, "
                    f"lr={current_lr:.2e}, time={epoch_time:.1f}s"
                )
            
            # Save best model
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self._save_checkpoint(epoch + 1, val_loss, val_metrics, "best_loss")
            
            if val_metrics['f1'] > self.best_val_f1:
                self.best_val_f1 = val_metrics['f1']
                self._save_checkpoint(epoch + 1, val_loss, val_metrics, "best_f1")
            
            # Periodic checkpoint
            if (epoch + 1) % self.config.save_every_n_epochs == 0:
                self._save_checkpoint(epoch + 1, val_loss, val_metrics, f"epoch_{epoch + 1}")
            
            # ================================================================
            # MODE COLLAPSE EARLY STOPPING
            # ================================================================
            # Stop training immediately if mode collapse is detected
            mode_collapse_severity = val_metrics.get('mode_collapse_severity', 'none')
            if mode_collapse_severity in ['total', 'severe']:
                tqdm.write(
                    f"\nüõë [MODE COLLAPSE STOP] Training stopped at epoch {epoch + 1}. "
                    f"Severity: {mode_collapse_severity}. "
                    f"Majority class: {val_metrics.get('majority_class_pct', 0):.1%}"
                )
                tqdm.write(
                    f"   Macro F1: {val_metrics['f1']:.4f} (this is the true metric)"
                )
                # Mark this in history for later analysis
                self.history[-1] = EpochMetrics(
                    epoch=epoch + 1,
                    train_loss=train_loss,
                    val_loss=val_loss,
                    train_accuracy=train_acc,
                    val_accuracy=val_metrics['accuracy'],
                    val_precision=val_metrics['precision'],
                    val_recall=val_metrics['recall'],
                    val_f1=val_metrics['f1'],  # This is now macro F1
                    learning_rate=current_lr,
                    epoch_time=epoch_time
                )
                break

            # Regular Early Stopping
            stop_metric = val_loss if self.config.early_stopping_metric == "val_loss" else val_metrics['f1']
            if self.early_stopping(stop_metric):
                tqdm.write(
                    f"\n[Early Stop] Triggered at epoch {epoch + 1}. "
                    f"Best val_loss: {self.best_val_loss:.4f}"
                )
                break
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ tqdm.write() –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
        tqdm.write("\n" + "=" * 80)
        tqdm.write("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        tqdm.write("=" * 80)
        tqdm.write(f"–í—Å–µ–≥–æ —ç–ø–æ—Ö: {len(self.history)}")
        tqdm.write(f"Best val_loss: {self.best_val_loss:.4f}")
        tqdm.write(f"Best val_f1: {self.best_val_f1:.4f}")
        tqdm.write("=" * 80 + "\n")
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å test_loader)
        test_results = None
        if test_loader is not None:
            test_results = self._test_model(test_loader)
            # Store test results for hyperopt logger
            self.test_results = test_results

        return self.history
    
    def _train_epoch(
        self,
        train_loader: DataLoader,
        epoch_num: int
    ) -> Tuple[float, float]:
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏."""
        self.model.train()
        
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω progress bar –±–µ–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –¥–ª—è –≤–µ–±-–∫–æ–Ω—Å–æ–ª–∏
        # disable=True –¥–ª—è batch-level, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ logging
        total_batches = len(train_loader)
        log_interval = max(1, total_batches // 10)  # –õ–æ–≥–∏—Ä—É–µ–º 10 —Ä–∞–∑ –∑–∞ —ç–ø–æ—Ö—É

        for batch_idx, batch in enumerate(train_loader):
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ (batch - —ç—Ç–æ Dict[str, Tensor])
            sequences: torch.Tensor = batch['sequence'].to(self.device)
            labels: torch.Tensor = batch['label'].to(self.device)

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è MixUp
            labels_a: torch.Tensor = labels
            labels_b: torch.Tensor = labels
            lam: float = 1.0

            # Data Augmentation: Gaussian noise
            if self.config.use_augmentation and self.training:
                sequences = sequences + torch.randn_like(sequences) * self.config.gaussian_noise_std

            # MixUp
            use_mixup = (
                self.mixup is not None and
                self.training and
                np.random.random() < self.config.mixup_prob
            )

            if use_mixup:
                sequences, labels_a, labels_b, lam = self.mixup(sequences, labels)
            
            # Forward pass
            if self.config.use_mixed_precision and self.scaler is not None:
                with autocast():
                    outputs = self.model(sequences)
                    if use_mixup:
                        targets_a = {'label': labels_a}
                        targets_b = {'label': labels_b}
                        loss_a, _ = self.criterion(outputs, targets_a)
                        loss_b, _ = self.criterion(outputs, targets_b)
                        loss = lam * loss_a + (1 - lam) * loss_b
                    else:
                        targets = {'label': labels}
                        loss, _ = self.criterion(outputs, targets)
            else:
                outputs = self.model(sequences)
                if use_mixup:
                    targets_a = {'label': labels_a}
                    targets_b = {'label': labels_b}
                    loss_a, _ = self.criterion(outputs, targets_a)
                    loss_b, _ = self.criterion(outputs, targets_b)
                    loss = lam * loss_a + (1 - lam) * loss_b
                else:
                    targets = {'label': labels}
                    loss, _ = self.criterion(outputs, targets)

            # Save original loss for display
            original_loss = loss.item()

            # Check for NaN loss
            if torch.isnan(loss) or torch.isinf(loss):
                logger.warning(f"NaN/Inf loss detected at batch {batch_idx}! Skipping batch.")
                continue

            # Gradient accumulation
            loss = loss / self.config.gradient_accumulation_steps

            # Backward pass
            if self.scaler is not None:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()

            # Optimizer step
            if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                if self.scaler is not None:
                    self.scaler.unscale_(self.optimizer)

                    # Check for NaN gradients
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.config.grad_clip_value
                    )

                    if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                        logger.warning(f"NaN/Inf gradient detected! Skipping optimizer step.")
                        self.optimizer.zero_grad()
                        continue

                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    if self.config.grad_clip_value > 0:
                        grad_norm = torch.nn.utils.clip_grad_norm_(
                            self.model.parameters(),
                            self.config.grad_clip_value
                        )

                        if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                            logger.warning(f"NaN/Inf gradient detected! Skipping optimizer step.")
                            self.optimizer.zero_grad()
                            continue

                    self.optimizer.step()

                self.optimizer.zero_grad()

                # –û—á–∏—Å—Ç–∫–∞ GPU –∫–µ—à–∞ –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
                if self.device.type == "cuda" and (batch_idx + 1) % 50 == 0:
                    torch.cuda.empty_cache()

            # Metrics
            total_loss += original_loss

            predictions = torch.argmax(
                outputs['direction_logits'], dim=-1
            ).cpu().numpy()

            if use_mixup:
                # –î–ª—è MixUp –∏—Å–ø–æ–ª—å–∑—É–µ–º original labels –¥–ª—è metrics
                all_labels.extend(labels_a.cpu().numpy())
            else:
                all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predictions)

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —á–µ—Ä–µ–∑ tqdm.write() –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≤ –∫–æ–Ω—Å–æ–ª—å
            if (batch_idx + 1) % log_interval == 0 or batch_idx == total_batches - 1:
                progress_pct = (batch_idx + 1) / total_batches * 100
                tqdm.write(
                    f"  [Train] Epoch {epoch_num}: {batch_idx + 1}/{total_batches} "
                    f"({progress_pct:.0f}%) - loss: {original_loss:.4f}"
                )
        
        avg_loss = total_loss / len(train_loader)
        accuracy = accuracy_score(all_labels, all_predictions)
        
        return avg_loss, accuracy
    
    def _validate_epoch(
        self,
        val_loader: DataLoader,
        epoch_num: int
    ) -> Tuple[float, Dict[str, float]]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏."""
        self.model.eval()
        
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±–∏—Ä–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–π tqdm –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç—Ä–æ–∫
        total_batches = len(val_loader)
        log_interval = max(1, total_batches // 5)  # –õ–æ–≥–∏—Ä—É–µ–º 5 —Ä–∞–∑ –∑–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é
        batch_idx = 0

        with torch.no_grad():
            for batch in val_loader:
                sequences = batch['sequence'].to(self.device)
                labels = batch['label'].to(self.device)
                
                # Forward
                outputs = self.model(sequences)
                
                # Loss
                targets = {'label': labels}
                loss, _ = self.criterion(outputs, targets)
                
                # Metrics
                total_loss += loss.item()
                predictions = torch.argmax(
                    outputs['direction_logits'], dim=-1
                ).cpu().numpy()
                all_predictions.extend(predictions)
                all_labels.extend(labels.cpu().numpy())

                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —á–µ—Ä–µ–∑ tqdm.write() –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
                batch_idx += 1
                if batch_idx % log_interval == 0 or batch_idx == total_batches:
                    progress_pct = batch_idx / total_batches * 100
                    tqdm.write(
                        f"  [Val] Epoch {epoch_num}: {batch_idx}/{total_batches} "
                        f"({progress_pct:.0f}%) - loss: {loss.item():.4f}"
                    )
        
        avg_loss = total_loss / len(val_loader)

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(all_labels, all_predictions)

        # CRITICAL FIX: Use MACRO average instead of WEIGHTED for F1
        # Weighted F1 rewards mode collapse to majority class!
        # Macro F1 gives equal weight to all classes, penalizing mode collapse
        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
            all_labels, all_predictions,
            average='weighted',
            zero_division=0
        )

        # MACRO F1 - this is what we optimize for
        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
            all_labels, all_predictions,
            average='macro',
            zero_division=0
        )

        # ================================================================
        # MODE COLLAPSE EARLY WARNING
        # ================================================================
        # Check prediction distribution to detect mode collapse
        from collections import Counter
        pred_dist = Counter(all_predictions)
        total_preds = len(all_predictions)

        # Calculate what percentage of predictions are the majority class
        most_common_class, most_common_count = pred_dist.most_common(1)[0]
        majority_pct = most_common_count / total_preds

        # Determine mode collapse severity
        mode_collapse_severity = "none"
        if majority_pct >= 0.99:
            mode_collapse_severity = "total"
        elif majority_pct >= 0.90:
            mode_collapse_severity = "severe"
        elif majority_pct >= 0.75:
            mode_collapse_severity = "mild"

        # Log warnings based on severity
        if mode_collapse_severity == "total":
            tqdm.write(
                f"  üî¥ TOTAL MODE COLLAPSE: {majority_pct:.1%} predictions are class {most_common_class}! "
                f"Distribution: {dict(pred_dist)}"
            )
            tqdm.write(
                f"     Weighted F1={f1_weighted:.4f} (misleading!), Macro F1={f1_macro:.4f} (true quality)"
            )
        elif mode_collapse_severity == "severe":
            tqdm.write(
                f"  üü† SEVERE MODE COLLAPSE: {majority_pct:.1%} predictions are class {most_common_class}! "
                f"Distribution: {dict(pred_dist)}"
            )
        elif mode_collapse_severity == "mild":
            tqdm.write(
                f"  üü° Prediction imbalance: {majority_pct:.1%} are class {most_common_class}. "
                f"Distribution: {dict(pred_dist)}"
            )

        # Return MACRO F1 as the primary metric (penalizes mode collapse)
        metrics = {
            'accuracy': accuracy,
            'precision': precision_macro,  # Changed to macro
            'recall': recall_macro,        # Changed to macro
            'f1': f1_macro,                # Changed to macro - CRITICAL!
            # Also store weighted for reference
            'f1_weighted': f1_weighted,
            'precision_weighted': precision_weighted,
            'recall_weighted': recall_weighted,
            # Mode collapse info
            'mode_collapse_severity': mode_collapse_severity,
            'majority_class_pct': majority_pct
        }

        return avg_loss, metrics
    
    def _test_model(self, test_loader: DataLoader) -> Dict[str, Any]:
        """
        –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π.

        Returns:
            Dict with all test metrics for logging
        """
        from collections import Counter

        tqdm.write("\n" + "=" * 80)
        tqdm.write("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
        tqdm.write("=" * 80)

        self.model.eval()

        all_predictions = []
        all_labels = []
        all_confidences = []

        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Testing", disable=not self.config.use_tqdm):
                sequences = batch['sequence'].to(self.device)
                labels = batch['label'].to(self.device)

                outputs = self.model(sequences)

                predictions = torch.argmax(
                    outputs['direction_logits'], dim=-1
                ).cpu().numpy()
                confidences = outputs['confidence'].squeeze(-1).cpu().numpy()

                all_predictions.extend(predictions)
                all_labels.extend(labels.cpu().numpy())
                all_confidences.extend(confidences)

        # Mode collapse detection
        pred_dist = Counter(all_predictions)
        label_dist = Counter(all_labels)
        total_preds = len(all_predictions)

        most_common_class, most_common_count = pred_dist.most_common(1)[0]
        majority_pct = most_common_count / total_preds

        # Determine mode collapse severity
        if majority_pct >= 0.99:
            mode_collapse_severity = "total"
        elif majority_pct >= 0.90:
            mode_collapse_severity = "severe"
        elif majority_pct >= 0.75:
            mode_collapse_severity = "mild"
        else:
            mode_collapse_severity = "none"

        # MACRO metrics (true quality measure)
        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
            all_labels, all_predictions,
            average='macro',
            zero_division=0
        )

        # WEIGHTED metrics (for reference)
        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
            all_labels, all_predictions,
            average='weighted',
            zero_division=0
        )

        accuracy = accuracy_score(all_labels, all_predictions)

        tqdm.write(f"Test Results (MACRO - true quality):")
        tqdm.write(f"  ‚Ä¢ Accuracy: {accuracy:.4f}")
        tqdm.write(f"  ‚Ä¢ Precision (macro): {precision_macro:.4f}")
        tqdm.write(f"  ‚Ä¢ Recall (macro): {recall_macro:.4f}")
        tqdm.write(f"  ‚Ä¢ F1 Score (macro): {f1_macro:.4f}")

        if mode_collapse_severity != "none":
            tqdm.write(f"\n‚ö†Ô∏è MODE COLLAPSE DETECTED ({mode_collapse_severity}):")
            tqdm.write(f"  ‚Ä¢ {majority_pct:.1%} predictions are class {most_common_class}")
            tqdm.write(f"  ‚Ä¢ F1 (weighted): {f1_weighted:.4f} <- MISLEADING!")
            tqdm.write(f"  ‚Ä¢ F1 (macro): {f1_macro:.4f} <- TRUE QUALITY")
        else:
            tqdm.write(f"\nWeighted metrics (for reference):")
            tqdm.write(f"  ‚Ä¢ F1 (weighted): {f1_weighted:.4f}")

        # Class distribution
        class_names = ['SELL', 'HOLD', 'BUY']
        tqdm.write(f"\nClass Distribution:")
        tqdm.write(f"  Labels:      {dict((class_names[k], v) for k, v in sorted(label_dist.items()))}")
        tqdm.write(f"  Predictions: {dict((class_names[k], v) for k, v in sorted(pred_dist.items()))}")

        # Classification report
        report = classification_report(
            all_labels, all_predictions,
            target_names=class_names,
            digits=4
        )
        tqdm.write(f"\nClassification Report:\n{report}")

        # Confusion matrix
        cm = confusion_matrix(all_labels, all_predictions)
        tqdm.write(f"\nConfusion Matrix:")
        tqdm.write(f"           SELL  HOLD   BUY")
        for i, row in enumerate(cm):
            tqdm.write(f"  {class_names[i]:>6}  {row[0]:>5}  {row[1]:>5}  {row[2]:>5}")

        # Confidence analysis
        all_confidences = np.array(all_confidences)
        correct_mask = np.array(all_predictions) == np.array(all_labels)

        mean_conf_correct = all_confidences[correct_mask].mean() if correct_mask.any() else 0.0
        mean_conf_incorrect = all_confidences[~correct_mask].mean() if (~correct_mask).any() else 0.0

        tqdm.write(f"\nConfidence Analysis:")
        tqdm.write(f"  ‚Ä¢ Mean confidence (correct): {mean_conf_correct:.4f}")
        tqdm.write(f"  ‚Ä¢ Mean confidence (incorrect): {mean_conf_incorrect:.4f}")

        tqdm.write("=" * 80 + "\n")

        # Return results for logging
        return {
            'accuracy': accuracy,
            'precision': precision_macro,
            'recall': recall_macro,
            'f1': f1_macro,
            'precision_weighted': precision_weighted,
            'recall_weighted': recall_weighted,
            'f1_weighted': f1_weighted,
            'mode_collapse_severity': mode_collapse_severity,
            'majority_class_pct': majority_pct,
            'confusion_matrix': cm.tolist(),
            'mean_confidence_correct': mean_conf_correct,
            'mean_confidence_incorrect': mean_conf_incorrect,
            'all_predictions': all_predictions,
            'all_labels': all_labels,
            'all_confidences': all_confidences.tolist()
        }
    
    def _save_checkpoint(
        self,
        epoch: int,
        val_loss: float,
        metrics: Dict[str, float],
        name: str
    ):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint."""
        checkpoint_path = self.checkpoint_dir / f"{name}.pt"
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'val_loss': val_loss,
            'metrics': metrics,
            'config': asdict(self.config),  # type: ignore[arg-type]
            'best_val_loss': self.best_val_loss,
            'best_val_f1': self.best_val_f1,
            'history': [m.to_dict() for m in self.history]
        }
        
        torch.save(checkpoint, checkpoint_path)
        logger.debug(f"Checkpoint saved: {checkpoint_path}")
    
    def load_checkpoint(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ checkpoint."""
        checkpoint = torch.load(path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if checkpoint.get('scheduler_state_dict') and self.scheduler:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        self.best_val_f1 = checkpoint.get('best_val_f1', 0.0)
        
        logger.info(f"Checkpoint loaded: {path}")
        return checkpoint
    
    @property
    def training(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è."""
        return self.model.training


# ============================================================================
# FACTORY FUNCTION
# ============================================================================

def create_trainer_v2(
    model: nn.Module,
    config: Optional[TrainerConfigV2] = None,
    preset: str = "production_small"
) -> ModelTrainerV2:
    """
    –°–æ–∑–¥–∞—Ç—å trainer —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏.
    
    Args:
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è preset)
        preset: –ü—Ä–µ—Å–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    
    Returns:
        ModelTrainerV2 instance
    """
    if config is None:
        if preset == "production_small":
            config = TrainerConfigV2(
                epochs=150,
                learning_rate=5e-5,
                batch_size=256,
                weight_decay=0.01,
                label_smoothing=0.1,
                use_augmentation=True,
                mixup_alpha=0.2,
                focal_gamma=2.5
            )
        elif preset == "quick_experiment":
            config = TrainerConfigV2(
                epochs=30,
                learning_rate=1e-4,
                batch_size=128,
                weight_decay=0.001,
                label_smoothing=0.0,
                use_augmentation=False,
                early_stopping_patience=10
            )
        else:
            config = TrainerConfigV2()
    
    return ModelTrainerV2(model, config)


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("MODEL TRAINER V2 - EXAMPLE")
    print("=" * 80)
    
    # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å
    from backend.ml_engine.models.hybrid_cnn_lstm import create_model
    
    model = create_model()
    
    # –°–æ–∑–¥–∞—ë–º trainer —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    config = TrainerConfigV2(
        epochs=5,  # –î–ª—è —Ç–µ—Å—Ç–∞
        learning_rate=5e-5,
        batch_size=64,
        use_mixed_precision=False,
        verbose=True
    )
    
    trainer = create_trainer_v2(model, config)
    
    print("\n‚úÖ ModelTrainerV2 —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ!")
    print(f"  ‚Ä¢ Device: {trainer.device}")
    print(f"  ‚Ä¢ Learning rate: {config.learning_rate}")
    print(f"  ‚Ä¢ Scheduler: {config.scheduler_type}")
    
    print("\n" + "=" * 80)
