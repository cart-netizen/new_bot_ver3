#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–µ Loss Functions –¥–ª—è ML –º–æ–¥–µ–ª–∏ - Industry Standard.

–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
1. LabelSmoothingCrossEntropy - –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overconfidence
2. FocalLossV2 - —É–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞ hard examples
3. AsymmetricFocalLoss - —Ä–∞–∑–Ω—ã–µ gamma –¥–ª—è pos/neg
4. MultiTaskLossV2 - —É–ª—É—á—à–µ–Ω–Ω—ã–π multi-task loss
5. ConfidenceCalibrationLoss - –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ confidence
6. DirectionalAccuracyLoss - —à—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ

–ü—É—Ç—å: backend/ml_engine/training/losses.py
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple, List
import numpy as np
from collections import Counter

from backend.core.logger import get_logger

logger = get_logger(__name__)


# ============================================================================
# LABEL SMOOTHING
# ============================================================================

class LabelSmoothingCrossEntropy(nn.Module):
    """
    Cross Entropy —Å Label Smoothing.
    
    Label Smoothing:
    - –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overconfidence –º–æ–¥–µ–ª–∏
    - –£–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é
    - –î–µ–ª–∞–µ—Ç –º–æ–¥–µ–ª—å –±–æ–ª–µ–µ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–æ–π
    
    –§–æ—Ä–º—É–ª–∞:
        soft_targets = (1 - smoothing) * one_hot + smoothing / num_classes
        loss = CrossEntropy(predictions, soft_targets)
    
    –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
        - smoothing=0.1 –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á
        - smoothing=0.05-0.15 –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(
        self,
        smoothing: float = 0.1,
        weight: Optional[torch.Tensor] = None,
        reduction: str = 'mean'
    ):
        """
        Args:
            smoothing: –°—Ç–µ–ø–µ–Ω—å —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (0 = off, 0.1 = recommended)
            weight: –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ (–¥–ª—è class imbalance)
            reduction: 'mean', 'sum', 'none'
        """
        super().__init__()
        
        assert 0 <= smoothing < 1, "smoothing –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ [0, 1)"
        
        self.smoothing = smoothing
        self.weight = weight
        self.reduction = reduction
        
        logger.info(f"‚úì LabelSmoothingCrossEntropy: smoothing={smoothing}")
    
    def forward(
        self,
        logits: torch.Tensor,
        targets: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            logits: (batch, num_classes) - raw logits
            targets: (batch,) - class indices
        
        Returns:
            loss: scalar –∏–ª–∏ (batch,) –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç reduction
        """
        num_classes = logits.size(-1)
        
        # Log softmax –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        log_probs = F.log_softmax(logits, dim=-1)
        
        # –°–æ–∑–¥–∞—ë–º soft targets
        with torch.no_grad():
            # One-hot encoding
            one_hot = torch.zeros_like(log_probs)
            one_hot.scatter_(1, targets.unsqueeze(1), 1)
            
            # Label smoothing
            smooth_targets = (
                (1 - self.smoothing) * one_hot +
                self.smoothing / num_classes
            )
        
        # Cross entropy —Å soft targets
        loss = -torch.sum(smooth_targets * log_probs, dim=-1)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º class weights
        if self.weight is not None:
            weight = self.weight.to(logits.device)
            loss = loss * weight[targets]
        
        # Reduction
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss


# ============================================================================
# FOCAL LOSS
# ============================================================================

class FocalLossV2(nn.Module):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è Focal Loss –¥–ª—è class imbalance.
    
    Focal Loss —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ hard examples,
    —É–º–µ–Ω—å—à–∞—è –≤–∫–ª–∞–¥ easy examples –≤ loss.
    
    –§–æ—Ä–º—É–ª–∞:
        FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)
    
    –≥–¥–µ:
        p_t = probability of correct class
        gamma = focusing parameter (higher = more focus on hard)
        alpha = class weights
    
    –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:
        - gamma=2.0-3.0 (higher –ø—Ä–∏ —Å–∏–ª—å–Ω–æ–º imbalance)
        - alpha = inverse class frequency
    """
    
    def __init__(
        self,
        gamma: float = 2.0,
        alpha: Optional[torch.Tensor] = None,
        reduction: str = 'mean',
        label_smoothing: float = 0.0
    ):
        """
        Args:
            gamma: Focusing parameter (0 = standard CE, 2-3 = recommended)
            alpha: Class weights tensor (num_classes,)
            reduction: 'mean', 'sum', 'none'
            label_smoothing: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
        """
        super().__init__()
        
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction
        self.label_smoothing = label_smoothing
        
        logger.info(
            f"‚úì FocalLossV2: gamma={gamma}, "
            f"alpha={'auto' if alpha is None else 'custom'}, "
            f"label_smoothing={label_smoothing}"
        )
    
    def forward(
        self,
        logits: torch.Tensor,
        targets: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            logits: (batch, num_classes)
            targets: (batch,)
        
        Returns:
            loss: scalar
        """
        num_classes = logits.size(-1)
        
        # Probabilities
        probs = F.softmax(logits, dim=-1)
        
        # Get probability of correct class
        ce_loss = F.cross_entropy(
            logits, targets,
            reduction='none',
            label_smoothing=self.label_smoothing
        )
        
        # p_t = probability of correct class
        p_t = probs.gather(1, targets.unsqueeze(1)).squeeze(1)
        
        # Focal weight: (1 - p_t)^gamma
        focal_weight = (1 - p_t) ** self.gamma
        
        # Apply focal weight
        focal_loss = focal_weight * ce_loss
        
        # Apply class weights (alpha)
        if self.alpha is not None:
            alpha = self.alpha.to(logits.device)
            alpha_t = alpha.gather(0, targets)
            focal_loss = alpha_t * focal_loss
        
        # Reduction
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class AsymmetricFocalLoss(nn.Module):
    """
    Asymmetric Focal Loss —Å —Ä–∞–∑–Ω—ã–º–∏ gamma –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö/–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö.
    
    –ü–æ–ª–µ–∑–Ω–æ –∫–æ–≥–¥–∞:
    - False Negatives –±–æ–ª–µ–µ –¥–æ—Ä–æ–≥–∏ —á–µ–º False Positives (–∏–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç)
    - –ù—É–∂–µ–Ω —Ä–∞–∑–Ω—ã–π —Ñ–æ–∫—É—Å –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ç–æ—Ä–≥–æ–≤–ª–µ:
    - –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ö–æ—Ä–æ—à—É—é —Å–¥–µ–ª–∫—É (FN) –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–µ—à–µ–≤–ª–µ —á–µ–º –ø–ª–æ—Ö–∞—è —Å–¥–µ–ª–∫–∞ (FP)
    """
    
    def __init__(
        self,
        gamma_pos: float = 2.0,
        gamma_neg: float = 4.0,
        alpha: Optional[torch.Tensor] = None,
        reduction: str = 'mean'
    ):
        """
        Args:
            gamma_pos: Gamma –¥–ª—è positive samples
            gamma_neg: Gamma –¥–ª—è negative samples (–æ–±—ã—á–Ω–æ –≤—ã—à–µ)
            alpha: Class weights
            reduction: 'mean', 'sum', 'none'
        """
        super().__init__()
        
        self.gamma_pos = gamma_pos
        self.gamma_neg = gamma_neg
        self.alpha = alpha
        self.reduction = reduction
    
    def forward(
        self,
        logits: torch.Tensor,
        targets: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            logits: (batch, num_classes)
            targets: (batch,)
        
        Returns:
            loss: scalar
        """
        probs = F.softmax(logits, dim=-1)
        
        ce_loss = F.cross_entropy(logits, targets, reduction='none')
        
        p_t = probs.gather(1, targets.unsqueeze(1)).squeeze(1)
        
        # Asymmetric gamma: –≤—ã—à–µ –¥–ª—è negative
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º "positive" –∫–∞–∫ non-HOLD –∫–ª–∞—Å—Å—ã (1 –∏ 2)
        is_positive = (targets != 0)  # HOLD = 0
        
        gamma = torch.where(
            is_positive,
            torch.tensor(self.gamma_pos, device=logits.device),
            torch.tensor(self.gamma_neg, device=logits.device)
        )
        
        focal_weight = (1 - p_t) ** gamma
        focal_loss = focal_weight * ce_loss
        
        if self.alpha is not None:
            alpha = self.alpha.to(logits.device)
            alpha_t = alpha.gather(0, targets)
            focal_loss = alpha_t * focal_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


# ============================================================================
# MULTI-TASK LOSS
# ============================================================================

class MultiTaskLossV2(nn.Module):
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π Multi-Task Loss –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
    
    Tasks:
    1. Direction prediction (classification)
    2. Confidence prediction (regression)
    3. Expected return prediction (regression)
    
    –£–ª—É—á—à–µ–Ω–∏—è:
    - Learnable task weights (uncertainty weighting)
    - Gradient balancing
    - Temperature scaling –¥–ª—è confidence
    """
    
    def __init__(
        self,
        direction_weight: float = 1.0,
        confidence_weight: float = 0.5,
        return_weight: float = 0.3,
        direction_criterion: Optional[nn.Module] = None,
        use_uncertainty_weighting: bool = False,
        confidence_temperature: float = 1.0
    ):
        """
        Args:
            direction_weight: –í–µ—Å direction loss
            confidence_weight: –í–µ—Å confidence loss
            return_weight: –í–µ—Å return loss
            direction_criterion: Custom criterion –¥–ª—è direction (FocalLoss, etc.)
            use_uncertainty_weighting: Learnable –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ uncertainty
            confidence_temperature: Temperature –¥–ª—è confidence calibration
        """
        super().__init__()
        
        self.direction_weight = direction_weight
        self.confidence_weight = confidence_weight
        self.return_weight = return_weight
        self.confidence_temperature = confidence_temperature
        
        # Direction criterion
        if direction_criterion is not None:
            self.direction_criterion = direction_criterion
        else:
            self.direction_criterion = nn.CrossEntropyLoss()
        
        # Regression losses
        self.mse_loss = nn.MSELoss()
        self.smooth_l1 = nn.SmoothL1Loss()  # –ë–æ–ª–µ–µ —Ä–æ–±–∞—Å—Ç–Ω—ã–π –¥–ª—è return
        
        # Uncertainty weighting (learnable)
        self.use_uncertainty_weighting = use_uncertainty_weighting
        if use_uncertainty_weighting:
            # log(sigma^2) –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏
            self.log_var_direction = nn.Parameter(torch.zeros(1))
            self.log_var_confidence = nn.Parameter(torch.zeros(1))
            self.log_var_return = nn.Parameter(torch.zeros(1))
        
        logger.info(
            f"‚úì MultiTaskLossV2: direction={direction_weight}, "
            f"confidence={confidence_weight}, return={return_weight}, "
            f"uncertainty_weighting={use_uncertainty_weighting}"
        )
    
    def forward(
        self,
        outputs: Dict[str, torch.Tensor],
        targets: Dict[str, torch.Tensor]
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            outputs: Dict —Å –≤—ã—Ö–æ–¥–∞–º–∏ –º–æ–¥–µ–ª–∏
                - direction_logits: (batch, num_classes)
                - confidence: (batch, 1)
                - expected_return: (batch, 1)
            targets: Dict —Å —Ü–µ–ª—è–º–∏
                - label: (batch,) - direction labels
                - confidence: (batch,) - target confidence (optional)
                - return: (batch,) - target return (optional)
        
        Returns:
            total_loss: scalar
            loss_components: Dict —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ loss
        """
        loss_components = {}
        
        # 1. Direction Loss
        direction_logits = outputs['direction_logits']
        direction_targets = targets['label']
        direction_loss = self.direction_criterion(direction_logits, direction_targets)
        loss_components['direction_loss'] = direction_loss.item()
        
        # 2. Confidence Loss (optional - –µ—Å–ª–∏ –µ—Å—Ç—å target)
        if 'confidence' in targets:
            pred_confidence = outputs['confidence'].squeeze(-1)
            target_confidence = targets['confidence']
            confidence_loss = self.mse_loss(pred_confidence, target_confidence)
        else:
            # Self-supervised: confidence –¥–æ–ª–∂–µ–Ω –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å—é
            pred_confidence = outputs['confidence'].squeeze(-1)
            pred_class = torch.argmax(direction_logits, dim=-1)
            is_correct = (pred_class == direction_targets).float()
            confidence_loss = self.mse_loss(pred_confidence, is_correct)
        
        loss_components['confidence_loss'] = confidence_loss.item()
        
        # 3. Return Loss (optional)
        if 'return' in targets:
            pred_return = outputs['expected_return'].squeeze(-1)
            target_return = targets['return']
            return_loss = self.smooth_l1(pred_return, target_return)
            loss_components['return_loss'] = return_loss.item()
        else:
            return_loss = torch.tensor(0.0, device=direction_loss.device)
            loss_components['return_loss'] = 0.0
        
        # Combine losses
        if self.use_uncertainty_weighting:
            # Uncertainty weighting: loss / (2 * sigma^2) + log(sigma)
            total_loss = (
                direction_loss * torch.exp(-self.log_var_direction) +
                self.log_var_direction +
                confidence_loss * torch.exp(-self.log_var_confidence) +
                self.log_var_confidence +
                return_loss * torch.exp(-self.log_var_return) +
                self.log_var_return
            )
            
            loss_components['log_var_direction'] = self.log_var_direction.item()
            loss_components['log_var_confidence'] = self.log_var_confidence.item()
            loss_components['log_var_return'] = self.log_var_return.item()
        else:
            total_loss = (
                self.direction_weight * direction_loss +
                self.confidence_weight * confidence_loss +
                self.return_weight * return_loss
            )
        
        loss_components['total_loss'] = total_loss.item()
        
        return total_loss, loss_components


# ============================================================================
# SPECIALIZED LOSSES
# ============================================================================

class DirectionalAccuracyLoss(nn.Module):
    """
    Loss —Å —à—Ç—Ä–∞—Ñ–æ–º –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ.
    
    –í —Ç—Ä–µ–π–¥–∏–Ω–≥–µ:
    - –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å BUY –∫–æ–≥–¥–∞ —Ä–µ–∞–ª—å–Ω–æ SELL = –æ—á–µ–Ω—å –ø–ª–æ—Ö–æ
    - –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å HOLD –∫–æ–≥–¥–∞ —Ä–µ–∞–ª—å–Ω–æ BUY/SELL = –º–µ–Ω–µ–µ –ø–ª–æ—Ö–æ
    
    –®—Ç—Ä–∞—Ñ—ã:
        BUY‚ÜîSELL: —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π —à—Ç—Ä–∞—Ñ
        BUY‚ÜîHOLD –∏–ª–∏ SELL‚ÜîHOLD: –º–µ–Ω—å—à–∏–π —à—Ç—Ä–∞—Ñ
        Correct: –Ω–µ—Ç —à—Ç—Ä–∞—Ñ–∞
    """
    
    def __init__(
        self,
        opposite_penalty: float = 2.0,
        neutral_penalty: float = 1.0,
        base_criterion: Optional[nn.Module] = None
    ):
        """
        Args:
            opposite_penalty: –ú–Ω–æ–∂–∏—Ç–µ–ª—å —à—Ç—Ä–∞—Ñ–∞ –∑–∞ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            neutral_penalty: –ú–Ω–æ–∂–∏—Ç–µ–ª—å —à—Ç—Ä–∞—Ñ–∞ –∑–∞ HOLD –æ—à–∏–±–∫—É
            base_criterion: –ë–∞–∑–æ–≤—ã–π criterion
        """
        super().__init__()
        
        self.opposite_penalty = opposite_penalty
        self.neutral_penalty = neutral_penalty
        
        if base_criterion is None:
            self.base_criterion = nn.CrossEntropyLoss(reduction='none')
        else:
            self.base_criterion = base_criterion
    
    def forward(
        self,
        logits: torch.Tensor,
        targets: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            logits: (batch, num_classes)
            targets: (batch,) - 0=HOLD, 1=BUY, 2=SELL
        
        Returns:
            loss: scalar
        """
        base_loss = self.base_criterion(logits, targets)
        
        # Predictions
        predictions = torch.argmax(logits, dim=-1)
        
        # Penalty weights
        # BUY=1, SELL=2 ‚Üí opposite –æ–∑–Ω–∞—á–∞–µ—Ç |pred - target| = 1 –∏ –æ–±–∞ != 0
        is_opposite = (
            ((predictions == 1) & (targets == 2)) |
            ((predictions == 2) & (targets == 1))
        )
        
        is_neutral_error = (
            ((predictions == 0) & (targets != 0)) |  # Pred HOLD, target BUY/SELL
            ((predictions != 0) & (targets == 0))    # Pred BUY/SELL, target HOLD
        )
        
        # Apply penalties
        penalty = torch.ones_like(base_loss)
        penalty[is_opposite] = self.opposite_penalty
        penalty[is_neutral_error] = self.neutral_penalty
        
        weighted_loss = base_loss * penalty
        
        return weighted_loss.mean()


class ConfidenceCalibrationLoss(nn.Module):
    """
    Loss –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ confidence –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.
    
    –¶–µ–ª—å: predicted confidence ‚âà actual accuracy
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Expected Calibration Error (ECE) –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π loss.
    """
    
    def __init__(
        self,
        n_bins: int = 10,
        calibration_weight: float = 0.1
    ):
        """
        Args:
            n_bins: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ bins –¥–ª—è ECE
            calibration_weight: –í–µ—Å calibration loss
        """
        super().__init__()
        
        self.n_bins = n_bins
        self.calibration_weight = calibration_weight
        self.bins = torch.linspace(0, 1, n_bins + 1)
    
    def forward(
        self,
        confidences: torch.Tensor,
        predictions: torch.Tensor,
        targets: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            confidences: (batch,) predicted confidence
            predictions: (batch,) predicted class
            targets: (batch,) true class
        
        Returns:
            ece: Expected Calibration Error
        """
        accuracies = (predictions == targets).float()
        
        ece = torch.tensor(0.0, device=confidences.device)
        
        for i in range(self.n_bins):
            bin_lower = self.bins[i].item()
            bin_upper = self.bins[i + 1].item()
            
            # Samples –≤ —ç—Ç–æ–º bin
            in_bin = (confidences > bin_lower) & (confidences <= bin_upper)
            
            if in_bin.sum() > 0:
                # –°—Ä–µ–¥–Ω—è—è accuracy –∏ confidence –≤ bin
                avg_confidence = confidences[in_bin].mean()
                avg_accuracy = accuracies[in_bin].mean()
                
                # |accuracy - confidence| –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –ø–æ —Ä–∞–∑–º–µ—Ä—É bin
                bin_weight = in_bin.float().mean()
                ece += bin_weight * torch.abs(avg_accuracy - avg_confidence)
        
        return self.calibration_weight * ece


# ============================================================================
# LOSS FACTORY
# ============================================================================

class LossFactory:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è loss functions."""
    
    @staticmethod
    def create_direction_loss(
        loss_type: str = "focal",
        num_classes: int = 3,
        class_weights: Optional[np.ndarray] = None,
        gamma: float = 2.5,
        label_smoothing: float = 0.1,
        device: str = "cpu"
    ) -> nn.Module:
        """
        –°–æ–∑–¥–∞—Ç—å loss –¥–ª—è direction prediction.
        
        Args:
            loss_type: 'ce', 'focal', 'focal_smooth', 'asymmetric'
            num_classes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
            class_weights: –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ (numpy array)
            gamma: Focal gamma
            label_smoothing: Label smoothing
            device: Device –¥–ª—è tensors
        
        Returns:
            Loss module
        """
        # Convert weights to tensor
        weight_tensor = None
        if class_weights is not None:
            weight_tensor = torch.tensor(class_weights, dtype=torch.float32, device=device)
        
        if loss_type == "ce":
            return nn.CrossEntropyLoss(weight=weight_tensor)
        
        elif loss_type == "ce_smooth":
            return LabelSmoothingCrossEntropy(
                smoothing=label_smoothing,
                weight=weight_tensor
            )
        
        elif loss_type == "focal":
            return FocalLossV2(
                gamma=gamma,
                alpha=weight_tensor,
                label_smoothing=0.0
            )
        
        elif loss_type == "focal_smooth":
            return FocalLossV2(
                gamma=gamma,
                alpha=weight_tensor,
                label_smoothing=label_smoothing
            )
        
        elif loss_type == "asymmetric":
            return AsymmetricFocalLoss(
                gamma_pos=gamma,
                gamma_neg=gamma + 1.0,
                alpha=weight_tensor
            )
        
        elif loss_type == "directional":
            base = FocalLossV2(gamma=gamma, alpha=weight_tensor)
            return DirectionalAccuracyLoss(
                opposite_penalty=2.0,
                neutral_penalty=1.0,
                base_criterion=base
            )
        
        else:
            raise ValueError(f"Unknown loss type: {loss_type}")
    
    @staticmethod
    def create_multi_task_loss(
        direction_loss_type: str = "focal_smooth",
        num_classes: int = 3,
        class_weights: Optional[np.ndarray] = None,
        gamma: float = 2.5,
        label_smoothing: float = 0.1,
        direction_weight: float = 1.0,
        confidence_weight: float = 0.5,
        return_weight: float = 0.3,
        use_uncertainty_weighting: bool = False,
        device: str = "cpu"
    ) -> MultiTaskLossV2:
        """
        –°–æ–∑–¥–∞—Ç—å MultiTaskLoss —Å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º direction loss.
        
        Returns:
            MultiTaskLossV2 instance
        """
        direction_criterion = LossFactory.create_direction_loss(
            loss_type=direction_loss_type,
            num_classes=num_classes,
            class_weights=class_weights,
            gamma=gamma,
            label_smoothing=label_smoothing,
            device=device
        )
        
        return MultiTaskLossV2(
            direction_weight=direction_weight,
            confidence_weight=confidence_weight,
            return_weight=return_weight,
            direction_criterion=direction_criterion,
            use_uncertainty_weighting=use_uncertainty_weighting
        )


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def compute_class_weights(
    labels: np.ndarray,
    method: str = "balanced",
    smooth_factor: float = 0.0
) -> np.ndarray:
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è imbalanced data.
    
    Args:
        labels: –ú–∞—Å—Å–∏–≤ –º–µ—Ç–æ–∫
        method: 'balanced', 'sqrt', 'log', 'effective'
        smooth_factor: –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤
    
    Returns:
        weights: Array –≤–µ—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    """
    class_counts = Counter(labels)
    num_classes = len(class_counts)
    total_samples = len(labels)
    
    if method == "balanced":
        # sklearn-style: n_samples / (n_classes * n_samples_per_class)
        weights = {
            cls: total_samples / (num_classes * count)
            for cls, count in class_counts.items()
        }
    
    elif method == "sqrt":
        # Square root of balanced
        weights = {
            cls: np.sqrt(total_samples / (num_classes * count))
            for cls, count in class_counts.items()
        }
    
    elif method == "log":
        # Log of inverse frequency
        weights = {
            cls: np.log(total_samples / count + 1)
            for cls, count in class_counts.items()
        }
    
    elif method == "effective":
        # Effective number of samples (CB loss paper)
        beta = 0.9999
        weights = {
            cls: (1 - beta) / (1 - beta ** count)
            for cls, count in class_counts.items()
        }
    
    else:
        raise ValueError(f"Unknown method: {method}")
    
    # Normalize
    weight_sum = sum(weights.values())
    weights = {k: v * num_classes / weight_sum for k, v in weights.items()}
    
    # Apply smoothing
    if smooth_factor > 0:
        weights = {k: v * (1 - smooth_factor) + smooth_factor for k, v in weights.items()}
    
    # Convert to array
    weight_array = np.array([weights.get(i, 1.0) for i in range(num_classes)])
    
    logger.info(f"Class weights ({method}): {weight_array}")
    
    return weight_array.astype(np.float32)


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("–¢–ï–°–¢ LOSS FUNCTIONS")
    print("=" * 80)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    batch_size = 32
    num_classes = 3
    
    logits = torch.randn(batch_size, num_classes)
    targets = torch.randint(0, num_classes, (batch_size,))
    
    print(f"\nüìä Input shapes:")
    print(f"  ‚Ä¢ logits: {logits.shape}")
    print(f"  ‚Ä¢ targets: {targets.shape}")
    
    # 1. Label Smoothing
    print("\n1Ô∏è‚É£ LabelSmoothingCrossEntropy:")
    ls_loss = LabelSmoothingCrossEntropy(smoothing=0.1)
    loss_val = ls_loss(logits, targets)
    print(f"  ‚Ä¢ Loss: {loss_val.item():.4f}")
    
    # 2. Focal Loss
    print("\n2Ô∏è‚É£ FocalLossV2:")
    focal_loss = FocalLossV2(gamma=2.5)
    loss_val = focal_loss(logits, targets)
    print(f"  ‚Ä¢ Loss: {loss_val.item():.4f}")
    
    # 3. Focal Loss —Å class weights
    print("\n3Ô∏è‚É£ FocalLossV2 —Å class weights:")
    labels_np = targets.numpy()
    weights = compute_class_weights(labels_np, method="balanced")
    weights_tensor = torch.tensor(weights)
    
    focal_weighted = FocalLossV2(gamma=2.5, alpha=weights_tensor)
    loss_val = focal_weighted(logits, targets)
    print(f"  ‚Ä¢ Loss: {loss_val.item():.4f}")
    
    # 4. MultiTaskLoss
    print("\n4Ô∏è‚É£ MultiTaskLossV2:")
    outputs = {
        'direction_logits': logits,
        'confidence': torch.sigmoid(torch.randn(batch_size, 1)),
        'expected_return': torch.randn(batch_size, 1) * 0.01
    }
    targets_dict = {'label': targets}
    
    multi_loss = MultiTaskLossV2(
        direction_criterion=focal_weighted,
        use_uncertainty_weighting=False
    )
    total_loss, components = multi_loss(outputs, targets_dict)
    
    print(f"  ‚Ä¢ Total loss: {total_loss.item():.4f}")
    print(f"  ‚Ä¢ Components: {components}")
    
    # 5. Factory
    print("\n5Ô∏è‚É£ LossFactory:")
    factory_loss = LossFactory.create_multi_task_loss(
        direction_loss_type="focal_smooth",
        class_weights=weights,
        gamma=2.5,
        label_smoothing=0.1
    )
    total_loss, components = factory_loss(outputs, targets_dict)
    print(f"  ‚Ä¢ Total loss: {total_loss.item():.4f}")
    
    print("\n" + "=" * 80)
    print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã!")
    print("=" * 80)
