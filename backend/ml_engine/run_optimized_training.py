#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python run_optimized_training.py --symbols BTCUSDT ETHUSDT --days 30
    python run_optimized_training.py --preset quick  # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
    python run_optimized_training.py --preset production  # Production

–ü—É—Ç—å: backend/ml_engine/run_optimized_training.py
"""

import os
import sys
import asyncio
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Optional

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
ROOT_DIR = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(ROOT_DIR))

import torch
import numpy as np


def setup_environment():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º."""
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    seed = 42
    torch.manual_seed(seed)
    np.random.seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
    import warnings
    warnings.filterwarnings('ignore', category=FutureWarning)
    warnings.filterwarnings('ignore', category=UserWarning)


def get_preset_config(preset: str) -> dict:
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –ø—Ä–µ—Å–µ—Ç–∞."""
    
    presets = {
        'quick': {
            'description': '–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç (5 –º–∏–Ω—É—Ç)',
            'epochs': 10,
            'learning_rate': 1e-4,
            'batch_size': 128,
            'model_preset': 'quick_experiment',
            'early_stopping_patience': 5
        },
        'development': {
            'description': '–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ (30 –º–∏–Ω—É—Ç)',
            'epochs': 50,
            'learning_rate': 5e-5,
            'batch_size': 256,
            'model_preset': 'production_small',
            'early_stopping_patience': 10
        },
        'production': {
            'description': 'Production (2-4 —á–∞—Å–∞)',
            'epochs': 150,
            'learning_rate': 5e-5,
            'batch_size': 256,
            'model_preset': 'production_small',
            'early_stopping_patience': 20
        },
        'production_large': {
            'description': 'Production Large Data (4-8 —á–∞—Å–æ–≤)',
            'epochs': 100,
            'learning_rate': 1e-4,
            'batch_size': 128,
            'model_preset': 'production_large',
            'early_stopping_patience': 15
        }
    }
    
    if preset not in presets:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–µ—Å–µ—Ç: {preset}")
        print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–µ—Å–µ—Ç—ã: {list(presets.keys())}")
        sys.exit(1)
    
    return presets[preset]


def print_banner():
    """–í—ã–≤–æ–¥ –±–∞–Ω–Ω–µ—Ä–∞."""
    banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                              ‚ïë
‚ïë       ü§ñ ML MODEL TRAINING - OPTIMIZED v2                                   ‚ïë
‚ïë                                                                              ‚ïë
‚ïë       Industry Standard Configuration                                        ‚ïë
‚ïë       - Learning Rate: 5e-5 (not 0.001!)                                    ‚ïë
‚ïë       - Batch Size: 256                                                      ‚ïë
‚ïë       - Focal Loss + Label Smoothing                                         ‚ïë
‚ïë       - MixUp Data Augmentation                                              ‚ïë
‚ïë       - CosineAnnealingWarmRestarts                                          ‚ïë
‚ïë                                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """
    print(banner)


def print_config(args, preset_config: dict):
    """–í—ã–≤–æ–¥ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
    print("\nüìã –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø:")
    print("‚îÄ" * 60)
    print(f"  –ü—Ä–µ—Å–µ—Ç: {args.preset} - {preset_config['description']}")
    print(f"  –°–∏–º–≤–æ–ª—ã: {args.symbols}")
    print(f"  –î–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö: {args.days}")
    print(f"  Epochs: {preset_config['epochs']}")
    print(f"  Learning Rate: {preset_config['learning_rate']}")
    print(f"  Batch Size: {preset_config['batch_size']}")
    print(f"  Model: {preset_config['model_preset']}")
    print(f"  Device: {args.device}")
    print(f"  Output: {args.output_dir}")
    print("‚îÄ" * 60)


async def run_training_with_orchestrator(args, preset_config: dict):
    """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ Orchestrator."""
    
    from backend.ml_engine.training_orchestrator_v2 import (
        TrainingOrchestratorV2,
        OrchestratorConfig
    )
    
    config = OrchestratorConfig(
        symbols=args.symbols,
        feature_store_days=args.days,
        model_preset=preset_config['model_preset'],
        trainer_preset=args.preset,
        output_dir=args.output_dir,
        device=args.device,
        use_feature_store=not args.legacy_only
    )
    
    orchestrator = TrainingOrchestratorV2(config)
    results = await orchestrator.run_training()
    
    return results


async def run_training_standalone(args, preset_config: dict):
    """
    Standalone –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ Orchestrator.
    
    –î–ª—è —Å–ª—É—á–∞–µ–≤ –∫–æ–≥–¥–∞ –Ω—É–∂–µ–Ω –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –∑–∞–ø—É—Å–∫.
    """
    from backend.core.logger import get_logger
    logger = get_logger(__name__)
    
    logger.info("Standalone training mode")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    from backend.ml_engine.training.data_loader import HistoricalDataLoader, DataConfig
    
    data_config = DataConfig(
        storage_path="data/ml_training",
        batch_size=preset_config['batch_size'],
        sequence_length=60
    )
    
    data_loader = HistoricalDataLoader(data_config)
    
    try:
        train_loader, val_loader, test_loader = data_loader.load_and_split(
            symbols=args.symbols
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return {'status': 'error', 'error': str(e)}
    
    if train_loader is None:
        logger.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return {'status': 'error', 'error': 'No training data'}
    
    logger.info(f"‚úì –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(train_loader.dataset)} samples")
    
    # 2. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    logger.info("\nüèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    from backend.ml_engine.models.hybrid_cnn_lstm_v2 import create_model_v2_from_preset
    
    model = create_model_v2_from_preset(preset_config['model_preset'])
    
    device = torch.device(args.device if args.device != 'auto' else 
                         ('cuda' if torch.cuda.is_available() else 'cpu'))
    model.to(device)
    
    model_size = model.get_model_size()
    logger.info(f"‚úì –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {model_size['total_params']:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    
    # 3. –û–±—É—á–µ–Ω–∏–µ
    logger.info("\nüéì –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    from backend.ml_engine.training.model_trainer_v2 import (
        ModelTrainerV2,
        TrainerConfigV2
    )
    
    trainer_config = TrainerConfigV2(
        epochs=preset_config['epochs'],
        learning_rate=preset_config['learning_rate'],
        batch_size=preset_config['batch_size'],
        weight_decay=0.01,
        label_smoothing=0.1,
        use_augmentation=True,
        mixup_alpha=0.2,
        focal_gamma=2.5,
        early_stopping_patience=preset_config['early_stopping_patience'],
        checkpoint_dir=args.output_dir,
        device=str(device)
    )
    
    trainer = ModelTrainerV2(model, trainer_config)
    
    history = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        test_loader=test_loader
    )
    
    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    logger.info("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = output_dir / f"model_{timestamp}.pt"
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'best_val_loss': trainer.best_val_loss,
        'best_val_f1': trainer.best_val_f1,
        'history': [m.to_dict() for m in history]
    }, model_path)
    
    logger.info(f"‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    
    return {
        'status': 'success',
        'model_path': str(model_path),
        'best_val_loss': trainer.best_val_loss,
        'best_val_f1': trainer.best_val_f1,
        'epochs_trained': len(history)
    }


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    
    parser = argparse.ArgumentParser(
        description="–û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s --preset quick                    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
  %(prog)s --preset production               # Production –æ–±—É—á–µ–Ω–∏–µ
  %(prog)s --symbols BTCUSDT ETHUSDT --days 30  # –£–∫–∞–∑–∞—Ç—å —Å–∏–º–≤–æ–ª—ã
  %(prog)s --legacy-only                     # –¢–æ–ª—å–∫–æ legacy –¥–∞–Ω–Ω—ã–µ
        """
    )
    
    parser.add_argument(
        "--preset",
        choices=["quick", "development", "production", "production_large"],
        default="production",
        help="–ü—Ä–µ—Å–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (default: production)"
    )
    
    parser.add_argument(
        "--symbols",
        nargs="+",
        default=["BTCUSDT", "ETHUSDT"],
        help="–¢–æ—Ä–≥–æ–≤—ã–µ –ø–∞—Ä—ã (default: BTCUSDT ETHUSDT)"
    )
    
    parser.add_argument(
        "--days",
        type=int,
        default=30,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö (default: 30)"
    )
    
    parser.add_argument(
        "--output-dir",
        default="models/trained",
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (default: models/trained)"
    )
    
    parser.add_argument(
        "--device",
        choices=["auto", "cuda", "cpu"],
        default="auto",
        help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (default: auto)"
    )
    
    parser.add_argument(
        "--legacy-only",
        action="store_true",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ legacy –¥–∞–Ω–Ω—ã–µ"
    )
    
    parser.add_argument(
        "--standalone",
        action="store_true",
        help="Standalone —Ä–µ–∂–∏–º –±–µ–∑ Orchestrator"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="–ü–æ–∫–∞–∑–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –±–µ–∑ –∑–∞–ø—É—Å–∫–∞"
    )
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    setup_environment()
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–µ—Å–µ—Ç–∞
    preset_config = get_preset_config(args.preset)
    
    # –í—ã–≤–æ–¥–∏–º –±–∞–Ω–Ω–µ—Ä –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    print_banner()
    print_config(args, preset_config)
    
    if args.dry_run:
        print("\n‚ö†Ô∏è Dry run - –æ–±—É—á–µ–Ω–∏–µ –Ω–µ –∑–∞–ø—É—â–µ–Ω–æ")
        return 0
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    print("\nüöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")
    print("=" * 60)
    
    try:
        if args.standalone:
            results = asyncio.run(run_training_standalone(args, preset_config))
        else:
            results = asyncio.run(run_training_with_orchestrator(args, preset_config))
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n" + "=" * 60)
        
        if results.get('status') == 'success':
            print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
            print(f"   Model: {results.get('model_path', 'N/A')}")
            print(f"   Best Val Loss: {results.get('best_val_loss', 'N/A'):.4f}")
            print(f"   Best Val F1: {results.get('best_val_f1', 'N/A'):.4f}")
            return 0
        else:
            print(f"‚ùå –û–®–ò–ë–ö–ê: {results.get('error', 'Unknown error')}")
            return 1
            
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return 130
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
