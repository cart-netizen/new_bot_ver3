"""
Adaptive ML Model for Layering Detection.

Professional machine learning model that adapts thresholds and confidence
scoring based on market conditions, reducing false positives and improving
detection accuracy.

Features:
1. Random Forest Classifier for pattern classification
2. Adaptive threshold prediction based on context
3. Feature importance analysis
4. Model evaluation and metrics
5. Incremental learning support
6. Model versioning and persistence

Path: backend/ml_engine/detection/adaptive_layering_model.py
"""

from __future__ import annotations  # Makes all type hints strings automatically

import numpy as np
import pandas as pd
import pickle
import json
from typing import Dict, List, Optional, Tuple, Any, TYPE_CHECKING
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

try:
  from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
  from sklearn.model_selection import train_test_split, cross_val_score
  from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
  )
  from sklearn.preprocessing import StandardScaler
  SKLEARN_AVAILABLE = True
except ImportError:
  SKLEARN_AVAILABLE = False
  # Dummy types for when sklearn is not available
  if TYPE_CHECKING:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.preprocessing import StandardScaler

from core.logger import get_logger

logger = get_logger(__name__)


@dataclass
class ModelMetrics:
  """ML model evaluation metrics."""
  accuracy: float
  precision: float
  recall: float
  f1_score: float
  roc_auc: float
  confusion_matrix: List[List[int]]

  # Additional stats
  true_positives: int
  false_positives: int
  true_negatives: int
  false_negatives: int

  # Thresholds
  optimal_threshold: float


@dataclass
class AdaptiveThresholds:
  """Adaptive thresholds for detection."""
  volume_threshold_usdt: float
  placement_duration_threshold: float
  cancellation_rate_threshold: float
  confidence_threshold: float

  # Context
  market_regime: str
  volatility: float
  liquidity: float


class AdaptiveLayeringModel:
  """
  Professional ML model for adaptive layering detection.

  Uses Random Forest to:
  1. Classify patterns as true positives vs false positives
  2. Predict adaptive thresholds based on market conditions
  3. Adjust confidence scoring dynamically
  4. Learn from historical data
  """

  def __init__(
      self,
      model_path: Optional[str] = None,
      enabled: bool = True
  ):
    """
    Initialize adaptive model.

    Args:
        model_path: Path to saved model (None = create new)
        enabled: Whether model is enabled
    """
    self.enabled = enabled and SKLEARN_AVAILABLE
    self.model_path = model_path

    if not SKLEARN_AVAILABLE:
      logger.warning(
        "âš ï¸  sklearn not available - AdaptiveLayeringModel disabled"
      )
      self.enabled = False
      return

    # Models
    self.classifier: Optional[RandomForestClassifier] = None
    self.threshold_predictor: Optional[GradientBoostingClassifier] = None
    self.scaler: Optional[StandardScaler] = None

    # Feature names (for consistency)
    self.feature_names: List[str] = []

    # Metrics
    self.metrics: Optional[ModelMetrics] = None
    self.feature_importance: Dict[str, float] = {}

    # Model metadata
    self.version = "1.0.0"
    self.trained_at: Optional[str] = None
    self.training_samples: int = 0

    # Load model if path provided
    if model_path and Path(model_path).exists():
      self.load(model_path)
    else:
      # Initialize new models
      self._init_models()

    if self.enabled:
      logger.info(
        f"âœ… AdaptiveLayeringModel initialized: "
        f"trained={self.classifier is not None and hasattr(self.classifier, 'n_estimators')}"
      )

  def _init_models(self):
    """Initialize ML models with default hyperparameters."""
    if not self.enabled:
      return

    # Classification model (true positive vs false positive)
    self.classifier = RandomForestClassifier(
      n_estimators=100,
      max_depth=10,
      min_samples_split=5,
      min_samples_leaf=2,
      random_state=42,
      n_jobs=-1  # Use all cores
    )

    # Threshold prediction model
    self.threshold_predictor = GradientBoostingClassifier(
      n_estimators=50,
      max_depth=5,
      learning_rate=0.1,
      random_state=42
    )

    # Feature scaler
    self.scaler = StandardScaler()

    # Define feature names
    self.feature_names = [
      # Pattern features
      'total_volume_btc',
      'total_volume_usdt',
      'placement_duration',
      'cancellation_rate',
      'spoofing_execution_ratio',
      'layer_count',
      'total_orders',
      'avg_order_size',
      'price_spread_bps',

      # Market context
      'volatility_24h',
      'volume_24h',
      'liquidity_score',
      'spread_bps',
      'hour_utc',
      'day_of_week',

      # Price impact
      'price_change_bps',
      'expected_impact_bps',
      'impact_ratio',

      # Component scores
      'volume_score',
      'timing_score',
      'cancellation_score',
      'execution_correlation_score',
      'price_impact_score'
    ]

  def train(
      self,
      training_data: pd.DataFrame,
      target_column: str = 'is_true_layering',
      test_size: float = 0.2
  ) -> ModelMetrics:
    """
    Train the model on labeled data.

    Args:
        training_data: DataFrame with features and labels
        target_column: Column name for target variable
        test_size: Fraction for test set

    Returns:
        ModelMetrics: Evaluation metrics
    """
    if not self.enabled:
      logger.error("Model training requires sklearn")
      return None

    logger.info(f"ðŸŽ“ Training AdaptiveLayeringModel with {len(training_data)} samples")

    # Prepare features
    X = self._prepare_features(training_data)
    y = training_data[target_column].astype(int)

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=test_size, random_state=42, stratify=y
    )

    # Scale features
    X_train_scaled = self.scaler.fit_transform(X_train)
    X_test_scaled = self.scaler.transform(X_test)

    # Train classifier
    logger.info("Training classifier...")
    self.classifier.fit(X_train_scaled, y_train)

    # Evaluate
    y_pred = self.classifier.predict(X_test_scaled)
    y_pred_proba = self.classifier.predict_proba(X_test_scaled)[:, 1]

    # Calculate metrics
    metrics = self._calculate_metrics(y_test, y_pred, y_pred_proba)
    self.metrics = metrics

    # Feature importance
    self.feature_importance = dict(zip(
      self.feature_names,
      self.classifier.feature_importances_
    ))

    # Sort by importance
    self.feature_importance = dict(
      sorted(self.feature_importance.items(), key=lambda x: x[1], reverse=True)
    )

    # Update metadata
    self.trained_at = datetime.now().isoformat()
    self.training_samples = len(training_data)

    # Log results
    logger.info(f"âœ… Training complete!")
    logger.info(f"   Accuracy:  {metrics.accuracy:.3f}")
    logger.info(f"   Precision: {metrics.precision:.3f}")
    logger.info(f"   Recall:    {metrics.recall:.3f}")
    logger.info(f"   F1 Score:  {metrics.f1_score:.3f}")
    logger.info(f"   ROC AUC:   {metrics.roc_auc:.3f}")
    logger.info(f"   Top features: {list(self.feature_importance.keys())[:5]}")

    return metrics

  def predict(self, features: Dict) -> Tuple[bool, float]:
    """
    Predict if pattern is true layering.

    Args:
        features: Feature dictionary

    Returns:
        Tuple of (is_true_layering, confidence)
    """
    if not self.enabled or self.classifier is None:
      # Fallback to detector confidence
      return (True, features.get('detector_confidence', 0.5))

    # Prepare features
    X = self._prepare_single_sample(features)
    X_scaled = self.scaler.transform(X)

    # Predict
    prediction = self.classifier.predict(X_scaled)[0]
    confidence = self.classifier.predict_proba(X_scaled)[0][1]

    return (bool(prediction), float(confidence))

  def predict_adaptive_thresholds(
      self,
      market_context: Dict
  ) -> AdaptiveThresholds:
    """
    Predict adaptive thresholds based on market conditions.

    Args:
        market_context: Market regime, volatility, liquidity, etc.

    Returns:
        AdaptiveThresholds: Recommended thresholds
    """
    if not self.enabled or not self.feature_importance:
      # Return default thresholds
      return AdaptiveThresholds(
        volume_threshold_usdt=200000.0,
        placement_duration_threshold=30.0,
        cancellation_rate_threshold=0.60,
        confidence_threshold=0.65,
        market_regime=market_context.get('market_regime', 'unknown'),
        volatility=market_context.get('volatility_24h', 0.0),
        liquidity=market_context.get('liquidity_score', 0.0)
      )

    # Extract market features
    market_regime = market_context.get('market_regime', 'unknown')
    volatility = market_context.get('volatility_24h', 0.0)
    liquidity = market_context.get('liquidity_score', 0.0)

    # Adaptive threshold calculation based on conditions
    base_volume = 200000.0
    base_duration = 30.0
    base_cancel_rate = 0.60
    base_confidence = 0.65

    # Adjust for volatility (high volatility â†’ higher thresholds)
    if volatility > 5.0:
      volume_multiplier = 1.5
      confidence_adjustment = 0.05
    elif volatility > 3.0:
      volume_multiplier = 1.2
      confidence_adjustment = 0.02
    else:
      volume_multiplier = 1.0
      confidence_adjustment = 0.0

    # Adjust for liquidity (low liquidity â†’ lower thresholds)
    if liquidity < 0.3:
      volume_multiplier *= 0.7
      confidence_adjustment -= 0.05
    elif liquidity < 0.5:
      volume_multiplier *= 0.85
      confidence_adjustment -= 0.02

    # Adjust for market regime
    if market_regime == 'bullish':
      volume_multiplier *= 1.1
    elif market_regime == 'bearish':
      volume_multiplier *= 0.9

    return AdaptiveThresholds(
      volume_threshold_usdt=base_volume * volume_multiplier,
      placement_duration_threshold=base_duration,
      cancellation_rate_threshold=base_cancel_rate,
      confidence_threshold=min(base_confidence + confidence_adjustment, 0.95),
      market_regime=market_regime,
      volatility=volatility,
      liquidity=liquidity
    )

  def _prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
    """Prepare feature matrix from DataFrame."""
    # Select and order features
    feature_df = pd.DataFrame()

    for feature_name in self.feature_names:
      if feature_name in df.columns:
        feature_df[feature_name] = df[feature_name]
      else:
        # Fill missing features with 0
        feature_df[feature_name] = 0.0

    # Handle NaN values
    feature_df = feature_df.fillna(0.0)

    return feature_df

  def _prepare_single_sample(self, features: Dict) -> np.ndarray:
    """Prepare single sample for prediction."""
    sample = []

    for feature_name in self.feature_names:
      value = features.get(feature_name, 0.0)
      # Handle None values
      if value is None:
        value = 0.0
      sample.append(float(value))

    return np.array([sample])

  def _calculate_metrics(
      self,
      y_true: np.ndarray,
      y_pred: np.ndarray,
      y_pred_proba: np.ndarray
  ) -> ModelMetrics:
    """Calculate evaluation metrics."""
    # Basic metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    roc_auc = roc_auc_score(y_true, y_pred_proba)

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    # Find optimal threshold
    from sklearn.metrics import roc_curve
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]

    return ModelMetrics(
      accuracy=float(accuracy),
      precision=float(precision),
      recall=float(recall),
      f1_score=float(f1),
      roc_auc=float(roc_auc),
      confusion_matrix=cm.tolist(),
      true_positives=int(tp),
      false_positives=int(fp),
      true_negatives=int(tn),
      false_negatives=int(fn),
      optimal_threshold=float(optimal_threshold)
    )

  def save(self, filepath: str):
    """Save model to disk."""
    if not self.enabled:
      return

    # Ensure directory exists
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)

    # Package for saving
    model_package = {
      'version': self.version,
      'trained_at': self.trained_at,
      'training_samples': self.training_samples,
      'feature_names': self.feature_names,
      'feature_importance': self.feature_importance,
      'metrics': self.metrics.__dict__ if self.metrics else None,
      'classifier': self.classifier,
      'threshold_predictor': self.threshold_predictor,
      'scaler': self.scaler
    }

    # Save with pickle
    with open(filepath, 'wb') as f:
      pickle.dump(model_package, f)

    logger.info(f"ðŸ’¾ Model saved: {filepath}")

  def load(self, filepath: str):
    """Load model from disk."""
    if not self.enabled:
      return

    try:
      with open(filepath, 'rb') as f:
        model_package = pickle.load(f)

      # Restore
      self.version = model_package.get('version', '1.0.0')
      self.trained_at = model_package.get('trained_at')
      self.training_samples = model_package.get('training_samples', 0)
      self.feature_names = model_package.get('feature_names', [])
      self.feature_importance = model_package.get('feature_importance', {})

      metrics_dict = model_package.get('metrics')
      if metrics_dict:
        self.metrics = ModelMetrics(**metrics_dict)

      self.classifier = model_package.get('classifier')
      self.threshold_predictor = model_package.get('threshold_predictor')
      self.scaler = model_package.get('scaler')

      logger.info(
        f"âœ… Model loaded: {filepath}, "
        f"samples={self.training_samples}, "
        f"trained_at={self.trained_at}"
      )

    except Exception as e:
      logger.error(f"Error loading model: {e}")
      self._init_models()

  def get_info(self) -> Dict:
    """Get model information."""
    return {
      'enabled': self.enabled,
      'sklearn_available': SKLEARN_AVAILABLE,
      'trained': self.classifier is not None and hasattr(self.classifier, 'n_estimators'),
      'version': self.version,
      'trained_at': self.trained_at,
      'training_samples': self.training_samples,
      'feature_count': len(self.feature_names),
      'metrics': self.metrics.__dict__ if self.metrics else None,
      'top_features': list(self.feature_importance.keys())[:10] if self.feature_importance else []
    }


# Example usage
if __name__ == "__main__":
  if SKLEARN_AVAILABLE:
    model = AdaptiveLayeringModel()
    print("Model initialized")
    print(f"Info: {model.get_info()}")
  else:
    print("sklearn not available")
