#!/usr/bin/env python3
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.

–ó–∞–ø—É—Å–∫:
    python diagnose_optimization.py --symbol BTCUSDT
"""
import sys
from pathlib import Path
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ backend
backend_path = Path(__file__).parent / "backend"
sys.path.insert(0, str(backend_path))
import argparse
import numpy as np
import json
from pathlib import Path
from collections import Counter
import sys


def check_environment():
  """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è."""
  print("\n" + "=" * 60)
  print("–ü–†–û–í–ï–†–ö–ê –û–ö–†–£–ñ–ï–ù–ò–Ø")
  print("=" * 60)

  try:
    import torch
    print(f"‚úì PyTorch: {torch.__version__}")
    print(f"  CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
      print(f"  CUDA version: {torch.version.cuda}")
      print(f"  GPU: {torch.cuda.get_device_name(0)}")
  except ImportError:
    print("‚ùå PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    return False

  try:
    import sklearn
    print(f"‚úì Scikit-learn: {sklearn.__version__}")
  except ImportError:
    print("‚ùå Scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    return False

  try:
    from imblearn import over_sampling
    print(f"‚úì imbalanced-learn —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
  except ImportError:
    print("‚ö†Ô∏è  imbalanced-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")

  print()
  return True


def check_data(symbol: str, data_path: str = "data/ml_training"):
  """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö."""
  print("=" * 60)
  print(f"–ü–†–û–í–ï–†–ö–ê –î–ê–ù–ù–´–•: {symbol}")
  print("=" * 60)

  symbol_path = Path(data_path) / symbol

  if not symbol_path.exists():
    print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {symbol_path}")
    return False

  features_dir = symbol_path / "features"
  labels_dir = symbol_path / "labels"

  if not features_dir.exists():
    print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è features –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    return False

  if not labels_dir.exists():
    print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è labels –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    return False

  # –ü–æ–¥—Å—á–µ—Ç —Ñ–∞–π–ª–æ–≤
  feature_files = list(features_dir.glob("*.npy"))
  label_files = list(labels_dir.glob("*.json"))

  print(f"\n–§–∞–π–ª—ã:")
  print(f"  Feature files: {len(feature_files)}")
  print(f"  Label files: {len(label_files)}")

  if len(feature_files) == 0:
    print("‚ùå –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ features!")
    return False

  if len(label_files) == 0:
    print("‚ùå –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ labels!")
    return False

  # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤
  print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤...")

  try:
    # Features
    sample_features = np.load(feature_files[0])
    print(f"  ‚úì Features shape: {sample_features.shape}")
    print(f"    Expected: (N, 110)")

    if sample_features.shape[1] != 110:
      print(f"    ‚ö†Ô∏è  –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤!")

    # Labels
    with open(label_files[0]) as f:
      sample_labels = json.load(f)

    print(f"  ‚úì Labels loaded: {len(sample_labels)} samples")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã labels
    if sample_labels:
      first_label = sample_labels[0]
      print(f"\n  Label structure:")
      for key in first_label.keys():
        print(f"    - {key}")

      # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
      required = ["timestamp", "current_mid_price"]
      future_fields = ["future_direction_60s", "future_movement_60s"]

      missing_required = [f for f in required if f not in first_label]
      missing_future = [f for f in future_fields if f not in first_label]

      if missing_required:
        print(f"\n  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è: {missing_required}")
        return False

      if missing_future:
        print(f"\n  ‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç future labels: {missing_future}")
        print(f"     –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python preprocessing_add_future_labels.py")
        return False

      # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π future_direction
      future_dir = first_label.get("future_direction_60s")
      if future_dir not in [-1, 0, 1, None]:
        print(f"\n  ‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ future_direction_60s: {future_dir}")
        return False

  except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    import traceback
    traceback.print_exc()
    return False

  # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–µ–º–ø–ª–æ–≤
  print(f"\n–ü–æ–¥—Å—á–µ—Ç —Å–µ–º–ø–ª–æ–≤...")
  total_samples = 0

  for feature_file in feature_files:
    data = np.load(feature_file)
    total_samples += data.shape[0]

  print(f"  –í—Å–µ–≥–æ —Å–µ–º–ø–ª–æ–≤: {total_samples:,}")

  # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—ä–µ–º—É
  if total_samples < 10_000:
    print(f"  ‚ùå –ö–†–ò–¢–ò–ß–ù–û: –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ({total_samples:,} < 10,000)")
    print(f"     –ú–∏–Ω–∏–º—É–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: 10,000")
    print(f"     –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: 100,000+")
    return False
  elif total_samples < 50_000:
    print(f"  ‚ö†Ô∏è  –ú–∞–ª–æ–≤–∞—Ç–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ({total_samples:,} < 50,000)")
    print(f"     –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --quick —Ä–µ–∂–∏–º")
    print(f"     –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: 100,000+")
  elif total_samples < 100_000:
    print(f"  ‚úì –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è quick optimization")
    print(f"     –î–ª—è full optimization —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: 100,000+")
  else:
    print(f"  ‚úì –û—Ç–ª–∏—á–Ω–æ! –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è full optimization")

  # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
  print(f"\n–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤...")

  all_directions = []
  for label_file in label_files[:3]:  # –ü–µ—Ä–≤—ã–µ 3 —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
    with open(label_file) as f:
      labels = json.load(f)
      for label in labels:
        direction = label.get("future_direction_60s")
        if direction is not None:
          all_directions.append(direction)

  if all_directions:
    class_dist = Counter(all_directions)
    print(f"  –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (–ø–µ—Ä–≤—ã–µ {len(all_directions)} —Å–µ–º–ø–ª–æ–≤):")

    for cls in sorted(class_dist.keys()):
      count = class_dist[cls]
      pct = (count / len(all_directions)) * 100
      cls_name = {-1: "DOWN", 0: "NEUTRAL", 1: "UP"}.get(cls, str(cls))
      print(f"    {cls_name:8} ({cls:2d}): {count:6,} ({pct:5.1f}%)")

    # Imbalance ratio
    max_count = max(class_dist.values())
    min_count = min(class_dist.values())
    ratio = max_count / min_count

    print(f"\n  Imbalance Ratio: {ratio:.2f}")

    if ratio < 1.5:
      print(f"    ‚úì –û—Ç–ª–∏—á–Ω—ã–π –±–∞–ª–∞–Ω—Å!")
    elif ratio < 3.0:
      print(f"    ‚úì –•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å")
    elif ratio < 5.0:
      print(f"    ‚ö†Ô∏è  –°—Ä–µ–¥–Ω–∏–π –¥–∏—Å–±–∞–ª–∞–Ω—Å - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Focal Loss")
    else:
      print(f"    ‚ùå –°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Focal Loss + SMOTE")

  print()
  return True


def test_dataloader(symbol: str):
  """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö."""
  print("=" * 60)
  print("–¢–ï–°–¢ DATALOADER")
  print("=" * 60)

  try:
    from ml_engine.training.data_loader import HistoricalDataLoader, DataConfig

    print("–°–æ–∑–¥–∞–Ω–∏–µ DataLoader...")
    config = DataConfig(
      storage_path="data/ml_training",
      sequence_length=60,
      batch_size=32
    )

    loader = HistoricalDataLoader(config)
    print("‚úì DataLoader —Å–æ–∑–¥–∞–Ω")

    print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}...")
    X, y, timestamps = loader.load_symbol_data(symbol)

    print(f"‚úì –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:")
    print(f"  X shape: {X.shape}")
    print(f"  y shape: {y.shape}")
    print(f"  timestamps shape: {timestamps.shape}")
    print(f"  Unique labels: {set(y)}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
    has_nan = np.isnan(X).any()
    has_inf = np.isinf(X).any()

    if has_nan:
      print(f"  ‚ùå Features —Å–æ–¥–µ—Ä–∂–∞—Ç NaN!")
      return False

    if has_inf:
      print(f"  ‚ùå Features —Å–æ–¥–µ—Ä–∂–∞—Ç Inf!")
      return False

    print(f"  ‚úì –ù–µ—Ç NaN/Inf –∑–Ω–∞—á–µ–Ω–∏–π")

    # –°–æ–∑–¥–∞–Ω–∏–µ sequences
    print(f"\n–°–æ–∑–¥–∞–Ω–∏–µ sequences...")
    sequences, seq_labels, seq_timestamps = loader.create_sequences(
      X, y, timestamps
    )

    print(f"‚úì Sequences —Å–æ–∑–¥–∞–Ω—ã:")
    print(f"  Shape: {sequences.shape}")
    print(f"  Expected: (N, 60, 110)")

    print()
    return True

  except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    import traceback
    traceback.print_exc()
    return False


def test_model_creation():
  """–¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏."""
  print("=" * 60)
  print("–¢–ï–°–¢ –°–û–ó–î–ê–ù–ò–Ø –ú–û–î–ï–õ–ò")
  print("=" * 60)

  try:
    from ml_engine.models.hybrid_cnn_lstm import create_model

    print("–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = create_model()
    print("‚úì –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")

    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"  Total parameters: {total_params:,}")
    print(f"  Trainable parameters: {trainable_params:,}")

    print()
    return True

  except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    import traceback
    traceback.print_exc()
    return False


def test_minimal_training(symbol: str):
  """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è."""
  print("=" * 60)
  print("–ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô –¢–ï–°–¢ –û–ë–£–ß–ï–ù–ò–Ø (3 —ç–ø–æ—Ö–∏)")
  print("=" * 60)

  try:
    from ml_engine.training.data_loader import HistoricalDataLoader, DataConfig
    from ml_engine.training.model_trainer import ModelTrainer, TrainerConfig
    from ml_engine.models.hybrid_cnn_lstm import create_model

    print("1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    config = DataConfig(
      storage_path="data/ml_training",
      sequence_length=60,
      batch_size=32
    )

    loader = HistoricalDataLoader(config)
    result = loader.load_and_prepare(symbols=[symbol])

    print(f"‚úì Train: {len(result['dataloaders']['train'])} batches")
    print(f"‚úì Val: {len(result['dataloaders']['val'])} batches")

    print("\n2. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = create_model()
    print("‚úì –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")

    print("\n3. –û–±—É—á–µ–Ω–∏–µ (3 —ç–ø–æ—Ö–∏ –Ω–∞ CPU)...")
    trainer_config = TrainerConfig(
      epochs=3,
      learning_rate=0.001,
      device="cpu",  # CPU –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
      early_stopping_patience=10
    )

    trainer = ModelTrainer(model, trainer_config)
    history = trainer.train(
      result['dataloaders']['train'],
      result['dataloaders']['val']
    )

    print(f"\n‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(history)} —ç–ø–æ—Ö")

    if history:
      last_epoch = history[-1]
      print(f"  Final train_loss: {last_epoch.train_loss:.4f}")
      print(f"  Final val_loss: {last_epoch.val_loss:.4f}")
      print(f"  Final val_accuracy: {last_epoch.val_accuracy:.4f}")

    print()
    return True

  except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    import traceback
    traceback.print_exc()
    return False


def main():
  """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
  parser = argparse.ArgumentParser(
    description="–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"
  )
  parser.add_argument(
    "--symbol",
    required=True,
    help="–¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, BTCUSDT)"
  )
  parser.add_argument(
    "--skip-training-test",
    action="store_true",
    help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è"
  )

  args = parser.parse_args()

  print("\n" + "=" * 60)
  print("–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò")
  print("=" * 60)
  print(f"Symbol: {args.symbol}")
  print("=" * 60)

  # –ß–µ–∫–ª–∏—Å—Ç
  results = {
    "–û–∫—Ä—É–∂–µ–Ω–∏–µ": False,
    "–î–∞–Ω–Ω—ã–µ": False,
    "DataLoader": False,
    "–ú–æ–¥–µ–ª—å": False,
    "–û–±—É—á–µ–Ω–∏–µ": False
  }

  # 1. –û–∫—Ä—É–∂–µ–Ω–∏–µ
  results["–û–∫—Ä—É–∂–µ–Ω–∏–µ"] = check_environment()
  if not results["–û–∫—Ä—É–∂–µ–Ω–∏–µ"]:
    print("\n‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.")
    sys.exit(1)

  # 2. –î–∞–Ω–Ω—ã–µ
  results["–î–∞–Ω–Ω—ã–µ"] = check_data(args.symbol)
  if not results["–î–∞–Ω–Ω—ã–µ"]:
    print("\n‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏! –°–æ–±–µ—Ä–∏—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ preprocessing.")
    sys.exit(1)

  # 3. DataLoader
  results["DataLoader"] = test_dataloader(args.symbol)
  if not results["DataLoader"]:
    print("\n‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å DataLoader! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö.")
    sys.exit(1)

  # 4. –ú–æ–¥–µ–ª—å
  results["–ú–æ–¥–µ–ª—å"] = test_model_creation()
  if not results["–ú–æ–¥–µ–ª—å"]:
    print("\n‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–æ–∑–¥–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏!")
    sys.exit(1)

  # 5. –û–±—É—á–µ–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
  if not args.skip_training_test:
    results["–û–±—É—á–µ–Ω–∏–µ"] = test_minimal_training(args.symbol)
    if not results["–û–±—É—á–µ–Ω–∏–µ"]:
      print("\n‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å –æ–±—É—á–µ–Ω–∏–µ–º!")
      print("   –í–æ–∑–º–æ–∂–Ω–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –∏–ª–∏ –¥–∞–Ω–Ω—ã—Ö.")
      sys.exit(1)
  else:
    print("\n‚äò –¢–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ–ø—É—â–µ–Ω")

  # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
  print("\n" + "=" * 60)
  print("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
  print("=" * 60)

  for test_name, passed in results.items():
    status = "‚úì" if passed else "‚ùå"
    print(f"{status} {test_name}")

  if all(results.values()):
    print("\nüéâ –í–°–ï –ü–†–û–í–ï–†–ö–ò –ü–†–û–ô–î–ï–ù–´!")
    print("\n–ú–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é:")
    print(f"  python hyperparameter_optimizer.py --symbol {args.symbol} --quick")
  else:
    print("\n‚ùå –ï–°–¢–¨ –ü–†–û–ë–õ–ï–ú–´!")
    print("   –ò—Å–ø—Ä–∞–≤—å—Ç–µ –æ—à–∏–±–∫–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.")

  print("=" * 60 + "\n")


if __name__ == "__main__":
  main()