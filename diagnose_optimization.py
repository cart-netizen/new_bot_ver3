#!/usr/bin/env python3
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.

–ó–∞–ø—É—Å–∫:
    python diagnose_optimization.py --symbol BTCUSDT
"""
import sys
from pathlib import Path
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ backend
backend_path = Path(__file__).parent / "backend"
sys.path.insert(0, str(backend_path))
import time
import logging
import argparse
import numpy as np
import json
from pathlib import Path
from collections import Counter
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Dict
import multiprocessing as mp

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
  level=logging.INFO,
  format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def check_environment():
  """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è."""
  print("\n" + "=" * 60)
  print("–ü–†–û–í–ï–†–ö–ê –û–ö–†–£–ñ–ï–ù–ò–Ø")
  print("=" * 60)
  start_time = time.time()
  logger.info("=" * 80)
  logger.info("–ù–ê–ß–ê–õ–û –ü–†–û–í–ï–†–ö–ò –û–ö–†–£–ñ–ï–ù–ò–Ø")
  logger.info("=" * 80)
  try:
    logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ PyTorch...")
    import torch
    print(f"‚úì PyTorch: {torch.__version__}")
    logger.info(f"‚úì PyTorch {torch.__version__} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    print(f"  CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
      print(f"  CUDA version: {torch.version.cuda}")
      print(f"  GPU: {torch.cuda.get_device_name(0)}")
      logger.info(f"  CUDA version: {torch.version.cuda}")
      logger.info(f"  GPU: {torch.cuda.get_device_name(0)}")
      logger.info(f"  GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
  except ImportError as e:
    print("‚ùå PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    logger.error(f"‚ùå PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
    return False

  try:
    logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ Scikit-learn...")
    import sklearn
    print(f"‚úì Scikit-learn: {sklearn.__version__}")
    logger.info(f"‚úì Scikit-learn {sklearn.__version__} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
  except ImportError as e:
    print("‚ùå Scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    logger.error(f"‚ùå Scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
    return False

  try:
    logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ imbalanced-learn...")
    from imblearn import over_sampling
    print(f"‚úì imbalanced-learn —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    logger.info("‚úì imbalanced-learn —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
  except ImportError:
    print("‚ö†Ô∏è  imbalanced-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")
    logger.warning("‚ö†Ô∏è  imbalanced-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö CPU —è–¥–µ—Ä
    cpu_count = mp.cpu_count()
    print(f"‚úì CPU cores: {cpu_count}")
    logger.info(f"‚úì –î–æ—Å—Ç—É–ø–Ω–æ CPU —è–¥–µ—Ä: {cpu_count}")

    elapsed = time.time() - start_time
    logger.info(f"‚úì –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed:.2f}s")

  print()
  return True

def _count_samples_in_file(file_path: Path) -> int:
  """–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–µ–º–ø–ª–æ–≤ –≤ —Ñ–∞–π–ª–µ (–≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏)."""
  try:
    data = np.load(file_path, mmap_mode='r')  # Memory-mapped –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
    return data.shape[0]
  except Exception as e:
    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file_path.name}: {e}")
    return 0

def check_data(symbol: str, data_path: str = "data/ml_training"):
  """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö."""
  start_time = time.time()
  logger.info("=" * 80)
  logger.info(f"–ù–ê–ß–ê–õ–û –ü–†–û–í–ï–†–ö–ò –î–ê–ù–ù–´–•: {symbol}")
  logger.info("=" * 80)
  print("=" * 60)
  print(f"–ü–†–û–í–ï–†–ö–ê –î–ê–ù–ù–´–•: {symbol}")
  print("=" * 60)

  logger.info(f"–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: {data_path}")

  symbol_path = Path(data_path) / symbol

  if not symbol_path.exists():
    logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {symbol_path}")
    print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {symbol_path}")
    return False

  logger.info(f"‚úì –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–∞–π–¥–µ–Ω–∞: {symbol_path}")
  features_dir = symbol_path / "features"
  labels_dir = symbol_path / "labels"

  if not features_dir.exists():
    print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è features –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è features –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {features_dir}")
    return False

  if not labels_dir.exists():
    print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è labels –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è labels –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {labels_dir}")
    return False
  logger.info("‚úì –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ features –∏ labels –Ω–∞–π–¥–µ–Ω—ã")
  # –ü–æ–¥—Å—á–µ—Ç —Ñ–∞–π–ª–æ–≤
  logger.info("–ü–æ–¥—Å—á–µ—Ç —Ñ–∞–π–ª–æ–≤...")
  files_start = time.time()

  feature_files = list(features_dir.glob("*.npy"))
  label_files = list(labels_dir.glob("*.json"))

  files_time = time.time() - files_start
  logger.info(f"‚úì –§–∞–π–ª—ã –ø–æ–¥—Å—á–∏—Ç–∞–Ω—ã –∑–∞ {files_time:.2f}s")

  print(f"\n–§–∞–π–ª—ã:")
  print(f"  Feature files: {len(feature_files)}")
  print(f"  Label files: {len(label_files)}")

  logger.info(f"Feature files: {len(feature_files)}")
  logger.info(f"Label files: {len(label_files)}")

  if len(feature_files) == 0:
    print("‚ùå –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ features!")
    logger.error("‚ùå –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ features!")
    return False

  if len(label_files) == 0:
    print("‚ùå –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ labels!")
    logger.error("‚ùå –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ labels!")
    return False

  # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤
  logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
  print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤...")
  load_start = time.time()
  try:
    # Features
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–º–µ—Ä–∞ features –∏–∑ {feature_files[0].name}...")
    sample_features = np.load(feature_files[0])
    print(f"  ‚úì Features shape: {sample_features.shape}")
    print(f"    Expected: (N, 110)")

    if sample_features.shape[1] != 110:
      logger.warning(f"‚ö†Ô∏è  –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {sample_features.shape[1]} –≤–º–µ—Å—Ç–æ 110")
      print(f"    ‚ö†Ô∏è  –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤!")

    # Labels
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–º–µ—Ä–∞ labels –∏–∑ {label_files[0].name}...")
    with open(label_files[0]) as f:
      sample_labels = json.load(f)

    print(f"  ‚úì Labels loaded: {len(sample_labels)} samples")
    logger.info(f"‚úì Labels –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(sample_labels)} —Å–µ–º–ø–ª–æ–≤")

    load_time = time.time() - load_start
    logger.info(f"‚úì –ü—Ä–∏–º–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∑–∞ {load_time:.2f}s")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã labels
    if sample_labels:
      first_label = sample_labels[0]
      print(f"\n  Label structure:")
      for key in first_label.keys():
        print(f"    - {key}")

      # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
      required = ["timestamp", "current_mid_price"]
      future_fields = ["future_direction_60s", "future_movement_60s"]

      missing_required = [f for f in required if f not in first_label]
      missing_future = [f for f in future_fields if f not in first_label]

      if missing_required:
        print(f"\n  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è: {missing_required}")
        return False

      if missing_future:
        print(f"\n  ‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç future labels: {missing_future}")
        print(f"     –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python preprocessing_add_future_labels.py")
        return False

      # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π future_direction
      future_dir = first_label.get("future_direction_60s")
      if future_dir not in [-1, 0, 1, None]:
        print(f"\n  ‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ future_direction_60s: {future_dir}")
        return False

  except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    import traceback
    traceback.print_exc()
    return False

  # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–µ–º–ø–ª–æ–≤
  logger.info("–ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–µ–º–ø–ª–æ–≤...")
  print(f"\n–ü–æ–¥—Å—á–µ—Ç —Å–µ–º–ø–ª–æ–≤...")
  count_start = time.time()

  # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Å–µ–º–ø–ª–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
  logger.info(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(feature_files)} —Ñ–∞–π–ª–æ–≤...")
  total_samples = 0

  # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è I/O –æ–ø–µ—Ä–∞—Ü–∏–π
  max_workers = min(mp.cpu_count(), len(feature_files))
  logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ {max_workers} –≤–æ—Ä–∫–µ—Ä–æ–≤ –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞")

  with ThreadPoolExecutor(max_workers=max_workers) as executor:
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç
    futures = {executor.submit(_count_samples_in_file, f): f for f in feature_files}

    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for future in as_completed(futures):
      count = future.result()
      total_samples += count

  count_time = time.time() - count_start
  logger.info(f"‚úì –ü–æ–¥—Å—á–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {count_time:.2f}s")

  print(f"  –í—Å–µ–≥–æ —Å–µ–º–ø–ª–æ–≤: {total_samples:,}")
  logger.info(f"–í—Å–µ–≥–æ —Å–µ–º–ø–ª–æ–≤: {total_samples:,}")

  # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—ä–µ–º—É
  if total_samples < 10_000:
    print(f"  ‚ùå –ö–†–ò–¢–ò–ß–ù–û: –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ({total_samples:,} < 10,000)")
    print(f"     –ú–∏–Ω–∏–º—É–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: 10,000")
    print(f"     –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: 100,000+")
    return False
  elif total_samples < 50_000:
    print(f"  ‚ö†Ô∏è  –ú–∞–ª–æ–≤–∞—Ç–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ({total_samples:,} < 50,000)")
    print(f"     –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --quick —Ä–µ–∂–∏–º")
    print(f"     –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: 100,000+")
  elif total_samples < 100_000:
    print(f"  ‚úì –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è quick optimization")
    print(f"     –î–ª—è full optimization —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: 100,000+")
  else:
    print(f"  ‚úì –û—Ç–ª–∏—á–Ω–æ! –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è full optimization")

  # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
  logger.info("–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤...")
  print(f"\n–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤...")
  analysis_start = time.time()


  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
  logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ labels –∏–∑ –ø–µ—Ä–≤—ã—Ö {min(10, len(label_files))} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞...")

  all_directions = []
  # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–æ–ª—å—à–µ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞, –Ω–æ –Ω–µ –≤—Å–µ
  sample_size = min(10, len(label_files))
  for i, label_file in enumerate(label_files[:sample_size]):
    logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {i + 1}/{sample_size}: {label_file.name}")  # –ü–µ—Ä–≤—ã–µ 3 —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
    with open(label_file) as f:
      labels = json.load(f)
      for label in labels:
        direction = label.get("future_direction_60s")
        if direction is not None:
          all_directions.append(direction)

  analysis_time = time.time() - analysis_start
  logger.info(f"‚úì –ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {analysis_time:.2f}s")

  if all_directions:
    class_dist = Counter(all_directions)
    print(f"  –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (–ø–µ—Ä–≤—ã–µ {len(all_directions)} —Å–µ–º–ø–ª–æ–≤):")

    for cls in sorted(class_dist.keys()):
      count = class_dist[cls]
      pct = (count / len(all_directions)) * 100
      cls_name = {-1: "DOWN", 0: "NEUTRAL", 1: "UP"}.get(cls, str(cls))
      print(f"    {cls_name:8} ({cls:2d}): {count:6,} ({pct:5.1f}%)")

    # Imbalance ratio
    max_count = max(class_dist.values())
    min_count = min(class_dist.values())
    ratio = max_count / min_count

    print(f"\n  Imbalance Ratio: {ratio:.2f}")

    if ratio < 1.5:
      print(f"    ‚úì –û—Ç–ª–∏—á–Ω—ã–π –±–∞–ª–∞–Ω—Å!")
    elif ratio < 3.0:
      print(f"    ‚úì –•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å")
    elif ratio < 5.0:
      print(f"    ‚ö†Ô∏è  –°—Ä–µ–¥–Ω–∏–π –¥–∏—Å–±–∞–ª–∞–Ω—Å - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Focal Loss")
    else:
      print(f"    ‚ùå –°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Focal Loss + SMOTE")
      logger.info(f"–ö–ª–∞—Å—Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {dict(class_dist)}")
      logger.info(f"Imbalance ratio: {ratio:.2f}")

    total_time = time.time() - start_time
    logger.info(f"‚úì –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.1f}s")
    logger.info("=" * 80)
  print()
  return True


def test_dataloader(symbol: str):
  """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö."""
  start_time = time.time()
  logger.info("=" * 80)
  logger.info("–ù–ê–ß–ê–õ–û –¢–ï–°–¢–ê DATALOADER")
  logger.info("=" * 80)
  print("=" * 60)
  print("–¢–ï–°–¢ DATALOADER")
  print("=" * 60)

  try:
    logger.info("–ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π...")
    from ml_engine.training.data_loader import HistoricalDataLoader, DataConfig

    print("–°–æ–∑–¥–∞–Ω–∏–µ DataLoader...")
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ DataLoader...")
    config = DataConfig(
      storage_path="data/ml_training",
      sequence_length=60,
      batch_size=32
    )

    loader = HistoricalDataLoader(config)
    print("‚úì DataLoader —Å–æ–∑–¥–∞–Ω")

    print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}...")
    X, y, timestamps = loader.load_symbol_data(symbol)

    print(f"‚úì –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:")
    print(f"  X shape: {X.shape}")
    print(f"  y shape: {y.shape}")
    print(f"  timestamps shape: {timestamps.shape}")
    print(f"  Unique labels: {set(y)}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
    has_nan = np.isnan(X).any()
    has_inf = np.isinf(X).any()

    if has_nan:
      print(f"  ‚ùå Features —Å–æ–¥–µ—Ä–∂–∞—Ç NaN!")
      return False

    if has_inf:
      print(f"  ‚ùå Features —Å–æ–¥–µ—Ä–∂–∞—Ç Inf!")
      return False

    print(f"  ‚úì –ù–µ—Ç NaN/Inf –∑–Ω–∞—á–µ–Ω–∏–π")

    # –°–æ–∑–¥–∞–Ω–∏–µ sequences
    print(f"\n–°–æ–∑–¥–∞–Ω–∏–µ sequences...")
    sequences, seq_labels, seq_timestamps = loader.create_sequences(
      X, y, timestamps
    )

    print(f"‚úì Sequences —Å–æ–∑–¥–∞–Ω—ã:")
    print(f"  Shape: {sequences.shape}")
    print(f"  Expected: (N, 60, 110)")

    print()
    return True

  except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    import traceback
    traceback.print_exc()
    return False


def test_model_creation():
  """–¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏."""
  print("=" * 60)
  print("–¢–ï–°–¢ –°–û–ó–î–ê–ù–ò–Ø –ú–û–î–ï–õ–ò")
  print("=" * 60)

  try:
    from ml_engine.models.hybrid_cnn_lstm import create_model

    print("–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = create_model()
    print("‚úì –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")

    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"  Total parameters: {total_params:,}")
    print(f"  Trainable parameters: {trainable_params:,}")

    print()
    return True

  except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    import traceback
    traceback.print_exc()
    return False


def test_minimal_training(symbol: str):
  """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è."""
  print("=" * 60)
  print("–ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô –¢–ï–°–¢ –û–ë–£–ß–ï–ù–ò–Ø (3 —ç–ø–æ—Ö–∏)")
  print("=" * 60)

  try:
    from ml_engine.training.data_loader import HistoricalDataLoader, DataConfig
    from ml_engine.training.model_trainer import ModelTrainer, TrainerConfig
    from ml_engine.models.hybrid_cnn_lstm import create_model

    print("1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    config = DataConfig(
      storage_path="data/ml_training",
      sequence_length=60,
      batch_size=32
    )

    loader = HistoricalDataLoader(config)
    result = loader.load_and_prepare(symbols=[symbol])

    print(f"‚úì Train: {len(result['dataloaders']['train'])} batches")
    print(f"‚úì Val: {len(result['dataloaders']['val'])} batches")

    print("\n2. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = create_model()
    print("‚úì –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")

    print("\n3. –û–±—É—á–µ–Ω–∏–µ (3 —ç–ø–æ—Ö–∏ –Ω–∞ CPU)...")
    trainer_config = TrainerConfig(
      epochs=3,
      learning_rate=0.001,
      device="cpu",  # CPU –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
      early_stopping_patience=10
    )

    trainer = ModelTrainer(model, trainer_config)
    history = trainer.train(
      result['dataloaders']['train'],
      result['dataloaders']['val']
    )

    print(f"\n‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(history)} —ç–ø–æ—Ö")

    if history:
      last_epoch = history[-1]
      print(f"  Final train_loss: {last_epoch.train_loss:.4f}")
      print(f"  Final val_loss: {last_epoch.val_loss:.4f}")
      print(f"  Final val_accuracy: {last_epoch.val_accuracy:.4f}")

    print()
    return True

  except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    import traceback
    traceback.print_exc()
    return False


def main():
  """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
  parser = argparse.ArgumentParser(
    description="–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"
  )
  parser.add_argument(
    "--symbol",
    required=True,
    help="–¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, BTCUSDT)"
  )
  parser.add_argument(
    "--skip-training-test",
    action="store_true",
    help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è"
  )

  args = parser.parse_args()

  print("\n" + "=" * 60)
  print("–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò")
  print("=" * 60)
  print(f"Symbol: {args.symbol}")
  print("=" * 60)

  # –ß–µ–∫–ª–∏—Å—Ç
  results = {
    "–û–∫—Ä—É–∂–µ–Ω–∏–µ": False,
    "–î–∞–Ω–Ω—ã–µ": False,
    "DataLoader": False,
    "–ú–æ–¥–µ–ª—å": False,
    "–û–±—É—á–µ–Ω–∏–µ": False
  }

  # 1. –û–∫—Ä—É–∂–µ–Ω–∏–µ
  results["–û–∫—Ä—É–∂–µ–Ω–∏–µ"] = check_environment()
  if not results["–û–∫—Ä—É–∂–µ–Ω–∏–µ"]:
    print("\n‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.")
    sys.exit(1)

  # 2. –î–∞–Ω–Ω—ã–µ
  results["–î–∞–Ω–Ω—ã–µ"] = check_data(args.symbol)
  if not results["–î–∞–Ω–Ω—ã–µ"]:
    print("\n‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏! –°–æ–±–µ—Ä–∏—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ preprocessing.")
    sys.exit(1)

  # 3. DataLoader
  results["DataLoader"] = test_dataloader(args.symbol)
  if not results["DataLoader"]:
    print("\n‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å DataLoader! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö.")
    sys.exit(1)

  # 4. –ú–æ–¥–µ–ª—å
  results["–ú–æ–¥–µ–ª—å"] = test_model_creation()
  if not results["–ú–æ–¥–µ–ª—å"]:
    print("\n‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–æ–∑–¥–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏!")
    sys.exit(1)

  # 5. –û–±—É—á–µ–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
  if not args.skip_training_test:
    results["–û–±—É—á–µ–Ω–∏–µ"] = test_minimal_training(args.symbol)
    if not results["–û–±—É—á–µ–Ω–∏–µ"]:
      print("\n‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å –æ–±—É—á–µ–Ω–∏–µ–º!")
      print("   –í–æ–∑–º–æ–∂–Ω–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –∏–ª–∏ –¥–∞–Ω–Ω—ã—Ö.")
      sys.exit(1)
  else:
    print("\n‚äò –¢–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ–ø—É—â–µ–Ω")

  # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
  print("\n" + "=" * 60)
  print("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
  print("=" * 60)

  for test_name, passed in results.items():
    status = "‚úì" if passed else "‚ùå"
    print(f"{status} {test_name}")

  if all(results.values()):
    print("\nüéâ –í–°–ï –ü–†–û–í–ï–†–ö–ò –ü–†–û–ô–î–ï–ù–´!")
    print("\n–ú–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é:")
    print(f"  python hyperparameter_optimizer.py --symbol {args.symbol} --quick")
  else:
    print("\n‚ùå –ï–°–¢–¨ –ü–†–û–ë–õ–ï–ú–´!")
    print("   –ò—Å–ø—Ä–∞–≤—å—Ç–µ –æ—à–∏–±–∫–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.")

  print("=" * 60 + "\n")


if __name__ == "__main__":
  main()