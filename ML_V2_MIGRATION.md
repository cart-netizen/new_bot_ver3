# üöÄ ML v2 Migration Complete - –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

## ‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (–æ–±–Ω–æ–≤–ª–µ–Ω–æ 2025-11-27)

–ü—Ä–æ–µ–∫—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–≤–µ–¥–µ–Ω –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ **–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö v2 –≤–µ—Ä—Å–∏–π** ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤.

### üéØ –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

**–ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –≤–µ—Ä—Å–∏—è:** `training_orchestrator.py` —Å v2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ –≤–Ω—É—Ç—Ä–∏
- ‚úÖ –í—Å–µ v2 –º–æ–¥–µ–ª–∏ –∏ trainer –∏–º–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è —á–µ—Ä–µ–∑ –∞–ª–∏–∞—Å—ã
- ‚úÖ –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º API
- ‚úÖ `TrainingOrchestratorV2` –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞
- ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å EpochMetrics (v2) –∏ dict (v1) —Ñ–æ—Ä–º–∞—Ç–æ–≤

---

## üìã –ß—Ç–æ –∏–∑–º–µ–Ω–µ–Ω–æ

### 1. –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–∏—Å–ø–æ–ª—å–∑—É—é—Ç v2)

**Backend ML Engine:**
- ‚úÖ `backend/ml_engine/training_orchestrator.py`
- ‚úÖ `backend/ml_engine/auto_retraining/retraining_pipeline.py`
- ‚úÖ `backend/ml_engine/inference/model_server.py`

**API:**
- ‚úÖ `backend/api/ml_management_api.py`

**Scripts:**
- ‚úÖ `train_model.py`

### 2. –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ v2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

**–ú–æ–¥–µ–ª—å:**
```python
from backend.ml_engine.models.hybrid_cnn_lstm_v2 import (
    HybridCNNLSTMv2 as HybridCNNLSTM,
    ModelConfigV2 as ModelConfig
)
```

**Trainer:**
```python
from backend.ml_engine.training.model_trainer_v2 import (
    ModelTrainerV2 as ModelTrainer,
    TrainerConfigV2 as TrainerConfig
)
```

### 3. –ê–ª–∏–∞—Å—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏

–í—Å–µ v2 –∫–ª–∞—Å—Å—ã –∏–º–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è —Å –∞–ª–∏–∞—Å–∞–º–∏, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º:
- `HybridCNNLSTMv2` ‚Üí `HybridCNNLSTM`
- `ModelConfigV2` ‚Üí `ModelConfig`
- `ModelTrainerV2` ‚Üí `ModelTrainer`
- `TrainerConfigV2` ‚Üí `TrainerConfig`

---

## üéØ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ v2

### ModelConfigV2 (hybrid_cnn_lstm_v2.py)

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:**
- ‚úÖ **Residual Connections** –≤ CNN –±–ª–æ–∫–∞—Ö
- ‚úÖ **Multi-Head Temporal Attention** (4 heads –≤–º–µ—Å—Ç–æ 1)
- ‚úÖ **Layer Normalization** –¥–ª—è LSTM
- ‚úÖ **GELU activation** –≤–º–µ—Å—Ç–æ ReLU
- ‚úÖ **Squeeze-and-Excitation** –±–ª–æ–∫–∏

**–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–∞–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:**
```python
cnn_channels: (32, 64, 128)     # –ë—ã–ª–æ: (64, 128, 256)
lstm_hidden: 128                # –ë—ã–ª–æ: 256
dropout: 0.4                    # –ë—ã–ª–æ: 0.3
```

### TrainerConfigV2 (model_trainer_v2.py)

**–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:**
```python
learning_rate: 5e-5             # –ë—ã–ª–æ: 0.001 ‚Üê –ö–†–ò–¢–ò–ß–ù–û!
weight_decay: 0.01              # –ë—ã–ª–æ: 1e-5 ‚Üê –ö–†–ò–¢–ò–ß–ù–û!
batch_size: 256                 # –ë—ã–ª–æ: 64 ‚Üê –ö–†–ò–¢–ò–ß–ù–û!
epochs: 150                     # –ë—ã–ª–æ: 100
```

**–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- ‚úÖ **CosineAnnealingWarmRestarts** scheduler
- ‚úÖ **Label Smoothing** (0.1)
- ‚úÖ **Gaussian Noise Augmentation** (std=0.01)
- ‚úÖ **MixUp Data Augmentation** (alpha=0.2)

### Loss Functions (training/losses.py)

- ‚úÖ **FocalLossV2** —Å gamma=2.5 (—Ñ–æ–∫—É—Å –Ω–∞ hard examples)
- ‚úÖ **LabelSmoothingCrossEntropy** (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overconfidence)
- ‚úÖ **AsymmetricFocalLoss** (—Ä–∞–∑–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è FP –∏ FN)
- ‚úÖ **DirectionalAccuracyLoss** (—à—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ)

### Data Augmentation (training/augmentation.py)

- ‚úÖ **MixUp** - —Å–º–µ—à–∏–≤–∞–Ω–∏–µ samples
- ‚úÖ **Time Masking** - –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
- ‚úÖ **Gaussian Noise** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞
- ‚úÖ **Feature Dropout** - dropout –æ—Ç–¥–µ–ª—å–Ω—ã—Ö features
- ‚úÖ **Time Warping** - –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å—Ç—è–∂–µ–Ω–∏–µ/—Å–∂–∞—Ç–∏–µ

### Class Balancing (class_balancing_v2.py)

- ‚úÖ **Adaptive Threshold Labeling** (percentile-based)
- ‚úÖ **Improved Oversampling** (ratio=0.5)
- ‚úÖ **Focal Loss enabled** –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

---

## üìä –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫

| –ú–µ—Ç—Ä–∏–∫–∞ | –ë—ã–ª–æ (v1) | –û–∂–∏–¥–∞–µ—Ç—Å—è (v2) | Industry Standard |
|---------|-----------|----------------|-------------------|
| **Accuracy** | 33.89% | **55-62%** | 55-65% |
| **F1 Score** | 32.50% | **52-58%** | 50-60% |
| **Val Loss** | 1.219 | **<0.7** | <0.6 |
| **Train-Val Gap** | 0.295 | **<0.08** | <0.1 |

---

## üîÑ –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

### –°—Ç–∞—Ä—ã–µ –º–æ–¥–µ–ª–∏ (v1)

–°—Ç–∞—Ä—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ:

```python
from backend.ml_engine.models.hybrid_cnn_lstm_v2 import load_from_v1_checkpoint

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ä–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
model_v2 = load_from_v1_checkpoint(
    checkpoint_path="checkpoints/old_model.pt",
    config_v2=ModelConfigV2()
)
```

### –ú–∏–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö

–í—Å–µ —Å—Ç–∞—Ä—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —Å–æ–≤–º–µ—Å—Ç–∏–º—ã —Å v2:
- ‚úÖ Feature Store
- ‚úÖ `.npy` —Ñ–∞–π–ª—ã
- ‚úÖ Label mapping (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π)

---

## üöÄ –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å v2

### –í–∞—Ä–∏–∞–Ω—Ç 1: –ß–µ—Ä–µ–∑ Training Orchestrator (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è, —Ç–µ–∫—É—â–∏–π —Å–ø–æ—Å–æ–±)

```python
from backend.ml_engine.training_orchestrator import TrainingOrchestrator

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç v2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤–Ω—É—Ç—Ä–∏
orchestrator = TrainingOrchestrator()
result = await orchestrator.train_model()
```

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–µ–∑–¥–µ –≤ –ø—Ä–æ–µ–∫—Ç–µ. –í–Ω—É—Ç—Ä–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç v2 –º–æ–¥–µ–ª–∏ –∏ trainer —á–µ—Ä–µ–∑ –∞–ª–∏–∞—Å—ã.

### –í–∞—Ä–∏–∞–Ω—Ç 1b: –ß–µ—Ä–µ–∑ TrainingOrchestratorV2 (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π)

```python
from backend.ml_engine.training_orchestrator import TrainingOrchestratorV2, OrchestratorConfig

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –ø—Ä–µ—Å–µ—Ç–∞–º–∏
config = OrchestratorConfig(
    model_preset="production_small",
    trainer_preset="production_small",
    feature_store_days=30,
    symbols=["BTCUSDT", "ETHUSDT"]
)

# –°–æ–∑–¥–∞–Ω–∏–µ orchestrator
orchestrator = TrainingOrchestratorV2(config)
result = await orchestrator.run_training()
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ v2 orchestrator:**
- ‚úÖ –ï–¥–∏–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ `OrchestratorConfig`
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø—Ä–µ—Å–µ—Ç–æ–≤ (production_small, production_large, quick_experiment, conservative)
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- ‚úÖ –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ Feature Store –∏ MLflow

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤.

### –í–∞—Ä–∏–∞–Ω—Ç 2: –Ø–≤–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

```python
from backend.ml_engine.models.hybrid_cnn_lstm_v2 import create_model_v2_from_preset
from backend.ml_engine.training.model_trainer_v2 import create_trainer_v2

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–µ—Å–µ—Ç–æ–º
model = create_model_v2_from_preset("production_small")

# –°–æ–∑–¥–∞–Ω–∏–µ trainer
trainer = create_trainer_v2(model, preset="production_small")

# –û–±—É—á–µ–Ω–∏–µ
history = trainer.train(train_loader, val_loader)
```

### –í–∞—Ä–∏–∞–Ω—Ç 3: –ß–µ—Ä–µ–∑ CLI

```bash
# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è v2 –≤–µ—Ä—Å–∏—è
python train_model.py --epochs 150 --batch-size 256

# –ò–ª–∏ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç
python backend/ml_optimized/scripts/run_optimized_training.py --preset production_small
```

---

## üîß –ü—Ä–µ—Å–µ—Ç—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π

### production_small (7-30 –¥–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö)

```python
learning_rate: 5e-5
batch_size: 256
weight_decay: 0.01
epochs: 150
dropout: 0.4
focal_gamma: 2.5
mixup_alpha: 0.2
label_smoothing: 0.1
```

### production_large (60+ –¥–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö)

```python
learning_rate: 1e-4
batch_size: 128
weight_decay: 0.005
epochs: 100
dropout: 0.3
focal_gamma: 2.0
```

### quick_experiment (–±—ã—Å—Ç—Ä—ã–µ —Ç–µ—Å—Ç—ã)

```python
learning_rate: 1e-4
batch_size: 128
epochs: 30
use_augmentation: False
```

### conservative (–∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è)

```python
learning_rate: 3e-5
batch_size: 256
weight_decay: 0.02
dropout: 0.5
focal_gamma: 3.0
label_smoothing: 0.15
```

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ v2 —Ñ–∞–π–ª–æ–≤

```
backend/ml_engine/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ hybrid_cnn_lstm.py          # v1 (–æ–±–Ω–æ–≤–ª–µ–Ω, –Ω–æ —É—Å—Ç–∞—Ä–µ–ª)
‚îÇ   ‚îî‚îÄ‚îÄ hybrid_cnn_lstm_v2.py       # ‚úÖ v2 (–ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø)
‚îÇ
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ model_trainer.py            # v1 (–æ–±–Ω–æ–≤–ª–µ–Ω, –Ω–æ —É—Å—Ç–∞—Ä–µ–ª)
‚îÇ   ‚îú‚îÄ‚îÄ model_trainer_v2.py         # ‚úÖ v2 (–ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø)
‚îÇ   ‚îú‚îÄ‚îÄ class_balancing.py          # v1 (–æ–±–Ω–æ–≤–ª–µ–Ω, –Ω–æ —É—Å—Ç–∞—Ä–µ–ª)
‚îÇ   ‚îú‚îÄ‚îÄ class_balancing_v2.py       # ‚úÖ v2 (–ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø)
‚îÇ   ‚îú‚îÄ‚îÄ losses.py                   # ‚úÖ v2 (–ù–û–í–û–ï)
‚îÇ   ‚îî‚îÄ‚îÄ augmentation.py             # ‚úÖ v2 (–ù–û–í–û–ï)
‚îÇ
‚îú‚îÄ‚îÄ inference/
‚îÇ   ‚îú‚îÄ‚îÄ model_server.py             # –û–±–Ω–æ–≤–ª–µ–Ω –¥–ª—è v2
‚îÇ   ‚îî‚îÄ‚îÄ model_server_v2.py          # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è
‚îÇ
‚îî‚îÄ‚îÄ training_orchestrator_v2.py     # ‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π orchestrator
```

---

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

### 1. GPU Memory

v2 –º–æ–¥–µ–ª—å —Å batch_size=256 —Ç—Ä–µ–±—É–µ—Ç ~4GB GPU –ø–∞–º—è—Ç–∏. –ù–∞ CPU –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω—ã–º.

**–†–µ—à–µ–Ω–∏–µ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏:**
```python
config = TrainerConfigV2(
    batch_size=128,  # –£–º–µ–Ω—å—à–∏—Ç—å
    gradient_accumulation_steps=2  # –ö–æ–º–ø–µ–Ω—Å–∏—Ä–æ–≤–∞—Ç—å
)
```

### 2. Compatibility —Å Production

–í—Å–µ endpoints –∏ API —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä—è –∞–ª–∏–∞—Å–∞–º.

### 3. Monitoring

v2 trainer –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ª–æ–≥–∏—Ä—É–µ—Ç –≤—Å–µ –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ MLflow:
- Learning rate –ø–æ —ç–ø–æ—Ö–∞–º
- Augmentation parameters
- Loss components (direction, confidence, return)

---

## üêõ Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: ImportError –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ v2

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ —Ñ–∞–π–ª—ã v2 —Å—É—â–µ—Å—Ç–≤—É—é—Ç
ls backend/ml_engine/models/hybrid_cnn_lstm_v2.py
ls backend/ml_engine/training/model_trainer_v2.py
```

### –ü—Ä–æ–±–ª–µ–º–∞: Out of Memory

**–†–µ—à–µ–Ω–∏–µ:**
```python
# –£–º–µ–Ω—å—à–∏—Ç—å batch_size
config.batch_size = 128

# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gradient accumulation
config.gradient_accumulation_steps = 2
```

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ —É–ª—É—á—à–∞—é—Ç—Å—è

**–†–µ—à–µ–Ω–∏–µ:**
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ learning rate (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 5e-5, –Ω–µ 0.001!)
2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ batch_size (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å ‚â•128)
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ NaN/Inf

---

## üìû –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–ü—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º:
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è
2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è v2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º

---

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞
**–î–∞—Ç–∞:** 2025-01-27
**–í–µ—Ä—Å–∏—è:** v2.0
