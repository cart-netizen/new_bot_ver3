# üîç NaN Loss Troubleshooting Guide

## –ü—Ä–æ–±–ª–µ–º–∞
```
Train Epoch 1:   0%|          | 1/859 [00:07<1:46:25,  7.44s/batch, loss=nan]
```

Loss —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è NaN –Ω–∞ –ø–µ—Ä–≤–æ–º (–∏–ª–∏ —Ä–∞–Ω–Ω–∏—Ö) –±–∞—Ç—á–∞—Ö –æ–±—É—á–µ–Ω–∏—è.

---

## ‚úÖ –ü—Ä–∏–º–µ–Ω—ë–Ω–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è (v1.0)

### 1. **–û—Ç–∫–ª—é—á–µ–Ω Mixed Precision** (–í–†–ï–ú–ï–ù–ù–û)
```python
# model_trainer_v2.py:123
use_mixed_precision: bool = False  # –ë—ã–ª–æ True
```

**–ü—Ä–∏—á–∏–Ω–∞**: Mixed precision (FP16) + gradient accumulation –º–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ

**–ö–æ–≥–¥–∞ –≤–∫–ª—é—á–∞—Ç—å –æ–±—Ä–∞—Ç–Ω–æ**:
- –ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ mixed precision
- –ö–æ–≥–¥–∞ –±—É–¥–µ—Ç –Ω–∞–π–¥–µ–Ω–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ root cause
- –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ, —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º loss

### 2. **–î–æ–±–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf**
```python
# –í _train_epoch():
if torch.isnan(loss) or torch.isinf(loss):
    logger.warning(f"NaN/Inf loss detected at batch {batch_idx}! Skipping batch.")
    continue
```

**–≠—Ñ—Ñ–µ–∫—Ç**: –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç "–ø–ª–æ—Ö–∏–µ" –±–∞—Ç—á–∏ –≤–º–µ—Å—Ç–æ –∫—Ä–∞—à–∞ –æ–±—É—á–µ–Ω–∏—è

### 3. **–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤**
```python
grad_norm = torch.nn.utils.clip_grad_norm_(...)
if torch.isnan(grad_norm) or torch.isinf(grad_norm):
    logger.warning(f"NaN/Inf gradient detected! Skipping optimizer step.")
    self.optimizer.zero_grad()
    continue
```

**–≠—Ñ—Ñ–µ–∫—Ç**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç update –≤–µ—Å–æ–≤ —Å NaN –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏

### 4. **–£–ª—É—á—à–µ–Ω GradScaler** (–∫–æ–≥–¥–∞ mixed precision –≤–∫–ª—é—á–µ–Ω)
```python
GradScaler(
    init_scale=2.**10,      # –ú–µ–Ω—å—à–∏–π –Ω–∞—á–∞–ª—å–Ω—ã–π scale (–±—ã–ª–æ 2^16)
    growth_interval=1000    # –†–µ–∂–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º scale
)
```

**–≠—Ñ—Ñ–µ–∫—Ç**: –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤

---

## üîç –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã NaN loss

### 1. Mixed Precision Overflow ‚ö°
**–°–∏–º–ø—Ç–æ–º—ã**: NaN –Ω–∞ –ø–µ—Ä–≤—ã—Ö –±–∞—Ç—á–∞—Ö —Å mixed precision

**–†–µ—à–µ–Ω–∏–µ**:
```python
use_mixed_precision=False  # ‚úÖ –£–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–æ
```

**–î–µ—Ç–∞–ª–∏**: FP16 –∏–º–µ–µ—Ç –º–µ–Ω—å—à–∏–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω —á–µ–º FP32. –ë–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è ‚Üí overflow ‚Üí NaN

---

### 2. –ù–µ–≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ üìä
**–°–∏–º–ø—Ç–æ–º—ã**: NaN –≤ features –∏–ª–∏ labels

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
```python
# –í DataLoader –¥–æ–±–∞–≤–∏—Ç—å:
print(f"Features NaN: {torch.isnan(sequences).any()}")
print(f"Features Inf: {torch.isinf(sequences).any()}")
print(f"Features min/max: {sequences.min()}, {sequences.max()}")
```

**–†–µ—à–µ–Ω–∏–µ**:
```python
# –í data preprocessing:
X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
```

---

### 3. –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π Learning Rate üìà
**–°–∏–º–ø—Ç–æ–º—ã**: Loss —Ä–∞—Å—Ç—ë—Ç –∏–ª–∏ —Å—Ä–∞–∑—É NaN

**–¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ**: `5e-5` (–æ—á–µ–Ω—å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ)

**–ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞**: –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å `1e-5`

---

### 4. Focal Loss + Class Imbalance üéØ
**–°–∏–º–ø—Ç–æ–º—ã**: NaN –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
```python
# –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å focal loss:
use_focal_loss=False
focal_gamma=0.0
```

**–ï—Å–ª–∏ –ø–æ–º–æ–≥–ª–æ**: –£–º–µ–Ω—å—à–∏—Ç—å gamma
```python
focal_gamma=1.0  # –ë—ã–ª–æ 2.5
```

---

### 5. Gradient Explosion üí•
**–°–∏–º–ø—Ç–æ–º—ã**: –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º–∏

**–¢–µ–∫—É—â–∞—è –∑–∞—â–∏—Ç–∞**:
```python
grad_clip_value=1.0  # –£–∂–µ –≤–∫–ª—é—á–µ–Ω–æ
```

**–ï—Å–ª–∏ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç**:
```python
grad_clip_value=0.5  # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π clipping
```

---

### 6. Batch Normalization Issues
**–°–∏–º–ø—Ç–æ–º—ã**: NaN –ø—Ä–∏ –º–∞–ª–µ–Ω—å–∫–æ–º batch size

**–¢–µ–∫—É—â–∏–π batch**: 128 (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –û–ö)

**–ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞**: –£–≤–µ–ª–∏—á–∏—Ç—å –¥–æ 256 –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Layer Norm

---

### 7. LSTM Hidden State Overflow
**–°–∏–º–ø—Ç–æ–º—ã**: NaN –≤ LSTM forward pass

**–†–µ—à–µ–Ω–∏–µ**: –£–∂–µ –µ—Å—Ç—å –≤ –º–æ–¥–µ–ª–∏ - orthogonal initialization

---

## üõ†Ô∏è –ü–æ—à–∞–≥–æ–≤–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

### –®–∞–≥ 1: –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –æ—Ç–∫–ª—é—á–µ–Ω–Ω—ã–º mixed precision
```bash
# –£–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–æ –≤ –∫–æ–¥–µ
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: Loss –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º (–Ω–µ NaN)

**–ï—Å–ª–∏ –≤—Å—ë –µ—â—ë NaN** ‚Üí –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —à–∞–≥—É 2

---

### –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
```python
# –í training_orchestrator.py –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö:
for batch in train_loader:
    sequences = batch['sequence']
    labels = batch['label']

    print(f"Sequences shape: {sequences.shape}")
    print(f"Sequences NaN: {torch.isnan(sequences).sum()}")
    print(f"Sequences Inf: {torch.isinf(sequences).sum()}")
    print(f"Sequences range: [{sequences.min():.4f}, {sequences.max():.4f}]")
    print(f"Labels: {labels.unique()}")
    break
```

**–ï—Å–ª–∏ –µ—Å—Ç—å NaN/Inf** ‚Üí –ø—Ä–æ–±–ª–µ–º–∞ –≤ preprocessing

---

### –®–∞–≥ 3: –£–ø—Ä–æ—Å—Ç–∏—Ç—å loss function
```python
# –í—Ä–µ–º–µ–Ω–Ω–æ:
use_focal_loss=False
use_class_weights=False
label_smoothing=0.0
```

**–ï—Å–ª–∏ –ø–æ–º–æ–≥–ª–æ** ‚Üí –ø—Ä–æ–±–ª–µ–º–∞ –≤ loss function –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö

---

### –®–∞–≥ 4: –£–º–µ–Ω—å—à–∏—Ç—å learning rate
```python
learning_rate=1e-5  # –ë—ã–ª–æ 5e-5
```

---

### –®–∞–≥ 5: –û—Ç–∫–ª—é—á–∏—Ç—å augmentation
```python
use_augmentation=False
mixup_alpha=0.0
```

**–ï—Å–ª–∏ –ø–æ–º–æ–≥–ª–æ** ‚Üí –ø—Ä–æ–±–ª–µ–º–∞ –≤ MixUp –∏–ª–∏ augmentation

---

### –®–∞–≥ 6: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å model forward pass
```python
# –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏:
model = create_model()
x = torch.randn(2, 60, 110)  # batch=2, seq=60, features=110
output = model(x)

print(f"Output NaN: {torch.isnan(output['direction_logits']).any()}")
print(f"Output range: {output['direction_logits'].min()}, {output['direction_logits'].max()}")
```

---

## üìã Checklist

–ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å:

- [ ] Loss - —á–∏—Å–ª–æ (–Ω–µ NaN)
- [ ] Gradients finite (–≤ –ª–æ–≥–∞—Ö –Ω–µ—Ç "NaN/Inf gradient detected")
- [ ] Accuracy —Ä–∞—Å—Ç—ë—Ç (—Ö–æ—Ç—è –±—ã –Ω–µ–º–Ω–æ–≥–æ)
- [ ] GPU memory stable (~7-8 GB)

---

## üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### –° –æ—Ç–∫–ª—é—á–µ–Ω–Ω—ã–º mixed precision:
```
Train Epoch 1:   1%|  | 10/859 [00:15<21:12, 1.50s/batch, loss=0.9234]  ‚úÖ
Train Epoch 1:   5%|‚ñå | 50/859 [01:15<20:45, 1.54s/batch, loss=0.8761]  ‚úÖ
```

### –ï—Å–ª–∏ NaN –≤—Å—ë –µ—â—ë –ø–æ—è–≤–ª—è–µ—Ç—Å—è:
```
‚ö†Ô∏è NaN/Inf loss detected at batch 42! Skipping batch.
Train Epoch 1:   5%|‚ñå | 50/859 [01:15<20:45, 1.54s/batch, loss=0.8761]  ‚ö†Ô∏è
```
‚Üí –°–º–æ—Ç—Ä–µ—Ç—å –ª–æ–≥–∏, –∫–∞–∫–∏–µ –±–∞—Ç—á–∏ –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç—Å—è
‚Üí –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —ç—Ç–∏ –±–∞—Ç—á–∏ –æ—Ç–¥–µ–ª—å–Ω–æ

---

## üîÑ –ü–ª–∞–Ω re-enable mixed precision

–ö–æ–≥–¥–∞ –æ–±—É—á–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ mixed precision:

### 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- –û–±—É—á–µ–Ω–∏–µ >10 —ç–ø–æ—Ö –±–µ–∑ NaN
- Val loss –ø–ª–∞–≤–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è
- –ù–µ—Ç warnings "NaN detected"

### 2. Re-enable —Å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
```python
use_mixed_precision=True
# GradScaler —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ:
# init_scale=2.**10, growth_interval=1000
```

### 3. –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –ø–µ—Ä–≤—ã–µ 100 –±–∞—Ç—á–µ–π
```
- –ï—Å–ª–∏ NaN ‚Üí –≤–µ—Ä–Ω—É—Ç—å False
- –ï—Å–ª–∏ OK ‚Üí –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ
```

### 4. –ï—Å–ª–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç
```
# –ú–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π scaling:
init_scale=2.**12  # –ë—ã–ª–æ 2.**10
```

---

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [PyTorch AMP Troubleshooting](https://pytorch.org/docs/stable/notes/amp_examples.html)
- [Debugging NaN in Neural Networks](https://github.com/pytorch/pytorch/issues/12633)
- [Focal Loss Numerical Stability](https://arxiv.org/abs/1708.02002)

---

–°–æ–∑–¥–∞–Ω–æ: 2025-11-27
–°—Ç–∞—Ç—É—Å: ‚úÖ Mixed precision –û–¢–ö–õ–Æ–ß–ï–ù
NaN detection: ‚úÖ –ê–ö–¢–ò–í–ù–ê
Next: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ mixed precision
