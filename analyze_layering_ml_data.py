#!/usr/bin/env python3
"""
Analyze Layering ML Training Data

Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° ÑÐ¾Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Layering ML:
- ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ ÑÐ±Ð¾Ñ€Ð°
- ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ quality Ð´Ð°Ð½Ð½Ñ‹Ñ…
- ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ distribution Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
- ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚ÑŒ Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÑŽ

Usage:
    python analyze_layering_ml_data.py
"""

import sys
from pathlib import Path

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent / "backend"))

try:
    import pandas as pd
    import numpy as np
    from backend.ml_engine.detection.layering_data_collector import LayeringDataCollector
    from backend.core.logger import get_logger
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("\nÐ£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚Ðµ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸:")
    print("  pip install pandas pyarrow scikit-learn")
    sys.exit(1)

logger = get_logger(__name__)


def main():
    print("=" * 80)
    print("ðŸ” LAYERING ML DATA ANALYSIS")
    print("=" * 80)
    print()

    # Initialize collector
    data_dir = "data/ml_training/layering"
    collector = LayeringDataCollector(
        data_dir=data_dir,
        enabled=True
    )

    # Get statistics
    stats = collector.get_statistics()

    print("ðŸ“Š COLLECTION STATISTICS")
    print("-" * 80)
    print(f"Status:              {'âœ… Enabled' if stats['enabled'] else 'âŒ Disabled'}")
    print(f"Total collected:     {stats['total_collected']:,}")
    print(f"Total saved:         {stats['total_saved']:,}")
    print(f"Buffer size:         {stats['buffer_size']:,}")
    print(f"Files on disk:       {stats['files_on_disk']:,}")
    print(f"Total on disk:       {stats['total_on_disk']:,}")
    print()

    # Check if we have data
    if stats['total_on_disk'] == 0:
        print("âš ï¸  NO DATA COLLECTED YET")
        print()
        print("Ð”Ð»Ñ Ð½Ð°Ñ‡Ð°Ð»Ð° ÑÐ±Ð¾Ñ€Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…:")
        print("1. Ð£Ð±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ Ñ‡Ñ‚Ð¾ Ð² config/.env ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾:")
        print("   TRADING_MODE=ONLY_TRAINING  (Ð¸Ð»Ð¸ FULL)")
        print()
        print("2. Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ Ð±Ð¾Ñ‚Ð°:")
        print("   python backend/main.py")
        print()
        print("3. ÐŸÐ¾Ð´Ð¾Ð¶Ð´Ð¸Ñ‚Ðµ Ð¿Ð¾ÐºÐ° detector Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ñ‚ layering Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹")
        print("   (Ð¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð½ÑÑ‚ÑŒ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ‡Ð°ÑÐ¾Ð²/Ð´Ð½ÐµÐ¹ Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ñ€Ñ‹Ð½Ð¾Ñ‡Ð½Ð¾Ð¹ Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸)")
        print()
        return

    # Load all data
    print("ðŸ“š Loading data from disk...")
    df = collector.load_all_data()

    if df.empty:
        print("âŒ No data loaded")
        return

    print(f"âœ… Loaded {len(df):,} samples")
    print()

    # Labeling statistics
    print("ðŸ·ï¸  LABELING STATISTICS")
    print("-" * 80)
    print(f"Total samples:       {len(df):,}")
    print(f"Labeled samples:     {stats['labeled_on_disk']:,} ({stats['labeling_rate']:.1%})")
    print(f"Unlabeled samples:   {len(df) - stats['labeled_on_disk']:,}")
    print()

    if stats['labeled_on_disk'] > 0:
        print(f"True Positives:      {stats['true_positives']:,}")
        print(f"False Positives:     {stats['false_positives']:,}")

        if stats['true_positives'] + stats['false_positives'] > 0:
            tp_rate = stats['true_positives'] / (stats['true_positives'] + stats['false_positives'])
            print(f"True Positive Rate:  {tp_rate:.1%}")
        print()

    # Feature statistics
    print("ðŸ“ˆ FEATURE STATISTICS (Top 10)")
    print("-" * 80)

    key_features = [
        'total_volume_btc',
        'total_volume_usdt',
        'placement_duration',
        'cancellation_rate',
        'spoofing_execution_ratio',
        'layer_count',
        'detector_confidence',
        'volatility_24h',
        'liquidity_score',
        'hour_utc'
    ]

    for feature in key_features:
        if feature in df.columns:
            values = df[feature].dropna()
            if len(values) > 0:
                mean_val = values.mean()
                std_val = values.std()
                min_val = values.min()
                max_val = values.max()

                print(f"{feature:30s}: "
                      f"mean={mean_val:8.2f}, "
                      f"std={std_val:8.2f}, "
                      f"min={min_val:8.2f}, "
                      f"max={max_val:8.2f}")
    print()

    # Symbol distribution
    print("ðŸŽ¯ SYMBOL DISTRIBUTION")
    print("-" * 80)
    symbol_counts = df['symbol'].value_counts()
    for symbol, count in symbol_counts.head(10).items():
        print(f"{symbol:12s}: {count:5d} samples ({count/len(df)*100:5.1f}%)")
    print()

    # Confidence distribution
    print("ðŸŽ² DETECTOR CONFIDENCE DISTRIBUTION")
    print("-" * 80)
    confidence_bins = pd.cut(df['detector_confidence'], bins=[0, 0.5, 0.65, 0.75, 0.85, 1.0])
    conf_dist = confidence_bins.value_counts().sort_index()
    for bin_range, count in conf_dist.items():
        print(f"{str(bin_range):20s}: {count:5d} samples ({count/len(df)*100:5.1f}%)")
    print()

    # Market regime distribution
    if 'market_regime' in df.columns:
        print("ðŸŒ MARKET REGIME DISTRIBUTION")
        print("-" * 80)
        regime_counts = df['market_regime'].value_counts()
        for regime, count in regime_counts.items():
            print(f"{regime:12s}: {count:5d} samples ({count/len(df)*100:5.1f}%)")
        print()

    # Time distribution
    if 'hour_utc' in df.columns:
        print("â° TIME DISTRIBUTION (UTC Hours)")
        print("-" * 80)
        hour_counts = df['hour_utc'].value_counts().sort_index()

        # Group by 4-hour blocks for readability
        for start_hour in range(0, 24, 4):
            end_hour = start_hour + 3
            block_count = hour_counts[(hour_counts.index >= start_hour) & (hour_counts.index <= end_hour)].sum()
            print(f"{start_hour:02d}:00 - {end_hour:02d}:59: {block_count:5d} samples ({block_count/len(df)*100:5.1f}%)")
        print()

    # Ready for training?
    print("ðŸŽ“ TRAINING READINESS")
    print("-" * 80)

    labeled_df = collector.get_labeled_data()

    if len(labeled_df) < 10:
        print("âŒ NOT READY - Insufficient labeled data")
        print(f"   Current: {len(labeled_df)} labeled samples")
        print(f"   Minimum: 10 labeled samples")
        print(f"   Recommended: 100+ labeled samples")
        print()
        print("Ð§Ñ‚Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ:")
        print("1. Ð ÐµÐ°Ð»Ð¸Ð·ÑƒÐ¹Ñ‚Ðµ automatic labeling (price action validation)")
        print("2. Ð˜Ð»Ð¸ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÑŒÑ‚Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ:")
        print("   collector.update_label(data_id, label=True/False)")
    elif len(labeled_df) < 100:
        print("âš ï¸  MARGINAL - ÐœÐ¾Ð¶Ð½Ð¾ Ð¾Ð±ÑƒÑ‡Ð°Ñ‚ÑŒ, Ð½Ð¾ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð±ÑƒÐ´ÐµÑ‚ Ð½Ð¸Ð·ÐºÐ¸Ð¼")
        print(f"   Current: {len(labeled_df)} labeled samples")
        print(f"   Recommended: 100+ labeled samples")
    else:
        print("âœ… READY FOR TRAINING!")
        print(f"   Labeled samples: {len(labeled_df)}")

        # Check class balance
        true_count = labeled_df['is_true_layering'].sum()
        false_count = len(labeled_df) - true_count

        print(f"   True Positives:  {true_count} ({true_count/len(labeled_df)*100:.1f}%)")
        print(f"   False Positives: {false_count} ({false_count/len(labeled_df)*100:.1f}%)")

        # Check balance
        min_class = min(true_count, false_count)
        max_class = max(true_count, false_count)
        balance_ratio = min_class / max_class if max_class > 0 else 0

        if balance_ratio < 0.3:
            print(f"   âš ï¸  UNBALANCED dataset (ratio: {balance_ratio:.2f})")
            print("      Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ Ð±Ð°Ð»Ð°Ð½Ñ 40/60 - 60/40")
        else:
            print(f"   âœ… Balanced dataset (ratio: {balance_ratio:.2f})")

        print()
        print("Ð”Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ:")
        print("  python backend/scripts/train_layering_model.py")

    print()
    print("=" * 80)
    print("âœ… ANALYSIS COMPLETE")
    print("=" * 80)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
