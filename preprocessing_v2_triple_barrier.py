#!/usr/bin/env python3
"""
Preprocessing V2 - Triple Barrier Labeling —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.

–ò–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å V1:
1. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Triple Barrier Method (ATR-based) –≤–º–µ—Å—Ç–æ fixed threshold
2. –î–æ–±–∞–≤–ª–µ–Ω 5-–º–∏–Ω—É—Ç–Ω—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç (300s)
3. –ë–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è HOLD –∫–ª–∞—Å—Å–∞
4. –î–æ–±–∞–≤–ª–µ–Ω—ã lagged —Ñ–∏—á–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π —Å–∏–ª—ã
5. –î–æ–±–∞–≤–ª–µ–Ω—ã derived —Ñ–∏—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ top-–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π

–ó–∞–ø—É—Å–∫:
    python preprocessing_v2_triple_barrier.py --start-date 2025-11-01

–§–∞–π–ª: preprocessing_v2_triple_barrier.py
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass

# –î–æ–±–∞–≤–ª—è–µ–º backend –≤ path
sys.path.insert(0, str(Path(__file__).parent))

PROJECT_ROOT = Path(__file__).resolve().parent

from backend.core.logger import get_logger
from backend.ml_engine.feature_store.feature_store import get_feature_store

logger = get_logger(__name__)


# =============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø - –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
# =============================================================================

@dataclass
class LabelingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Triple Barrier –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤."""

    # –ì–æ—Ä–∏–∑–æ–Ω—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
    horizons: List[int] = None

    # ATR –º–Ω–æ–∂–∏—Ç–µ–ª–∏ –¥–ª—è —à–∏—Ä–∏–Ω—ã –±–∞—Ä—å–µ—Ä–æ–≤
    # –ß–µ–º –≤—ã—à–µ - —Ç–µ–º —à–∏—Ä–µ –±–∞—Ä—å–µ—Ä—ã, —Ç–µ–º –º–µ–Ω—å—à–µ HOLD
    tp_multiplier: float = 2.0   # Take Profit = entry + tp_mult * ATR
    sl_multiplier: float = 2.0   # Stop Loss = entry - sl_mult * ATR

    # Fallback threshold –µ—Å–ª–∏ ATR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–≤ % –æ—Ç —Ü–µ–Ω—ã)
    fixed_threshold_pct: float = 0.15  # 0.15% –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç

    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è non-HOLD (–≤ %)
    # –ï—Å–ª–∏ –¥–≤–∏–∂–µ–Ω–∏–µ < min_movement_pct -> HOLD
    min_movement_pct: float = 0.05  # 0.05%

    def __post_init__(self):
        if self.horizons is None:
            # –ì–æ—Ä–∏–∑–æ–Ω—Ç—ã: 1 –º–∏–Ω, 3 –º–∏–Ω, 5 –º–∏–Ω
            self.horizons = [60, 180, 300]


# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è HOLD
DEFAULT_CONFIG = LabelingConfig(
    horizons=[60, 180, 300],  # 1, 3, 5 –º–∏–Ω—É—Ç
    tp_multiplier=2.0,
    sl_multiplier=2.0,
    fixed_threshold_pct=0.15,
    min_movement_pct=0.05
)


# =============================================================================
# LAGGED FEATURES - –õ–∞–≥–æ–≤—ã–µ —Ñ–∏—á–∏
# =============================================================================

# –§–∏—á–∏ —Å –Ω–∞–∏–≤—ã—Å—à–µ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π (–∏–∑ feature_quality_analyzer)
TOP_CORRELATED_FEATURES = [
    'imbalance_5',
    'depth_imbalance_ratio',
    'imbalance_10',
    'volume_delta_5',
    'gap_size',
    'momentum_10',
    'update_frequency',
    'quote_intensity',
    'rsi_28',
    'roc',
]

# –§–∏—á–∏ —Å –Ω–∞–∏–≤—ã—Å—à–∏–º Fisher Ratio (–ª—É—á—à–∞—è —Å–µ–ø–∞—Ä–∞–±–µ–ª—å–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤)
TOP_SEPARATING_FEATURES = [
    'orderbook_volatility',
    'bid_ask_spread_rel',
    'effective_spread',
    'trade_arrival_rate',
    'smart_money_index',
]

# –õ–∞–≥–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è lagged features (–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ samples)
# –ü—Ä–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ~15s: lag=4 ‚âà 1 –º–∏–Ω—É—Ç–∞ –Ω–∞–∑–∞–¥
LAGS = [1, 2, 4, 8]  # ~15s, 30s, 1min, 2min –Ω–∞–∑–∞–¥


def add_lagged_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç lagged –≤–µ—Ä—Å–∏–∏ top-–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∏—á.

    –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ –≤–∏–¥–µ—Ç—å –¥–∏–Ω–∞–º–∏–∫—É –∏–∑–º–µ–Ω–µ–Ω–∏–π.

    Args:
        df: DataFrame —Å —Ñ–∏—á–∞–º–∏

    Returns:
        DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ lagged —Ñ–∏—á–∞–º–∏
    """
    print("\nüìä –î–æ–±–∞–≤–ª–µ–Ω–∏–µ Lagged Features...")

    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å–ø–∏—Å–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∏—á –¥–ª—è –ª–∞–≥–≥–∏–Ω–≥–∞
    features_to_lag = list(set(TOP_CORRELATED_FEATURES + TOP_SEPARATING_FEATURES))

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ —Ñ–∏—á–∏ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
    available_features = [f for f in features_to_lag if f in df.columns]
    print(f"   –§–∏—á–∏ –¥–ª—è –ª–∞–≥–≥–∏–Ω–≥–∞: {len(available_features)}/{len(features_to_lag)}")

    new_columns = {}

    for feature in available_features:
        for lag in LAGS:
            col_name = f"{feature}_lag{lag}"
            new_columns[col_name] = df[feature].shift(lag)

    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –Ω–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∑–∞ —Ä–∞–∑ (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ)
    if new_columns:
        new_df = pd.DataFrame(new_columns, index=df.index)
        df = pd.concat([df, new_df], axis=1)

    n_new_features = len(new_columns)
    print(f"   ‚úì –î–æ–±–∞–≤–ª–µ–Ω–æ {n_new_features} lagged features")

    return df


def add_derived_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ —Ñ–∏—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ top-–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π.

    –°–æ–∑–¥–∞–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ñ–∏—á, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –∏–º–µ—Ç—å –ª—É—á—à—É—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—É—é —Å–∏–ª—É.

    Args:
        df: DataFrame —Å —Ñ–∏—á–∞–º–∏

    Returns:
        DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ derived —Ñ–∏—á–∞–º–∏
    """
    print("\nüîß –î–æ–±–∞–≤–ª–µ–Ω–∏–µ Derived Features...")

    derived = {}

    # 1. –ò–∑–º–µ–Ω–µ–Ω–∏—è imbalance (momentum of imbalance)
    if 'imbalance_5' in df.columns:
        derived['imbalance_5_change'] = df['imbalance_5'].diff()
        derived['imbalance_5_change_pct'] = df['imbalance_5'].pct_change()

        if 'imbalance_5_lag4' in df.columns:
            # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞ 1 –º–∏–Ω—É—Ç—É
            derived['imbalance_5_momentum'] = df['imbalance_5'] - df['imbalance_5_lag4']

    # 2. Ratio —Ñ–∏—á–∏
    if 'imbalance_5' in df.columns and 'imbalance_10' in df.columns:
        # –û—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –∫ –¥–ª–∏–Ω–Ω–æ–º—É imbalance
        derived['imbalance_ratio_5_10'] = df['imbalance_5'] / (df['imbalance_10'] + 1e-10)

    # 3. Volatility-adjusted imbalance
    if 'imbalance_5' in df.columns and 'orderbook_volatility' in df.columns:
        derived['imbalance_vol_adjusted'] = df['imbalance_5'] / (df['orderbook_volatility'] + 1e-10)

    # 4. Spread momentum
    if 'bid_ask_spread_rel' in df.columns:
        derived['spread_change'] = df['bid_ask_spread_rel'].diff()
        derived['spread_momentum'] = df['bid_ask_spread_rel'].diff().rolling(4).mean()

    # 5. Volume-weighted imbalance
    if 'imbalance_5' in df.columns and 'volume' in df.columns:
        derived['imbalance_volume_weighted'] = df['imbalance_5'] * np.log1p(df['volume'])

    # 6. RSI momentum
    if 'rsi_14' in df.columns and 'rsi_28' in df.columns:
        derived['rsi_diff'] = df['rsi_14'] - df['rsi_28']
        derived['rsi_14_momentum'] = df['rsi_14'].diff(4)

    # 7. Composite signals
    if all(f in df.columns for f in ['imbalance_5', 'depth_imbalance_ratio', 'volume_delta_5']):
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª –∏–∑ top-3 –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á
        derived['composite_signal'] = (
            df['imbalance_5'].rank(pct=True) * 0.4 +
            df['depth_imbalance_ratio'].rank(pct=True) * 0.4 +
            df['volume_delta_5'].rank(pct=True) * 0.2
        )

    # 8. Orderbook pressure change
    if 'smart_money_index' in df.columns:
        derived['smart_money_momentum'] = df['smart_money_index'].diff()
        derived['smart_money_acceleration'] = df['smart_money_index'].diff().diff()

    # 9. Trade intensity ratio
    if 'trade_arrival_rate' in df.columns and 'quote_intensity' in df.columns:
        derived['trade_quote_ratio'] = df['trade_arrival_rate'] / (df['quote_intensity'] + 1e-10)

    # 10. Volatility regime
    if 'orderbook_volatility' in df.columns:
        vol_ma = df['orderbook_volatility'].rolling(20).mean()
        derived['volatility_regime'] = df['orderbook_volatility'] / (vol_ma + 1e-10)

    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ —Ñ–∏—á–∏
    if derived:
        derived_df = pd.DataFrame(derived, index=df.index)
        df = pd.concat([df, derived_df], axis=1)

    print(f"   ‚úì –î–æ–±–∞–≤–ª–µ–Ω–æ {len(derived)} derived features")

    return df


# =============================================================================
# TRIPLE BARRIER LABELING
# =============================================================================

def compute_atr(df: pd.DataFrame, period: int = 14) -> pd.Series:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç Average True Range –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –±–∞—Ä—å–µ—Ä–æ–≤.

    Args:
        df: DataFrame —Å OHLC –¥–∞–Ω–Ω—ã–º–∏
        period: –ü–µ—Ä–∏–æ–¥ ATR

    Returns:
        Series —Å ATR –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    """
    if 'high' not in df.columns or 'low' not in df.columns:
        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º close –∫–∞–∫ proxy –¥–ª—è high/low
        if 'close' in df.columns:
            return df['close'] * 0.002  # 0.2% –æ—Ç —Ü–µ–Ω—ã –∫–∞–∫ proxy ATR
        elif 'current_mid_price' in df.columns:
            return df['current_mid_price'] * 0.002
        return None

    high = df['high']
    low = df['low']
    close = df['close'].shift(1) if 'close' in df.columns else df['current_mid_price'].shift(1)

    tr1 = high - low
    tr2 = (high - close).abs()
    tr3 = (low - close).abs()

    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr = true_range.rolling(window=period).mean()

    return atr


def apply_triple_barrier_label(
    current_price: float,
    future_price: float,
    atr: float,
    config: LabelingConfig
) -> Tuple[int, float]:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç Triple Barrier –ª–æ–≥–∏–∫—É –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è label.

    Args:
        current_price: –¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞
        future_price: –ë—É–¥—É—â–∞—è —Ü–µ–Ω–∞
        atr: Average True Range –Ω–∞ –º–æ–º–µ–Ω—Ç –≤—Ö–æ–¥–∞
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è labeling

    Returns:
        (label, movement) –≥–¥–µ label: 0=SELL, 1=HOLD, 2=BUY
    """
    if current_price <= 0:
        return 1, 0.0

    movement = (future_price - current_price) / current_price
    movement_pct = abs(movement) * 100

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ ATR –∏–ª–∏ fixed threshold
    if atr is not None and atr > 0 and not np.isnan(atr):
        threshold = (atr / current_price) * config.tp_multiplier
    else:
        threshold = config.fixed_threshold_pct / 100

    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
    min_threshold = config.min_movement_pct / 100
    threshold = max(threshold, min_threshold)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º label
    if movement > threshold:
        return 2, movement  # BUY
    elif movement < -threshold:
        return 0, movement  # SELL
    else:
        return 1, movement  # HOLD


class TripleBarrierPreprocessor:
    """
    Preprocessing —Å Triple Barrier Method.
    """

    def __init__(
        self,
        config: LabelingConfig = None,
        feature_store_group: str = "training_features",
        start_date: str = None,
        end_date: str = None
    ):
        self.config = config or DEFAULT_CONFIG
        self.feature_store_group = feature_store_group
        self.start_date = start_date
        self.end_date = end_date
        self.feature_store = get_feature_store()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_samples': 0,
            'labeled_samples': 0,
            'label_distribution': {}
        }

    def process_all_data(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        print("\n" + "=" * 80)
        print("PREPROCESSING V2 - TRIPLE BARRIER LABELING")
        print("=" * 80)
        print(f"Feature Group: {self.feature_store_group}")
        print(f"–ü–µ—Ä–∏–æ–¥: {self.start_date or '–Ω–∞—á–∞–ª–æ'} ‚Üí {self.end_date or '–∫–æ–Ω–µ—Ü'}")
        print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã Triple Barrier:")
        print(f"  ‚Ä¢ –ì–æ—Ä–∏–∑–æ–Ω—Ç—ã: {self.config.horizons} —Å–µ–∫—É–Ω–¥")
        print(f"  ‚Ä¢ TP –º–Ω–æ–∂–∏—Ç–µ–ª—å: {self.config.tp_multiplier}x ATR")
        print(f"  ‚Ä¢ SL –º–Ω–æ–∂–∏—Ç–µ–ª—å: {self.config.sl_multiplier}x ATR")
        print(f"  ‚Ä¢ Fixed threshold: {self.config.fixed_threshold_pct}%")
        print(f"  ‚Ä¢ Min movement: {self.config.min_movement_pct}%")
        print("=" * 80)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Feature Store...")
        df = self.feature_store.read_offline_features(
            feature_group=self.feature_store_group,
            start_date=self.start_date,
            end_date=self.end_date
        )

        if df is None or df.empty:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ Feature Store!")
            return

        print(f"   ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} —Å–µ–º–ø–ª–æ–≤")
        print(f"   ‚úì –°–∏–º–≤–æ–ª—ã: {df['symbol'].unique().tolist()}")

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è timestamps
        df = self._normalize_timestamps(df)

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        initial_count = len(df)
        df = df.drop_duplicates(subset=['symbol', 'timestamp'], keep='last')
        if len(df) < initial_count:
            print(f"   ‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–æ {initial_count - len(df):,} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        symbols = df['symbol'].unique()
        all_processed = []

        for symbol in symbols:
            print(f"\n{'‚îÄ' * 70}")
            print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {symbol}")
            print(f"{'‚îÄ' * 70}")

            symbol_df = df[df['symbol'] == symbol].copy()
            processed_df = self._process_symbol(symbol, symbol_df)
            all_processed.append(processed_df)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        final_df = pd.concat(all_processed, ignore_index=True)

        # –î–æ–±–∞–≤–ª—è–µ–º lagged features
        final_df = add_lagged_features(final_df)

        # –î–æ–±–∞–≤–ª—è–µ–º derived features
        final_df = add_derived_features(final_df)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        print(f"\n{'=' * 70}")
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        print(f"{'=' * 70}")

        self._cleanup_old_files(final_df)

        success = self.feature_store.write_offline_features(
            feature_group=self.feature_store_group,
            features=final_df,
            timestamp_column='timestamp'
        )

        if success:
            print(f"   ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(final_df):,} —Å–µ–º–ø–ª–æ–≤")
        else:
            print("   ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è!")

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._print_summary()

    def _normalize_timestamps(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç timestamps –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã."""
        print("\nüîß –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è timestamps...")

        normalized = []
        for ts in df['timestamp']:
            if pd.isna(ts):
                normalized.append(None)
            elif isinstance(ts, (int, np.integer, float, np.floating)):
                normalized.append(int(ts))
            else:
                try:
                    dt = pd.to_datetime(ts)
                    normalized.append(int(dt.timestamp() * 1000))
                except:
                    normalized.append(None)

        df['timestamp'] = normalized
        print("   ‚úì Timestamps –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã")
        return df

    def _process_symbol(self, symbol: str, df: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞."""
        df = df.sort_values('timestamp').reset_index(drop=True)
        n = len(df)
        print(f"   –°–µ–º–ø–ª–æ–≤: {n:,}")

        # –í—ã—á–∏—Å–ª—è–µ–º ATR –µ—Å–ª–∏ –µ—Å—Ç—å OHLC –¥–∞–Ω–Ω—ã–µ
        atr = compute_atr(df)
        has_atr = atr is not None and not atr.isna().all()
        if has_atr:
            print(f"   ‚úì ATR –≤—ã—á–∏—Å–ª–µ–Ω (mean: {atr.mean():.6f})")
        else:
            print(f"   ‚ö†Ô∏è ATR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fixed threshold")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º price column
        price_col = 'current_mid_price' if 'current_mid_price' in df.columns else 'close'

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å timestamp -> row –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        timestamp_to_idx = dict(zip(df['timestamp'], range(n)))

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç
        for horizon in self.config.horizons:
            label_col = f'future_direction_{horizon}s'
            movement_col = f'future_movement_{horizon}s'

            print(f"\n   –ì–æ—Ä–∏–∑–æ–Ω—Ç {horizon}s:")

            labels = np.full(n, 1, dtype=np.int32)  # Default = HOLD
            movements = np.full(n, 0.0, dtype=np.float64)
            labeled_count = 0

            for idx in range(n):
                current_ts = df.iloc[idx]['timestamp']
                current_price = df.iloc[idx][price_col]
                current_atr = atr.iloc[idx] if has_atr else None

                if pd.isna(current_price) or current_price <= 0:
                    continue

                # –ò—â–µ–º future price
                target_ts = current_ts + (horizon * 1000)
                tolerance = 15000  # ¬±15 —Å–µ–∫—É–Ω–¥

                future_price = None
                for future_idx in range(idx + 1, n):
                    future_ts = df.iloc[future_idx]['timestamp']
                    if abs(future_ts - target_ts) <= tolerance:
                        future_price = df.iloc[future_idx][price_col]
                        break
                    if future_ts > target_ts + tolerance:
                        break

                if future_price is not None and not pd.isna(future_price):
                    label, movement = apply_triple_barrier_label(
                        current_price, future_price, current_atr, self.config
                    )
                    labels[idx] = label
                    movements[idx] = movement
                    labeled_count += 1

            df[label_col] = labels
            df[movement_col] = movements

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è —ç—Ç–æ–≥–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
            sell = (labels == 0).sum()
            hold = (labels == 1).sum()
            buy = (labels == 2).sum()

            print(f"      Labeled: {labeled_count:,}/{n:,} ({100*labeled_count/n:.1f}%)")
            print(f"      Distribution: SELL={sell:,} ({100*sell/n:.1f}%) | "
                  f"HOLD={hold:,} ({100*hold/n:.1f}%) | BUY={buy:,} ({100*buy/n:.1f}%)")

            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            if horizon == 300:  # –û—Å–Ω–æ–≤–Ω–æ–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                self.stats['total_samples'] += n
                self.stats['labeled_samples'] += labeled_count
                self.stats['label_distribution'] = {
                    'SELL': sell, 'HOLD': hold, 'BUY': buy
                }

        return df

    def _cleanup_old_files(self, df: pd.DataFrame):
        """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ parquet —Ñ–∞–π–ª—ã –ø–µ—Ä–µ–¥ –∑–∞–ø–∏—Å—å—é –Ω–æ–≤—ã—Ö."""
        print("\nüóëÔ∏è –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤...")

        if 'timestamp' not in df.columns:
            return

        dates = pd.to_datetime(df['timestamp'], unit='ms').dt.strftime('%Y-%m-%d').unique()

        feature_store_dir = PROJECT_ROOT / "data" / "feature_store" / "offline" / self.feature_store_group
        deleted = 0

        for date_str in dates:
            partition_dir = feature_store_dir / f"date={date_str}"
            if partition_dir.exists():
                for f in partition_dir.glob("*.parquet"):
                    try:
                        f.unlink()
                        deleted += 1
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {f}: {e}")

        print(f"   ‚úì –£–¥–∞–ª–µ–Ω–æ {deleted} —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤")

    def _print_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
        print("\n" + "=" * 80)
        print("üìä –ò–¢–û–ì–ò PREPROCESSING V2")
        print("=" * 80)

        print(f"\n‚úì –í—Å–µ–≥–æ —Å–µ–º–ø–ª–æ–≤: {self.stats['total_samples']:,}")
        print(f"‚úì –†–∞–∑–º–µ—á–µ–Ω–æ: {self.stats['labeled_samples']:,}")

        dist = self.stats.get('label_distribution', {})
        if dist:
            total = sum(dist.values())
            print(f"\nüìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (–≥–æ—Ä–∏–∑–æ–Ω—Ç 300s):")
            for cls, count in dist.items():
                pct = 100 * count / total if total > 0 else 0
                bar = "‚ñà" * int(pct / 2)
                print(f"   {cls:4}: {bar} {count:,} ({pct:.1f}%)")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ mode collapse —Ä–∏—Å–∫
            hold_pct = 100 * dist.get('HOLD', 0) / total if total > 0 else 0
            if hold_pct > 60:
                print(f"\n‚ö†Ô∏è WARNING: HOLD class = {hold_pct:.1f}%")
                print("   –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ tp_multiplier/sl_multiplier")
            elif hold_pct > 40:
                print(f"\n‚úì HOLD class = {hold_pct:.1f}% - –ø—Ä–∏–µ–º–ª–µ–º–æ")
            else:
                print(f"\n‚úì HOLD class = {hold_pct:.1f}% - –æ—Ç–ª–∏—á–Ω–æ!")

        print("\n" + "=" * 80)
        print("‚úÖ Preprocessing –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print("=" * 80)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Preprocessing V2 —Å Triple Barrier Labeling"
    )
    parser.add_argument(
        '--feature-group',
        default='training_features',
        help='Feature group –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏'
    )
    parser.add_argument(
        '--start-date',
        default=None,
        help='–ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--end-date',
        default=None,
        help='–ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--tp-mult',
        type=float,
        default=2.0,
        help='Take Profit –º–Ω–æ–∂–∏—Ç–µ–ª—å ATR (default: 2.0)'
    )
    parser.add_argument(
        '--sl-mult',
        type=float,
        default=2.0,
        help='Stop Loss –º–Ω–æ–∂–∏—Ç–µ–ª—å ATR (default: 2.0)'
    )
    parser.add_argument(
        '--min-movement',
        type=float,
        default=0.05,
        help='–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ –≤ %% (default: 0.05)'
    )

    args = parser.parse_args()

    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = LabelingConfig(
        horizons=[60, 180, 300],  # 1, 3, 5 –º–∏–Ω—É—Ç
        tp_multiplier=args.tp_mult,
        sl_multiplier=args.sl_mult,
        min_movement_pct=args.min_movement
    )

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    processor = TripleBarrierPreprocessor(
        config=config,
        feature_store_group=args.feature_group,
        start_date=args.start_date,
        end_date=args.end_date
    )

    # –ó–∞–ø—É—Å–∫
    processor.process_all_data()


if __name__ == "__main__":
    main()
