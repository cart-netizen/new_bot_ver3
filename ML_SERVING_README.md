# ML Model Serving Infrastructure

Complete production-ready ML infrastructure –¥–ª—è trading bot.

## üìã –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ

1. [–û–±–∑–æ—Ä](#–æ–±–∑–æ—Ä)
2. [–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã](#–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã)
3. [Quick Start](#quick-start)
4. [Model Registry](#model-registry)
5. [Model Server](#model-server)
6. [A/B Testing](#ab-testing)
7. [ONNX Optimization](#onnx-optimization)
8. [Integration](#integration)
9. [API Reference](#api-reference)
10. [Troubleshooting](#troubleshooting)

---

## üéØ –û–±–∑–æ—Ä

ML Model Serving Infrastructure –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω—ã–π lifecycle management –¥–ª—è ML –º–æ–¥–µ–ª–µ–π:

- **Model Registry**: –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏
- **Model Server**: FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è inference (port 8001)
- **A/B Testing**: Testing –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ production
- **ONNX Optimization**: –≠–∫—Å–ø–æ—Ä—Ç –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è latency < 3ms

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Trading Bot (Main)                     ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ          Model Client (HTTP Client)                  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ HTTP (port 8001)
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               Model Server (FastAPI)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Model    ‚îÇ  ‚îÇ   A/B     ‚îÇ  ‚îÇ  ONNX Sessions    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Registry  ‚îÇ  ‚îÇ  Testing  ‚îÇ  ‚îÇ  (Optimized)      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ            Loaded Models (PyTorch/ONNX)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº
                  File System
            models/
            ‚îú‚îÄ‚îÄ hybrid_cnn_lstm/
            ‚îÇ   ‚îú‚îÄ‚îÄ v1.0.0/
            ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.pt
            ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.onnx
            ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata.json
            ‚îÇ   ‚îú‚îÄ‚îÄ v1.1.0/
            ‚îÇ   ‚îî‚îÄ‚îÄ production -> v1.0.0
```

---

## üß© –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

### 1. Model Registry
**–§–∞–π–ª**: `backend/ml_engine/inference/model_registry.py`

–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ lifecycle –º–æ–¥–µ–ª–µ–π:
- –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π
- Staging ‚Üí Production promotion
- –ú–µ—Ç—Ä–∏–∫–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
- Symlink management –¥–ª—è stages

### 2. Model Server
**–§–∞–π–ª**: `backend/ml_engine/inference/model_server_v2.py`

FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è inference:
- Single –∏ batch predictions
- Hot reload –º–æ–¥–µ–ª–µ–π
- A/B testing support
- Health monitoring

### 3. A/B Testing Manager
**–§–∞–π–ª**: `backend/ml_engine/inference/ab_testing.py`

Testing –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π:
- Traffic splitting (90/10)
- Statistical significance testing
- Automatic promotion/rollback
- Comprehensive metrics

### 4. ONNX Optimizer
**–§–∞–π–ª**: `backend/ml_engine/optimization/onnx_optimizer.py`

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π:
- PyTorch ‚Üí ONNX export
- INT8 quantization
- Benchmarking
- Latency optimization

### 5. Model Client
**–§–∞–π–ª**: `backend/ml_engine/inference/model_client.py`

HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏:
- Async predictions
- Batch support
- Health checks
- A/B test management

---

## üöÄ Quick Start

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install onnx onnxruntime fastapi uvicorn scipy
```

### 2. –ó–∞–ø—É—Å–∫ Model Server

```bash
# –í –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ
cd backend
python -m backend.ml_engine.inference.model_server_v2

# –ò–ª–∏ —á–µ—Ä–µ–∑ uvicorn
uvicorn backend.ml_engine.inference.model_server_v2:app --host 0.0.0.0 --port 8001
```

Server –∑–∞–ø—É—Å—Ç–∏—Ç—Å—è –Ω–∞ `http://localhost:8001`

### 3. –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏

```python
import asyncio
from pathlib import Path
from backend.ml_engine.inference.model_registry import get_model_registry

async def register_model():
    registry = get_model_registry()

    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å trained –º–æ–¥–µ–ª—å
    model_info = await registry.register_model(
        name="hybrid_cnn_lstm",
        version="1.0.0",
        model_path=Path("path/to/model.pt"),
        model_type="HybridCNNLSTM",
        description="Initial production model",
        metrics={
            "accuracy": 0.85,
            "sharpe_ratio": 2.5,
            "latency_ms": 5.0
        },
        training_params={
            "input_size": 110,
            "lstm_hidden_size": 256,
            "lstm_layers": 2,
            "dropout": 0.3
        }
    )

    # Promote to production
    await registry.promote_to_production("hybrid_cnn_lstm", "1.0.0")
    print(f"Model registered: {model_info.metadata.name} v{model_info.metadata.version}")

asyncio.run(register_model())
```

### 4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –±–æ—Ç–µ

```python
from backend.ml_engine.inference.model_client import get_model_client
import numpy as np

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞
client = get_model_client("http://localhost:8001")
await client.initialize()

# Prediction
features = np.random.randn(60, 110)  # 60 timesteps, 110 features
prediction = await client.predict(
    symbol="BTCUSDT",
    features=features
)

print(f"Prediction: {prediction['prediction']}")
print(f"Latency: {prediction['latency_ms']:.2f}ms")
```

---

## üì¶ Model Registry

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è

```
models/
‚îú‚îÄ‚îÄ hybrid_cnn_lstm/
‚îÇ   ‚îú‚îÄ‚îÄ v1.0.0/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.pt           # PyTorch –≤–µ—Å–∞
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.onnx         # ONNX (optional)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata.json      # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metrics.json       # –ú–µ—Ç—Ä–∏–∫–∏ (deprecated, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ metadata)
‚îÇ   ‚îú‚îÄ‚îÄ v1.1.0/
‚îÇ   ‚îú‚îÄ‚îÄ v2.0.0/
‚îÇ   ‚îú‚îÄ‚îÄ production -> v1.0.0   # Symlink
‚îÇ   ‚îî‚îÄ‚îÄ staging -> v1.1.0      # Symlink
```

### API

#### –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏

```python
from backend.ml_engine.inference.model_registry import get_model_registry

registry = get_model_registry()

model_info = await registry.register_model(
    name="hybrid_cnn_lstm",
    version="1.1.0",
    model_path=Path("models/trained_model.pt"),
    model_type="HybridCNNLSTM",
    description="Improved model with better features",
    metrics={
        "accuracy": 0.87,
        "precision": 0.85,
        "recall": 0.89,
        "sharpe_ratio": 2.8
    },
    training_params={
        "epochs": 50,
        "batch_size": 64,
        "learning_rate": 0.001
    },
    tags=["improved", "production-candidate"]
)
```

#### –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

```python
# –ü–æ –≤–µ—Ä—Å–∏–∏
model_info = await registry.get_model("hybrid_cnn_lstm", "1.1.0")

# Production –≤–µ—Ä—Å–∏—è
model_info = await registry.get_production_model("hybrid_cnn_lstm")

# Staging –≤–µ—Ä—Å–∏—è
model_info = await registry.get_staging_model("hybrid_cnn_lstm")

# Latest –≤–µ—Ä—Å–∏—è (–µ—Å–ª–∏ production –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
model_info = await registry.get_model("hybrid_cnn_lstm")
```

#### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ stages

```python
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å staging
await registry.set_model_stage("hybrid_cnn_lstm", "1.1.0", ModelStage.STAGING)

# Promote to production
await registry.promote_to_production("hybrid_cnn_lstm", "1.1.0")

# Retire (archive)
await registry.retire_model("hybrid_cnn_lstm", "1.0.0")
```

#### –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π

```python
# –í—Å–µ –≤–µ—Ä—Å–∏–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
models = await registry.list_models("hybrid_cnn_lstm")

# –í—Å–µ –º–æ–¥–µ–ª–∏
all_models = await registry.list_models()

for model in models:
    print(f"{model.metadata.name} v{model.metadata.version} - {model.metadata.stage}")
```

#### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

```python
comparison = await registry.compare_models(
    name="hybrid_cnn_lstm",
    version1="1.0.0",
    version2="1.1.0"
)

print(f"Accuracy improvement: {comparison['metrics_comparison']['accuracy']['diff_pct']:.2f}%")
print(f"Size difference: {comparison['size_comparison']['diff_mb']:.2f} MB")
```

---

## üñ•Ô∏è Model Server

### Endpoints

#### POST /api/ml/predict
Single prediction

**Request**:
```json
{
  "symbol": "BTCUSDT",
  "features": [0.1, 0.2, ..., 0.5],  // Flattened feature vector
  "model_name": "hybrid_cnn_lstm",    // Optional
  "model_version": "1.0.0"            // Optional
}
```

**Response**:
```json
{
  "symbol": "BTCUSDT",
  "prediction": {
    "direction": 0,      // 0=HOLD, 1=BUY, 2=SELL
    "confidence": 0.85,
    "expected_return": 0.025
  },
  "model_name": "hybrid_cnn_lstm",
  "model_version": "1.0.0",
  "variant": null,
  "latency_ms": 3.5,
  "timestamp": "2025-11-06T12:00:00"
}
```

#### POST /api/ml/predict/batch
Batch predictions

**Request**:
```json
{
  "requests": [
    {"symbol": "BTCUSDT", "features": [...]},
    {"symbol": "ETHUSDT", "features": [...]}
  ],
  "max_batch_size": 32
}
```

#### GET /api/ml/models
–°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

**Response**:
```json
[
  {
    "name": "hybrid_cnn_lstm",
    "version": "1.0.0",
    "stage": "Production",
    "model_type": "HybridCNNLSTM",
    "metrics": {"accuracy": 0.85, "sharpe": 2.5},
    "size_mb": 12.5,
    "loaded": true
  }
]
```

#### POST /api/ml/models/reload
Hot reload –º–æ–¥–µ–ª–∏

**Request**:
```json
{
  "model_name": "hybrid_cnn_lstm",
  "version": "1.1.0"  // Optional, default = production
}
```

#### GET /api/ml/health
Health check

**Response**:
```json
{
  "status": "healthy",
  "timestamp": "2025-11-06T12:00:00",
  "loaded_models": ["hybrid_cnn_lstm:1.0.0"],
  "active_experiments": ["v1_vs_v2"],
  "uptime_seconds": 3600.5
}
```

### curl Examples

```bash
# Health check
curl http://localhost:8001/api/ml/health

# Prediction
curl -X POST http://localhost:8001/api/ml/predict \
  -H "Content-Type: application/json" \
  -d '{
    "symbol": "BTCUSDT",
    "features": [0.1, 0.2, 0.3, ..., 0.5]
  }'

# List models
curl http://localhost:8001/api/ml/models

# Reload model
curl -X POST http://localhost:8001/api/ml/models/reload \
  -H "Content-Type: application/json" \
  -d '{"model_name": "hybrid_cnn_lstm", "version": "1.1.0"}'
```

---

## üß™ A/B Testing

### Workflow

1. **Create Experiment**: –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å control –∏ treatment –º–æ–¥–µ–ª–∏
2. **Traffic Splitting**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ route 90% ‚Üí control, 10% ‚Üí treatment
3. **Collect Metrics**: Record predictions –∏ outcomes
4. **Analyze**: Statistical significance testing
5. **Decision**: Promote treatment –∏–ª–∏ rollback

### API

#### –°–æ–∑–¥–∞–Ω–∏–µ A/B —Ç–µ—Å—Ç–∞

```python
from backend.ml_engine.inference.model_client import get_model_client

client = get_model_client()
await client.initialize()

# Create experiment
success = await client.create_ab_test(
    experiment_id="v1_vs_v2",
    control_model="hybrid_cnn_lstm",
    control_version="1.0.0",
    treatment_model="hybrid_cnn_lstm",
    treatment_version="1.1.0",
    traffic_split=0.9  # 90% control, 10% treatment
)

print(f"Experiment created: {success}")
```

#### Predictions —á–µ—Ä–µ–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç

```python
# Predictions –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ routing
prediction = await client.predict(
    symbol="BTCUSDT",
    features=features,
    # experiment_id –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –µ—Å–ª–∏ active
)

# –í–∞—Ä–∏–∞–Ω—Ç —É–∫–∞–∑–∞–Ω –≤ response
print(f"Variant: {prediction.get('variant')}")  # control –∏–ª–∏ treatment
```

#### –ê–Ω–∞–ª–∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞

```python
# –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π –∞–Ω–∞–ª–∏–∑
analysis = await client.get_ab_test_analysis("v1_vs_v2")

print(f"Control accuracy: {analysis['control']['accuracy']:.2%}")
print(f"Treatment accuracy: {analysis['treatment']['accuracy']:.2%}")
print(f"Improvement: {analysis['improvement']['accuracy']:.2%}")
print(f"Recommendation: {analysis['recommendation']['action']}")
print(f"Reasons: {analysis['recommendation']['reasons']}")
```

#### –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞

```python
# Stop –∏ –ø–æ–ª—É—á–∏—Ç—å final report
report = await client.stop_ab_test("v1_vs_v2")

print(f"Final recommendation: {report['recommendation']['action']}")
# Actions: "promote", "rollback", "continue"

if report['recommendation']['action'] == "promote":
    # Promote treatment to production
    registry = get_model_registry()
    await registry.promote_to_production("hybrid_cnn_lstm", "1.1.0")

    # Reload –Ω–∞ server
    await client.reload_model("hybrid_cnn_lstm", "1.1.0")
```

### –ú–µ—Ç—Ä–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è

A/B —Ç–µ—Å—Ç —Å–æ–±–∏—Ä–∞–µ—Ç –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç:

**Performance Metrics**:
- Accuracy, Precision, Recall, F1
- Win rate, Average return, Sharpe ratio, Total P&L

**Technical Metrics**:
- Average latency, P95 latency
- Error rate
- Throughput

**Statistical Tests**:
- Two-sample t-test –¥–ª—è accuracy
- Confidence level (default 95%)
- P-value –¥–ª—è significance

### –ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ recommendations based on:

1. **Promote treatment –µ—Å–ª–∏**:
   - Accuracy improvement >= 2% (configurable)
   - Statistical significance (p < 0.05)
   - Latency degradation < 2ms
   - Error rate –Ω–µ —É–≤–µ–ª–∏—á–∏–ª—Å—è

2. **Rollback –µ—Å–ª–∏**:
   - Latency degradation > 2ms
   - Error rate —É–≤–µ–ª–∏—á–∏–ª—Å—è > 50%
   - Accuracy degraded > 5%

3. **Continue –µ—Å–ª–∏**:
   - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ samples
   - Improvement marginal
   - Not statistically significant

---

## ‚ö° ONNX Optimization

### Export to ONNX

```python
from backend.ml_engine.optimization.onnx_optimizer import get_onnx_optimizer
from backend.ml_engine.models.hybrid_cnn_lstm import HybridCNNLSTM
from pathlib import Path

optimizer = get_onnx_optimizer()

# Load PyTorch model
model = HybridCNNLSTM(input_size=110, ...)
model_path = Path("models/hybrid_cnn_lstm/v1.0.0/model.pt")
onnx_path = Path("models/hybrid_cnn_lstm/v1.0.0/model.onnx")

# Export
success = await optimizer.export_to_onnx(
    model=model,
    model_path=model_path,
    output_path=onnx_path,
    input_shape=(1, 60, 110),  # batch, timesteps, features
    opset_version=14
)

print(f"Export success: {success}")
```

### Quantization (FP32 ‚Üí INT8)

```python
# Quantize –¥–ª—è -75% memory, -40% latency
quantized_path = Path("models/hybrid_cnn_lstm/v1.0.0/model_quantized.onnx")

success = await optimizer.quantize_model(
    onnx_path=onnx_path,
    output_path=quantized_path,
    quantization_type="dynamic"
)

print(f"Quantization success: {success}")
```

### Benchmarking

```python
# Benchmark original ONNX
metrics = await optimizer.benchmark(
    onnx_path=onnx_path,
    input_shape=(1, 60, 110),
    num_iterations=1000,
    warmup_iterations=100
)

print(f"Average latency: {metrics['latency_ms']:.2f}ms")
print(f"P95 latency: {metrics['p95_ms']:.2f}ms")
print(f"Throughput: {metrics['throughput']:.0f} predictions/sec")

# Benchmark quantized
quant_metrics = await optimizer.benchmark(
    onnx_path=quantized_path,
    input_shape=(1, 60, 110),
    num_iterations=1000
)

speedup = metrics['latency_ms'] / quant_metrics['latency_ms']
print(f"Quantized speedup: {speedup:.2f}x")
```

### Full Optimization Pipeline

```python
# Complete: Export + Quantize + Benchmark
results = await optimizer.export_and_optimize(
    model=model,
    model_path=model_path,
    output_dir=Path("models/hybrid_cnn_lstm/v1.0.0"),
    input_shape=(1, 60, 110),
    quantize=True,
    benchmark_iterations=1000
)

print(f"Export: {results['export_success']}")
print(f"Quantize: {results['quantize_success']}")
print(f"ONNX FP32: {results['benchmarks']['onnx_fp32']['latency_ms']:.2f}ms")
print(f"ONNX INT8: {results['benchmarks']['onnx_int8']['latency_ms']:.2f}ms")
print(f"Speedup: {results['comparison']['speedup']:.2f}x")
```

---

## üîó Integration

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –æ—Å–Ω–æ–≤–Ω—ã–º –±–æ—Ç–æ–º

#### main.py updates

```python
from backend.ml_engine.inference.model_client import get_model_client

class TradingBot:
    def __init__(self):
        # ...existing code...

        # Initialize Model Client
        self.model_client = get_model_client(
            server_url=settings.ML_SERVER_URL
        )

    async def start(self):
        # ...existing code...

        # Initialize ML client
        await self.model_client.initialize()

        # Health check
        healthy = await self.model_client.health_check()
        if not healthy:
            logger.warning("ML Model Server is not healthy")

    async def stop(self):
        # ...existing code...

        # Cleanup ML client
        await self.model_client.cleanup()
```

#### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ analysis loop

```python
async def _analysis_loop(self):
    while self.running:
        for symbol in self.symbols:
            # ...extract features...

            # ML prediction
            ml_prediction = await self.model_client.predict(
                symbol=symbol,
                features=features_array
            )

            if ml_prediction:
                # Use prediction
                direction = ml_prediction['prediction']['direction']
                confidence = ml_prediction['prediction']['confidence']

                # Integrate with strategy consensus
                # ...
```

---

## üìö API Reference

### Model Registry

```python
class ModelRegistry:
    async def register_model(name, version, model_path, model_type, ...) -> ModelInfo
    async def get_model(name, version=None, stage=None) -> ModelInfo
    async def list_models(name=None) -> List[ModelInfo]
    async def set_model_stage(name, version, stage) -> bool
    async def promote_to_production(name, version) -> bool
    async def retire_model(name, version) -> bool
    async def delete_model(name, version) -> bool
    async def update_metrics(name, version, metrics) -> bool
    async def compare_models(name, version1, version2) -> Dict
```

### Model Client

```python
class ModelClient:
    async def initialize()
    async def cleanup()
    async def predict(symbol, features, model_name=None, model_version=None) -> Dict
    async def batch_predict(requests, max_batch_size=32) -> List[Dict]
    async def health_check() -> bool
    async def list_models() -> List[Dict]
    async def reload_model(model_name, version=None) -> bool
    async def create_ab_test(experiment_id, control_model, ...) -> bool
    async def get_ab_test_analysis(experiment_id) -> Dict
    async def stop_ab_test(experiment_id) -> Dict
```

### ONNX Optimizer

```python
class ONNXOptimizer:
    async def export_to_onnx(model, model_path, output_path, input_shape, ...) -> bool
    async def quantize_model(onnx_path, output_path, quantization_type="dynamic") -> bool
    async def optimize_graph(onnx_path, output_path) -> bool
    async def benchmark(onnx_path, input_shape, num_iterations=1000, ...) -> Dict
    async def compare_pytorch_onnx(pytorch_model, onnx_path, ...) -> Dict
    async def export_and_optimize(model, model_path, output_dir, ...) -> Dict
```

---

## üêõ Troubleshooting

### Model Server –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è

**Problem**: `ModuleNotFoundError: No module named 'onnx'`

**Solution**:
```bash
pip install onnx onnxruntime
```

---

### ONNX export fails

**Problem**: `RuntimeError: ONNX export failed: Unsupported operator`

**Solution**:
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ opset_version (–ø–æ–ø—Ä–æ–±—É–π—Ç–µ 11, 13, 14)
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã PyTorch –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –≤ ONNX
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `verbose=True` –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∞

---

### Predictions slow

**Problem**: Latency > 10ms

**Solutions**:
1. Use ONNX version: `use_onnx=True` –ø—Ä–∏ loading
2. Quantize model: INT8 quantization
3. Reduce model size: Prune –∏–ª–∏ distill
4. Check batch size: Batch predictions –¥–ª—è throughput

---

### A/B test not routing

**Problem**: –í—Å–µ predictions –∏–¥—É—Ç –Ω–∞ control

**Solution**:
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ experiment —Å–æ–∑–¥–∞–Ω
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ experiment status = RUNNING
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ traffic split (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å control + treatment = 1.0)

---

### Model Registry symlinks not working (Windows)

**Problem**: Symlinks –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ Windows

**Solution**:
- Enable Developer Mode –≤ Windows
- –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ version –Ω–∞–ø—Ä—è–º—É—é –≤–º–µ—Å—Ç–æ stage
- –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Linux/Mac –¥–ª—è development

---

## üéì Best Practices

### 1. Model Versioning

- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ semantic versioning: `major.minor.patch`
- `major`: Breaking changes (new architecture)
- `minor`: Improvements (better features, hyperparams)
- `patch`: Bug fixes (training improvements)

### 2. A/B Testing

- –í—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–π—Ç–µ —Å 90/10 split
- –ú–∏–Ω–∏–º—É–º 100 samples per variant
- Run –º–∏–Ω–∏–º—É–º 24 —á–∞—Å–∞
- Monitor latency AND accuracy

### 3. Production Deployment

- –í—Å–µ–≥–¥–∞ test –≤ staging –ø–µ—Ä–µ–¥ production
- Use A/B test –¥–ª—è validation
- Keep previous version –≤ production –¥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞
- Monitor drift –ø–æ—Å–ª–µ deployment

### 4. Performance

- Export to ONNX –¥–ª—è production
- Quantize –µ—Å–ª–∏ latency –∫—Ä–∏—Ç–∏—á–Ω–∞
- Benchmark –ø–µ—Ä–µ–¥ deployment
- Monitor latency –≤ production

---

## üìù Changelog

### v2.0.0 (2025-11-06)
- ‚úÖ Initial release
- ‚úÖ Model Registry
- ‚úÖ Model Server v2
- ‚úÖ A/B Testing Infrastructure
- ‚úÖ ONNX Optimization
- ‚úÖ Complete integration tests

### Planned v2.1.0
- [ ] MLflow integration
- [ ] Auto-retraining pipeline
- [ ] Feature store
- [ ] Multi-GPU support

---

## üìû Support

–î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ issues:
- GitHub Issues: https://github.com/cart-netizen/new_bot_ver3/issues
- Documentation: –°–º. —ç—Ç–æ—Ç —Ñ–∞–π–ª
- Code Examples: `backend/tests/test_ml_serving.py`

---

**Status**: ‚úÖ Production Ready
**Version**: 2.0.0
**Last Updated**: 2025-11-06
