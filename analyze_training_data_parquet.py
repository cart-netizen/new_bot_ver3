#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è –∏–∑ Feature Store (Parquet —Ñ–∞–π–ª—ã).

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
- –û–±—ä–µ–º —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (class imbalance)
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
- –ö–∞—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫
- –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –æ–±—É—á–µ–Ω–∏—é

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python analyze_training_data_parquet.py
    python analyze_training_data_parquet.py --feature-group training_features
    python analyze_training_data_parquet.py --show-details
"""

import sys
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from collections import Counter
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent))

from backend.ml_engine.feature_store.feature_store import FeatureStore
from backend.core.logger import get_logger

logger = get_logger(__name__)


class ParquetDataAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ Feature Store."""

    def __init__(self, feature_group: str = "training_features"):
        self.feature_group = feature_group
        self.feature_store = FeatureStore()

    def load_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Feature Store."""
        print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Feature Store...")
        print(f"   Feature Group: {self.feature_group}\n")

        try:
            df = self.feature_store.read_features(
                feature_group=self.feature_group,
                start_time=None,  # –í—Å–µ –¥–∞–Ω–Ω—ã–µ
                end_time=None
            )

            if df.empty:
                print("‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
                return df

            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} –∑–∞–ø–∏—Å–µ–π")
            print(f"   –ü–µ—Ä–∏–æ–¥: {df['timestamp'].min()} ‚Üí {df['timestamp'].max()}\n")

            return df

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return pd.DataFrame()

    def analyze(self, show_details: bool = False) -> dict:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö."""
        df = self.load_data()

        if df.empty:
            return {}

        print("=" * 80)
        print("–ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø ML –ú–û–î–ï–õ–ò")
        print("=" * 80)
        print()

        results = {
            "total_samples": len(df),
            "data_quality": self._analyze_data_quality(df),
            "class_distribution": self._analyze_class_distribution(df),
            "feature_statistics": self._analyze_features(df, show_details),
            "temporal_coverage": self._analyze_temporal(df),
            "readiness": self._assess_readiness(df)
        }

        self._print_summary(results)
        self._print_recommendations(results)

        return results

    def _analyze_data_quality(self, df: pd.DataFrame) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö."""
        print("üìä –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•")
        print("-" * 80)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–∫
        has_labels = 'future_direction' in df.columns or 'label' in df.columns
        label_col = 'future_direction' if 'future_direction' in df.columns else 'label'

        if not has_labels:
            print("‚ùå –ö–†–ò–¢–ò–ß–ù–û: –ù–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ —Å –º–µ—Ç–∫–∞–º–∏!")
            print("   ‚Üí –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python preprocessing_add_future_labels_parquet.py\n")
            return {"has_labels": False}

        # –ü–æ–¥—Å—á–µ—Ç NaN
        total_samples = len(df)
        nan_labels = df[label_col].isna().sum()
        valid_samples = total_samples - nan_labels

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_cols = [col for col in df.columns if col.startswith(('orderbook_', 'candle_', 'indicator_'))]
        nan_features = df[feature_cols].isna().sum().sum()

        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_samples:,}")
        print(f"  ‚Ä¢ –í–∞–ª–∏–¥–Ω—ã—Ö –º–µ—Ç–æ–∫: {valid_samples:,} ({valid_samples/total_samples*100:.1f}%)")

        if nan_labels > 0:
            print(f"  ‚ö†Ô∏è  NaN –º–µ—Ç–æ–∫: {nan_labels:,} ({nan_labels/total_samples*100:.1f}%)")

        if nan_features > 0:
            print(f"  ‚ö†Ô∏è  NaN –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö: {nan_features:,}")

        quality_score = (valid_samples / total_samples) * 100
        if quality_score >= 95:
            print(f"  ‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ: {quality_score:.1f}% (–æ—Ç–ª–∏—á–Ω–æ)")
        elif quality_score >= 80:
            print(f"  ‚ö†Ô∏è  –ö–∞—á–µ—Å—Ç–≤–æ: {quality_score:.1f}% (–ø—Ä–∏–µ–º–ª–µ–º–æ)")
        else:
            print(f"  ‚ùå –ö–∞—á–µ—Å—Ç–≤–æ: {quality_score:.1f}% (–Ω–∏–∑–∫–æ–µ)")

        print()

        return {
            "has_labels": True,
            "label_column": label_col,
            "total_samples": total_samples,
            "valid_samples": valid_samples,
            "nan_labels": nan_labels,
            "nan_features": nan_features,
            "quality_score": quality_score
        }

    def _analyze_class_distribution(self, df: pd.DataFrame) -> dict:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤."""
        print("üìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í")
        print("-" * 80)

        label_col = 'future_direction' if 'future_direction' in df.columns else 'label'

        if label_col not in df.columns:
            print("‚ùå –ö–æ–ª–æ–Ω–∫–∞ —Å –º–µ—Ç–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\n")
            return {}

        # –£–±–∏—Ä–∞–µ–º NaN
        labels = df[label_col].dropna()

        if len(labels) == 0:
            print("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –º–µ—Ç–æ–∫\n")
            return {}

        # –ü–æ–¥—Å—á–µ—Ç –∫–ª–∞—Å—Å–æ–≤
        class_counts = Counter(labels)
        total = len(labels)

        # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤
        class_names = {0: "DOWN", 1: "NEUTRAL", 2: "UP"}
        if -1 in class_counts:
            class_names = {-1: "DOWN", 0: "NEUTRAL", 1: "UP"}

        print(f"  –§–æ—Ä–º–∞—Ç –º–µ—Ç–æ–∫: {list(class_counts.keys())}")
        print()

        for cls in sorted(class_counts.keys()):
            count = class_counts[cls]
            pct = (count / total) * 100
            name = class_names.get(cls, f"Class {cls}")
            bar = "‚ñà" * int(pct / 2)  # –í–∏–∑—É–∞–ª—å–Ω–∞—è –ø–æ–ª–æ—Å–∫–∞
            print(f"  {name:8} ({cls:2}): {count:6,} ({pct:5.1f}%) {bar}")

        print()

        # –û—Ü–µ–Ω–∫–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
        max_count = max(class_counts.values())
        min_count = min(class_counts.values())
        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')

        if imbalance_ratio <= 2.0:
            print(f"  ‚úÖ –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {imbalance_ratio:.2f}x (—Ö–æ—Ä–æ—à–æ)")
        elif imbalance_ratio <= 5.0:
            print(f"  ‚ö†Ô∏è  –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {imbalance_ratio:.2f}x (–ø—Ä–∏–µ–º–ª–µ–º–æ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ class weights)")
        else:
            print(f"  ‚ùå –°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å: {imbalance_ratio:.2f}x (—Ç—Ä–µ–±—É–µ—Ç—Å—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞)")

        print()

        return {
            "total_labeled": total,
            "class_counts": dict(class_counts),
            "class_percentages": {cls: (count/total)*100 for cls, count in class_counts.items()},
            "imbalance_ratio": imbalance_ratio
        }

    def _analyze_features(self, df: pd.DataFrame, show_details: bool) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
        print("üî¨ –ü–†–ò–ó–ù–ê–ö–ò")
        print("-" * 80)

        # –ì—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        orderbook_cols = [col for col in df.columns if col.startswith('orderbook_')]
        candle_cols = [col for col in df.columns if col.startswith('candle_')]
        indicator_cols = [col for col in df.columns if col.startswith('indicator_')]

        print(f"  ‚Ä¢ OrderBook –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(orderbook_cols)}")
        print(f"  ‚Ä¢ Candle –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(candle_cols)}")
        print(f"  ‚Ä¢ Indicator –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(indicator_cols)}")
        print(f"  ‚Ä¢ –í–°–ï–ì–û –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(orderbook_cols) + len(candle_cols) + len(indicator_cols)}")
        print()

        if show_details:
            print("  –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º:")
            all_features = orderbook_cols + candle_cols + indicator_cols

            for feat in all_features[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                values = df[feat].dropna()
                print(f"    {feat:40}: mean={values.mean():8.4f}, std={values.std():8.4f}")

            if len(all_features) > 10:
                print(f"    ... –∏ –µ—â—ë {len(all_features) - 10} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            print()

        return {
            "orderbook_count": len(orderbook_cols),
            "candle_count": len(candle_cols),
            "indicator_count": len(indicator_cols),
            "total_features": len(orderbook_cols) + len(candle_cols) + len(indicator_cols)
        }

    def _analyze_temporal(self, df: pd.DataFrame) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è."""
        print("‚è∞ –í–†–ï–ú–ï–ù–ù–û–ï –ü–û–ö–†–´–¢–ò–ï")
        print("-" * 80)

        if 'timestamp' not in df.columns:
            print("‚ùå –ö–æ–ª–æ–Ω–∫–∞ timestamp –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\n")
            return {}

        timestamps = pd.to_datetime(df['timestamp'])
        start_time = timestamps.min()
        end_time = timestamps.max()
        duration = end_time - start_time

        print(f"  ‚Ä¢ –ù–∞—á–∞–ª–æ: {start_time}")
        print(f"  ‚Ä¢ –ö–æ–Ω–µ—Ü: {end_time}")
        print(f"  ‚Ä¢ –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration.days} –¥–Ω–µ–π, {duration.seconds//3600} —á–∞—Å–æ–≤")
        print()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤
        if 'symbol' in df.columns:
            symbols = df['symbol'].unique()
            print(f"  ‚Ä¢ –°–∏–º–≤–æ–ª–æ–≤: {len(symbols)}")
            if len(symbols) <= 20:
                for sym in sorted(symbols):
                    count = (df['symbol'] == sym).sum()
                    print(f"    - {sym}: {count:,} –∑–∞–ø–∏—Å–µ–π")
            print()

        return {
            "start_time": str(start_time),
            "end_time": str(end_time),
            "duration_days": duration.days,
            "symbols": list(df['symbol'].unique()) if 'symbol' in df.columns else []
        }

    def _assess_readiness(self, df: pd.DataFrame) -> dict:
        """–û—Ü–µ–Ω–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –æ–±—É—á–µ–Ω–∏—é."""
        label_col = 'future_direction' if 'future_direction' in df.columns else 'label'

        has_labels = label_col in df.columns
        valid_samples = len(df[label_col].dropna()) if has_labels else 0

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        min_samples = 1000  # –ú–∏–Ω–∏–º—É–º –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è
        recommended_samples = 10000  # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ

        readiness_score = 0
        issues = []
        recommendations = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –ù–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–∫
        if not has_labels:
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –º–µ—Ç–∫–∏ (future_direction/label)")
            recommendations.append("–ó–∞–ø—É—Å—Ç–∏—Ç–µ: python preprocessing_add_future_labels_parquet.py")
        else:
            readiness_score += 25

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        if valid_samples >= recommended_samples:
            readiness_score += 50
        elif valid_samples >= min_samples:
            readiness_score += 25
            recommendations.append(f"–°–æ–±–µ—Ä–∏—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö (—Ç–µ–∫—É—â–∏—Ö: {valid_samples:,}, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: {recommended_samples:,})")
        else:
            issues.append(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({valid_samples:,} < {min_samples:,})")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
        if has_labels and valid_samples > 0:
            labels = df[label_col].dropna()
            class_counts = Counter(labels)
            if len(class_counts) >= 2:
                max_count = max(class_counts.values())
                min_count = min(class_counts.values())
                imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')

                if imbalance_ratio <= 5.0:
                    readiness_score += 25
                else:
                    issues.append(f"–°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ ({imbalance_ratio:.1f}x)")
                    recommendations.append("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ class_weights –∏–ª–∏ focal_loss –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏")

        return {
            "ready": readiness_score >= 75,
            "score": readiness_score,
            "issues": issues,
            "recommendations": recommendations
        }

    def _print_summary(self, results: dict):
        """–ü–µ—á–∞—Ç—å –∏—Ç–æ–≥–æ–≤–æ–π —Å–≤–æ–¥–∫–∏."""
        print("=" * 80)
        print("–ò–¢–û–ì–û–í–ê–Ø –û–¶–ï–ù–ö–ê –ì–û–¢–û–í–ù–û–°–¢–ò –ö –û–ë–£–ß–ï–ù–ò–Æ")
        print("=" * 80)
        print()

        readiness = results["readiness"]
        score = readiness["score"]

        # –í–∏–∑—É–∞–ª—å–Ω—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä
        bar_length = 50
        filled = int((score / 100) * bar_length)
        bar = "‚ñà" * filled + "‚ñë" * (bar_length - filled)

        print(f"  –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å: [{bar}] {score}%")
        print()

        if readiness["ready"]:
            print("  ‚úÖ –ì–û–¢–û–í–û –ö –û–ë–£–ß–ï–ù–ò–Æ!")
            print("     –ú–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.")
        else:
            print("  ‚ö†Ô∏è  –ù–ï –ì–û–¢–û–í–û –ö –û–ë–£–ß–ï–ù–ò–Æ")
            print("     –¢—Ä–µ–±—É—é—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∏ (—Å–º. —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∏–∂–µ)")

        print()

    def _print_recommendations(self, results: dict):
        """–ü–µ—á–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
        readiness = results["readiness"]

        if readiness["issues"]:
            print("‚ùå –ü–†–û–ë–õ–ï–ú–´:")
            for issue in readiness["issues"]:
                print(f"   ‚Ä¢ {issue}")
            print()

        if readiness["recommendations"]:
            print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
            for rec in readiness["recommendations"]:
                print(f"   ‚Ä¢ {rec}")
            print()

        if readiness["ready"]:
            print("üöÄ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
            print("   1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ:")
            print("      python backend/ml_engine/training_orchestrator.py")
            print()
            print("   2. –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ MLflow:")
            print("      http://localhost:5000")
            print()

        print("=" * 80)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    parser = argparse.ArgumentParser(
        description="–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è –∏–∑ Feature Store (Parquet)"
    )
    parser.add_argument(
        "--feature-group",
        default="training_features",
        help="–ù–∞–∑–≤–∞–Ω–∏–µ feature group (default: training_features)"
    )
    parser.add_argument(
        "--show-details",
        action="store_true",
        help="–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º"
    )

    args = parser.parse_args()

    analyzer = ParquetDataAnalyzer(feature_group=args.feature_group)
    analyzer.analyze(show_details=args.show_details)


if __name__ == "__main__":
    main()
